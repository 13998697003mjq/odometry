{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PWC-Net-small model finetuning (with cyclical learning rate schedule)\n",
    "=======================================================\n",
    "\n",
    "In this notebook we:\n",
    "- Use a small model (no dense or residual connections), 6 level pyramid, uspample level 2 by 4 as the final flow prediction\n",
    "- Train the PWC-Net-small model on a mix of the `FlyingChairs` and `FlyingThings3DHalfRes` dataset using a Cyclic<sub>short</sub> schedule of our own\n",
    "- Let the Cyclic<sub>short</sub> schedule oscillate between `2e-05` and `1e-06` for 200,000 steps\n",
    "- Switch to the \"robust\" loss described in the paper, instead of the \"multiscale\" loss used during training\n",
    "\n",
    "Below, look for `TODO` references and customize this notebook based on your own needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "[2018a]<a name=\"2018a\"></a> Sun et al. 2018. PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume. [[arXiv]](https://arxiv.org/abs/1709.02371) [[web]](http://research.nvidia.com/publication/2018-02_PWC-Net%3A-CNNs-for) [[PyTorch (Official)]](https://github.com/NVlabs/PWC-Net/tree/master/PyTorch) [[Caffe (Official)]](https://github.com/NVlabs/PWC-Net/tree/master/Caffe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pwcnet_finetune.ipynb\n",
    "\n",
    "PWC-Net model finetuning.\n",
    "\n",
    "Written by Phil Ferriere\n",
    "\n",
    "Licensed under the MIT License (see LICENSE for details)\n",
    "\n",
    "Tensorboard:\n",
    "    [win] tensorboard --logdir=E:\\\\repos\\\\tf-optflow\\\\tfoptflow\\\\pwcnet-sm-6-2-cyclic-chairsthingsmix_finetuned\n",
    "    [ubu] tensorboard --logdir=/media/EDrive/repos/tf-optflow/tfoptflow/pwcnet-sm-6-2-cyclic-chairsthingsmix_finetuned\n",
    "\"\"\"\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from dataset_mpisintel import MPISintelDataset\n",
    "from dataset_base import _DEFAULT_DS_TUNE_OPTIONS\n",
    "from dataset_flyingchairs import FlyingChairsDataset\n",
    "from dataset_flyingthings3d import FlyingThings3DHalfResDataset\n",
    "from dataset_mixer import MixedDataset\n",
    "from model_pwcnet import ModelPWCNet, _DEFAULT_PWCNET_FINETUNE_OPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 24 12:22:50 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.40.04    Driver Version: 418.40.04    CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P40           Off  | 00000000:04:00.0 Off |                  Off |\n",
      "| N/A   35C    P0    50W / 250W |    749MiB / 24451MiB |     19%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P40           Off  | 00000000:06:00.0 Off |                  Off |\n",
      "| N/A   34C    P0    50W / 250W |    749MiB / 24451MiB |     20%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla P40           Off  | 00000000:07:00.0 Off |                  Off |\n",
      "| N/A   35C    P0    50W / 250W |    749MiB / 24451MiB |     20%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     30176      C   python                                       737MiB |\n",
      "|    1     30165      C   python                                       737MiB |\n",
      "|    2     30164      C   python                                       737MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Set this first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: You MUST set dataset_root to the correct path on your machine!\n",
    "if sys.platform.startswith(\"win\"):\n",
    "    _DATASET_ROOT = 'E:/datasets/'\n",
    "else:\n",
    "    _DATASET_ROOT = '/Vol1/dbstore/datasets/sintel/'\n",
    "_MPISINTEL_ROOT = _DATASET_ROOT + 'Sintel_color'\n",
    "    \n",
    "# TODO: You MUST adjust the settings below based on the number of GPU(s) used for training\n",
    "# Set controller device and devices\n",
    "# A one-gpu setup would be something like controller='/device:GPU:0' and gpu_devices=['/device:GPU:0']\n",
    "# Here, we use a dual-GPU setup, as shown below\n",
    "# gpu_devices = ['/device:GPU:0', '/device:GPU:1']\n",
    "# controller = '/device:CPU:0'\n",
    "gpu_devices = ['/device:GPU:0']\n",
    "controller = '/device:GPU:0'\n",
    "\n",
    "# TODO: You MUST adjust this setting below based on the amount of memory on your GPU(s)\n",
    "# Batch size\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune on `sintel` mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: You MUST set the batch size based on the capabilities of your GPU(s) \n",
    "#  Load train dataset\n",
    "ds_opts = deepcopy(_DEFAULT_DS_TUNE_OPTIONS)\n",
    "ds_opts['in_memory'] = False                          # Too many samples to keep in memory at once, so don't preload them\n",
    "ds_opts['aug_type'] = 'heavy'                         # Apply all supported augmentations\n",
    "ds_opts['batch_size'] = batch_size * len(gpu_devices) # Use a multiple of 8; here, 16 for dual-GPU mode (Titan X & 1080 Ti)\n",
    "#ds_opts['crop_preproc'] = (256, 448)                  # Crop to a smaller input size\n",
    "#ds1 = FlyingChairsDataset(mode='train_with_val', ds_root=_FLYINGCHAIRS_ROOT, options=ds_opts)\n",
    "ds_opts['type'] = 'into_future'\n",
    "#ds2 = FlyingThings3DHalfResDataset(mode='train_with_val', ds_root=_FLYINGTHINGS3DHALFRES_ROOT, options=ds_opts)\n",
    "#ds = MixedDataset(mode='train_with_val', datasets=[ds1, ds2], options=ds_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_opts = deepcopy(_DEFAULT_DS_TUNE_OPTIONS)\n",
    "ds_opts['type'] = 'clean'\n",
    "ds = MPISintelDataset(mode='train_with_val', ds_root=_MPISINTEL_ROOT, options=ds_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Configuration:\n",
      "  verbose              False\n",
      "  in_memory            False\n",
      "  crop_preproc         (384, 768)\n",
      "  scale_preproc        None\n",
      "  type                 clean\n",
      "  tb_test_imgs         False\n",
      "  random_seed          1969\n",
      "  val_split            0.03\n",
      "  aug_type             heavy\n",
      "  aug_labels           True\n",
      "  fliplr               0.5\n",
      "  flipud               0.5\n",
      "  translate            (0.5, 0.05)\n",
      "  scale                (0.5, 0.05)\n",
      "  mode                 train_with_val\n",
      "  train size           1009\n",
      "  val size             32\n"
     ]
    }
   ],
   "source": [
    "# Display dataset configuration\n",
    "ds.print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = './models/pwcnet-lg-6-2-multisteps-chairsthingsmix/pwcnet.ckpt-595000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the default options\n",
    "nn_opts = deepcopy(_DEFAULT_PWCNET_FINETUNE_OPTIONS)\n",
    "nn_opts['verbose'] = True\n",
    "nn_opts['ckpt_path'] = './models/pwcnet-lg-6-2-multisteps-chairsthingsmix/pwcnet.ckpt-595000'\n",
    "nn_opts['ckpt_dir'] = './pwcnet-lg-6-2-cyclic-sintel_finetuned/'\n",
    "nn_opts['batch_size'] = 8\n",
    "#nn_opts['x_shape'] = [2, ds_opts['crop_preproc'][0], ds_opts['crop_preproc'][1], 3]\n",
    "#nn_opts['y_shape'] = [ds_opts['crop_preproc'][0], ds_opts['crop_preproc'][1], 2]\n",
    "nn_opts['use_tf_data'] = True # Use tf.data reader\n",
    "nn_opts['gpu_devices'] = gpu_devices\n",
    "nn_opts['controller'] = controller\n",
    "\n",
    "# Use the PWC-Net-small model in quarter-resolution mode\n",
    "nn_opts['use_dense_cx'] = True\n",
    "nn_opts['use_res_cx'] = True\n",
    "nn_opts['pyr_lvls'] = 6\n",
    "nn_opts['flow_pred_lvl'] = 2\n",
    "\n",
    "# Robust loss as described doesn't work, so try the following:\n",
    "nn_opts['loss_fn'] = 'loss_multiscale' # 'loss_multiscale' # 'loss_robust' # 'loss_robust'\n",
    "nn_opts['q'] = 1. # 0.4 # 1. # 0.4 # 1.\n",
    "nn_opts['epsilon'] = 0. # 0.01 # 0. # 0.01 # 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rate schedule. This schedule is for a single GPU using a batch size of 8.\n",
    "# Below,we adjust the schedule to the size of the batch and the number of GPUs.\n",
    "nn_opts['lr_policy'] = 'multisteps'\n",
    "nn_opts['init_lr'] = 1e-05\n",
    "nn_opts['lr_boundaries'] = [80000, 120000, 160000, 200000]\n",
    "nn_opts['lr_values'] = [1e-05, 5e-06, 2.5e-06, 1.25e-06, 6.25e-07]\n",
    "nn_opts['max_steps'] = 200000\n",
    "\n",
    "# Below,we adjust the schedule to the size of the batch and our number of GPUs (2).\n",
    "nn_opts['max_steps'] = int(nn_opts['max_steps'] * 8 / 8)\n",
    "nn_opts['cyclic_lr_stepsize'] = int(nn_opts['cyclic_lr_stepsize'] * 8 / 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "... model built.\n",
      "Configuring training ops...\n",
      "... training ops configured.\n",
      "Initializing from pre-trained model at ./models/pwcnet-lg-6-2-multisteps-chairsthingsmix/pwcnet.ckpt-595000 for finetuning...\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./models/pwcnet-lg-6-2-multisteps-chairsthingsmix/pwcnet.ckpt-595000\n",
      "... model initialized\n",
      "\n",
      "Model Configuration:\n",
      "  verbose                True\n",
      "  ckpt_path              ./models/pwcnet-lg-6-2-multisteps-chairsthingsmix/pwcnet.ckpt-595000\n",
      "  ckpt_dir               ./pwcnet-lg-6-2-cyclic-sintel_finetuned/\n",
      "  max_to_keep            10\n",
      "  x_dtype                <dtype: 'float32'>\n",
      "  x_shape                [2, 384, 768, 3]\n",
      "  y_dtype                <dtype: 'float32'>\n",
      "  y_shape                [384, 768, 2]\n",
      "  train_mode             fine-tune\n",
      "  adapt_info             None\n",
      "  sparse_gt_flow         False\n",
      "  display_step           100\n",
      "  snapshot_step          1000\n",
      "  val_step               1000\n",
      "  val_batch_size         -1\n",
      "  tb_val_imgs            top_flow\n",
      "  tb_test_imgs           None\n",
      "  gpu_devices            ['/device:GPU:0']\n",
      "  controller             /device:GPU:0\n",
      "  use_tf_data            True\n",
      "  use_mixed_precision    False\n",
      "  loss_scaler            128.0\n",
      "  batch_size             8\n",
      "  lr_policy              multisteps\n",
      "  max_steps              200000\n",
      "  lr_boundaries          [80000, 120000, 160000, 200000]\n",
      "  lr_values              [1e-05, 5e-06, 2.5e-06, 1.25e-06, 6.25e-07]\n",
      "  loss_fn                loss_multiscale\n",
      "  alphas                 [0.32, 0.08, 0.02, 0.01, 0.005]\n",
      "  gamma                  0.0004\n",
      "  q                      1.0\n",
      "  epsilon                0.0\n",
      "  pyr_lvls               6\n",
      "  flow_pred_lvl          2\n",
      "  search_range           4\n",
      "  use_dense_cx           True\n",
      "  use_res_cx             True\n",
      "  mode                   train_with_val\n",
      "  trainable params       14079050\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model and display the model configuration\n",
    "nn = ModelPWCNet(mode='train_with_val', options=nn_opts, dataset=ds)\n",
    "nn.print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start finetuning...\n",
      "WARNING:tensorflow:From /Vol0/user/f.konokhov/tfoptflow/tfoptflow/dataset_base.py:1062: shuffle_and_repeat (from tensorflow.contrib.data.python.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.shuffle_and_repeat(...)`.\n",
      "WARNING:tensorflow:From /Vol0/user/f.konokhov/tfoptflow/tfoptflow/dataset_base.py:1065: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.map_and_batch(...)`.\n",
      "2019-04-24 12:31:08 Iter 100 [Train]: loss=109.68, epe=2.80, lr=0.000010, samples/sec=6.4, sec/step=1.251, eta=2 days, 21:26:36\n",
      "2019-04-24 12:32:54 Iter 200 [Train]: loss=107.05, epe=2.73, lr=0.000010, samples/sec=7.7, sec/step=1.035, eta=2 days, 9:25:57\n",
      "2019-04-24 12:34:40 Iter 300 [Train]: loss=104.22, epe=2.64, lr=0.000010, samples/sec=7.8, sec/step=1.030, eta=2 days, 9:07:48\n",
      "2019-04-24 12:36:26 Iter 400 [Train]: loss=100.05, epe=2.51, lr=0.000010, samples/sec=7.7, sec/step=1.033, eta=2 days, 9:17:26\n",
      "2019-04-24 12:38:11 Iter 500 [Train]: loss=94.04, epe=2.34, lr=0.000010, samples/sec=7.8, sec/step=1.027, eta=2 days, 8:55:02\n",
      "2019-04-24 12:39:55 Iter 600 [Train]: loss=104.23, epe=2.62, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 8:33:57\n",
      "2019-04-24 12:41:41 Iter 700 [Train]: loss=96.46, epe=2.40, lr=0.000010, samples/sec=7.8, sec/step=1.031, eta=2 days, 9:05:11\n",
      "2019-04-24 12:43:27 Iter 800 [Train]: loss=90.97, epe=2.22, lr=0.000010, samples/sec=7.7, sec/step=1.033, eta=2 days, 9:08:29\n",
      "2019-04-24 12:45:12 Iter 900 [Train]: loss=100.62, epe=2.50, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 8:42:10\n",
      "2019-04-24 12:46:59 Iter 1000 [Train]: loss=85.77, epe=2.05, lr=0.000010, samples/sec=7.7, sec/step=1.043, eta=2 days, 9:39:56\n",
      "2019-04-24 12:47:03 Iter 1000 [Val]: loss=78.77, epe=1.96\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-1000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-1000\n",
      "2019-04-24 12:49:01 Iter 1100 [Train]: loss=92.61, epe=2.27, lr=0.000010, samples/sec=7.7, sec/step=1.038, eta=2 days, 9:20:50\n",
      "2019-04-24 12:50:47 Iter 1200 [Train]: loss=85.24, epe=2.04, lr=0.000010, samples/sec=7.8, sec/step=1.031, eta=2 days, 8:56:58\n",
      "2019-04-24 12:52:32 Iter 1300 [Train]: loss=97.00, epe=2.38, lr=0.000010, samples/sec=7.7, sec/step=1.034, eta=2 days, 9:03:44\n",
      "2019-04-24 12:54:18 Iter 1400 [Train]: loss=102.45, epe=2.58, lr=0.000010, samples/sec=7.8, sec/step=1.029, eta=2 days, 8:44:51\n",
      "2019-04-24 12:56:04 Iter 1500 [Train]: loss=90.29, epe=2.19, lr=0.000010, samples/sec=7.7, sec/step=1.034, eta=2 days, 8:59:59\n",
      "2019-04-24 12:57:50 Iter 1600 [Train]: loss=87.37, epe=2.10, lr=0.000010, samples/sec=7.7, sec/step=1.034, eta=2 days, 9:00:00\n",
      "2019-04-24 12:59:35 Iter 1700 [Train]: loss=82.45, epe=1.94, lr=0.000010, samples/sec=7.7, sec/step=1.033, eta=2 days, 8:54:13\n",
      "2019-04-24 13:01:21 Iter 1800 [Train]: loss=96.44, epe=2.39, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 8:27:01\n",
      "2019-04-24 13:03:06 Iter 1900 [Train]: loss=87.79, epe=2.11, lr=0.000010, samples/sec=7.8, sec/step=1.031, eta=2 days, 8:43:47\n",
      "2019-04-24 13:04:51 Iter 2000 [Train]: loss=87.29, epe=2.08, lr=0.000010, samples/sec=7.8, sec/step=1.029, eta=2 days, 8:35:21\n",
      "2019-04-24 13:04:53 Iter 2000 [Val]: loss=73.95, epe=1.84\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-2000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-2000\n",
      "2019-04-24 13:06:46 Iter 2100 [Train]: loss=87.09, epe=2.08, lr=0.000010, samples/sec=7.8, sec/step=1.026, eta=2 days, 8:23:43\n",
      "2019-04-24 13:08:31 Iter 2200 [Train]: loss=89.50, epe=2.18, lr=0.000010, samples/sec=7.8, sec/step=1.026, eta=2 days, 8:22:08\n",
      "2019-04-24 13:10:16 Iter 2300 [Train]: loss=88.43, epe=2.12, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 8:13:24\n",
      "2019-04-24 13:12:02 Iter 2400 [Train]: loss=88.99, epe=2.13, lr=0.000010, samples/sec=7.7, sec/step=1.034, eta=2 days, 8:44:13\n",
      "2019-04-24 13:13:47 Iter 2500 [Train]: loss=87.70, epe=2.09, lr=0.000010, samples/sec=7.8, sec/step=1.031, eta=2 days, 8:34:09\n",
      "2019-04-24 13:15:33 Iter 2600 [Train]: loss=90.13, epe=2.17, lr=0.000010, samples/sec=7.8, sec/step=1.027, eta=2 days, 8:18:27\n",
      "2019-04-24 13:17:18 Iter 2700 [Train]: loss=83.54, epe=1.96, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 8:09:54\n",
      "2019-04-24 13:19:03 Iter 2800 [Train]: loss=80.08, epe=1.87, lr=0.000010, samples/sec=7.8, sec/step=1.029, eta=2 days, 8:21:05\n",
      "2019-04-24 13:20:49 Iter 2900 [Train]: loss=88.67, epe=2.09, lr=0.000010, samples/sec=7.8, sec/step=1.028, eta=2 days, 8:16:22\n",
      "2019-04-24 13:22:34 Iter 3000 [Train]: loss=83.04, epe=1.94, lr=0.000010, samples/sec=7.8, sec/step=1.028, eta=2 days, 8:14:50\n",
      "2019-04-24 13:22:35 Iter 3000 [Val]: loss=72.59, epe=1.79\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-3000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-3000\n",
      "2019-04-24 13:24:30 Iter 3100 [Train]: loss=82.80, epe=1.92, lr=0.000010, samples/sec=7.7, sec/step=1.036, eta=2 days, 8:38:50\n",
      "2019-04-24 13:26:14 Iter 3200 [Train]: loss=80.96, epe=1.90, lr=0.000010, samples/sec=7.8, sec/step=1.019, eta=2 days, 7:42:50\n",
      "2019-04-24 13:27:59 Iter 3300 [Train]: loss=84.10, epe=1.98, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 8:00:09\n",
      "2019-04-24 13:29:44 Iter 3400 [Train]: loss=84.33, epe=2.00, lr=0.000010, samples/sec=7.8, sec/step=1.028, eta=2 days, 8:09:27\n",
      "2019-04-24 13:31:30 Iter 3500 [Train]: loss=89.67, epe=2.12, lr=0.000010, samples/sec=7.8, sec/step=1.031, eta=2 days, 8:16:14\n",
      "2019-04-24 13:33:15 Iter 3600 [Train]: loss=85.14, epe=2.00, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 7:54:05\n",
      "2019-04-24 13:35:00 Iter 3700 [Train]: loss=83.21, epe=1.96, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 7:53:27\n",
      "2019-04-24 13:36:45 Iter 3800 [Train]: loss=82.08, epe=1.95, lr=0.000010, samples/sec=7.8, sec/step=1.030, eta=2 days, 8:07:11\n",
      "2019-04-24 13:38:30 Iter 3900 [Train]: loss=84.49, epe=2.00, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 7:45:31\n",
      "2019-04-24 13:40:15 Iter 4000 [Train]: loss=83.77, epe=1.96, lr=0.000010, samples/sec=7.8, sec/step=1.027, eta=2 days, 7:53:24\n",
      "2019-04-24 13:40:16 Iter 4000 [Val]: loss=78.21, epe=1.96\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-4000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-4000\n",
      "2019-04-24 13:42:09 Iter 4100 [Train]: loss=83.26, epe=1.94, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 7:47:25\n",
      "2019-04-24 13:43:54 Iter 4200 [Train]: loss=75.12, epe=1.72, lr=0.000010, samples/sec=7.8, sec/step=1.026, eta=2 days, 7:47:21\n",
      "2019-04-24 13:45:39 Iter 4300 [Train]: loss=86.97, epe=2.05, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 7:33:58\n",
      "2019-04-24 13:47:24 Iter 4400 [Train]: loss=82.07, epe=1.90, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 7:30:45\n",
      "2019-04-24 13:49:09 Iter 4500 [Train]: loss=76.03, epe=1.75, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 7:38:27\n",
      "2019-04-24 13:50:54 Iter 4600 [Train]: loss=84.79, epe=2.00, lr=0.000010, samples/sec=7.8, sec/step=1.027, eta=2 days, 7:45:10\n",
      "2019-04-24 13:52:38 Iter 4700 [Train]: loss=82.51, epe=1.91, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 7:23:13\n",
      "2019-04-24 13:54:23 Iter 4800 [Train]: loss=78.87, epe=1.81, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 7:31:03\n",
      "2019-04-24 13:56:08 Iter 4900 [Train]: loss=76.61, epe=1.75, lr=0.000010, samples/sec=7.8, sec/step=1.020, eta=2 days, 7:16:14\n",
      "2019-04-24 13:57:52 Iter 5000 [Train]: loss=81.09, epe=1.89, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 7:22:27\n",
      "2019-04-24 13:57:53 Iter 5000 [Val]: loss=75.89, epe=1.84\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-5000 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-5000\n",
      "2019-04-24 13:59:47 Iter 5100 [Train]: loss=81.31, epe=1.85, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 7:29:18\n",
      "2019-04-24 14:01:32 Iter 5200 [Train]: loss=77.48, epe=1.80, lr=0.000010, samples/sec=7.8, sec/step=1.030, eta=2 days, 7:43:29\n",
      "2019-04-24 14:03:17 Iter 5300 [Train]: loss=85.32, epe=2.03, lr=0.000010, samples/sec=7.8, sec/step=1.020, eta=2 days, 7:09:30\n",
      "2019-04-24 14:05:01 Iter 5400 [Train]: loss=76.71, epe=1.77, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 7:12:44\n",
      "2019-04-24 14:06:47 Iter 5500 [Train]: loss=81.73, epe=1.90, lr=0.000010, samples/sec=7.8, sec/step=1.028, eta=2 days, 7:33:49\n",
      "2019-04-24 14:08:32 Iter 5600 [Train]: loss=78.86, epe=1.82, lr=0.000010, samples/sec=7.8, sec/step=1.026, eta=2 days, 7:22:54\n",
      "2019-04-24 14:10:16 Iter 5700 [Train]: loss=82.06, epe=1.93, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 7:17:22\n",
      "2019-04-24 14:12:01 Iter 5800 [Train]: loss=84.97, epe=2.02, lr=0.000010, samples/sec=7.8, sec/step=1.020, eta=2 days, 7:00:15\n",
      "2019-04-24 14:13:46 Iter 5900 [Train]: loss=78.83, epe=1.80, lr=0.000010, samples/sec=7.8, sec/step=1.027, eta=2 days, 7:21:52\n",
      "2019-04-24 14:15:31 Iter 6000 [Train]: loss=78.84, epe=1.82, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 7:03:08\n",
      "2019-04-24 14:15:32 Iter 6000 [Val]: loss=83.69, epe=2.21\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-6000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-6000\n",
      "2019-04-24 14:17:25 Iter 6100 [Train]: loss=78.21, epe=1.79, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 7:02:36\n",
      "2019-04-24 14:19:09 Iter 6200 [Train]: loss=82.57, epe=1.90, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 6:59:36\n",
      "2019-04-24 14:20:54 Iter 6300 [Train]: loss=75.87, epe=1.76, lr=0.000010, samples/sec=7.8, sec/step=1.027, eta=2 days, 7:15:22\n",
      "2019-04-24 14:22:39 Iter 6400 [Train]: loss=80.54, epe=1.85, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 7:06:17\n",
      "2019-04-24 14:24:24 Iter 6500 [Train]: loss=77.24, epe=1.77, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 7:07:08\n",
      "2019-04-24 14:26:10 Iter 6600 [Train]: loss=77.98, epe=1.79, lr=0.000010, samples/sec=7.8, sec/step=1.030, eta=2 days, 7:18:36\n",
      "2019-04-24 14:27:54 Iter 6700 [Train]: loss=80.32, epe=1.84, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 6:57:37\n",
      "2019-04-24 14:29:40 Iter 6800 [Train]: loss=76.68, epe=1.75, lr=0.000010, samples/sec=7.8, sec/step=1.026, eta=2 days, 7:04:09\n",
      "2019-04-24 14:31:24 Iter 6900 [Train]: loss=79.14, epe=1.82, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 6:50:17\n",
      "2019-04-24 14:33:10 Iter 7000 [Train]: loss=70.91, epe=1.61, lr=0.000010, samples/sec=7.8, sec/step=1.031, eta=2 days, 7:14:47\n",
      "2019-04-24 14:33:11 Iter 7000 [Val]: loss=63.24, epe=1.45\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-7000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-7000\n",
      "2019-04-24 14:35:05 Iter 7100 [Train]: loss=79.59, epe=1.82, lr=0.000010, samples/sec=7.8, sec/step=1.029, eta=2 days, 7:09:30\n",
      "2019-04-24 14:36:49 Iter 7200 [Train]: loss=78.30, epe=1.80, lr=0.000010, samples/sec=7.9, sec/step=1.019, eta=2 days, 6:33:32\n",
      "2019-04-24 14:38:34 Iter 7300 [Train]: loss=77.06, epe=1.76, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 6:40:11\n",
      "2019-04-24 14:40:18 Iter 7400 [Train]: loss=77.66, epe=1.77, lr=0.000010, samples/sec=7.8, sec/step=1.023, eta=2 days, 6:42:34\n",
      "2019-04-24 14:42:03 Iter 7500 [Train]: loss=71.33, epe=1.61, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 6:40:14\n",
      "2019-04-24 14:43:48 Iter 7600 [Train]: loss=79.83, epe=1.85, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 6:42:47\n",
      "2019-04-24 14:45:33 Iter 7700 [Train]: loss=75.60, epe=1.72, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 6:41:41\n",
      "2019-04-24 14:47:17 Iter 7800 [Train]: loss=78.74, epe=1.81, lr=0.000010, samples/sec=7.8, sec/step=1.023, eta=2 days, 6:36:47\n",
      "2019-04-24 14:49:03 Iter 7900 [Train]: loss=72.76, epe=1.64, lr=0.000010, samples/sec=7.8, sec/step=1.032, eta=2 days, 7:03:06\n",
      "2019-04-24 14:50:48 Iter 8000 [Train]: loss=72.24, epe=1.61, lr=0.000010, samples/sec=7.8, sec/step=1.026, eta=2 days, 6:44:29\n",
      "2019-04-24 14:50:49 Iter 8000 [Val]: loss=74.55, epe=1.81\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-8000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-8000\n",
      "2019-04-24 14:52:45 Iter 8100 [Train]: loss=81.44, epe=1.88, lr=0.000010, samples/sec=7.8, sec/step=1.027, eta=2 days, 6:44:38\n",
      "2019-04-24 14:54:30 Iter 8200 [Train]: loss=77.54, epe=1.75, lr=0.000010, samples/sec=7.8, sec/step=1.023, eta=2 days, 6:30:07\n",
      "2019-04-24 14:56:15 Iter 8300 [Train]: loss=72.52, epe=1.63, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 6:24:02\n",
      "2019-04-24 14:58:00 Iter 8400 [Train]: loss=73.65, epe=1.65, lr=0.000010, samples/sec=7.8, sec/step=1.020, eta=2 days, 6:17:15\n",
      "2019-04-24 14:59:45 Iter 8500 [Train]: loss=80.18, epe=1.84, lr=0.000010, samples/sec=7.8, sec/step=1.028, eta=2 days, 6:41:24\n",
      "2019-04-24 15:01:31 Iter 8600 [Train]: loss=75.22, epe=1.70, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 6:30:19\n",
      "2019-04-24 15:03:16 Iter 8700 [Train]: loss=74.29, epe=1.70, lr=0.000010, samples/sec=7.8, sec/step=1.020, eta=2 days, 6:12:26\n",
      "2019-04-24 15:05:01 Iter 8800 [Train]: loss=78.64, epe=1.80, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 6:21:56\n",
      "2019-04-24 15:06:46 Iter 8900 [Train]: loss=74.37, epe=1.68, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 6:13:20\n",
      "2019-04-24 15:08:31 Iter 9000 [Train]: loss=72.26, epe=1.58, lr=0.000010, samples/sec=7.8, sec/step=1.023, eta=2 days, 6:15:45\n",
      "2019-04-24 15:08:32 Iter 9000 [Val]: loss=75.05, epe=1.87\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-9000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-9000\n",
      "2019-04-24 15:10:26 Iter 9100 [Train]: loss=77.12, epe=1.76, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 6:17:09\n",
      "2019-04-24 15:12:12 Iter 9200 [Train]: loss=78.77, epe=1.82, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 6:10:12\n",
      "2019-04-24 15:13:57 Iter 9300 [Train]: loss=69.58, epe=1.53, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 6:08:30\n",
      "2019-04-24 15:15:42 Iter 9400 [Train]: loss=77.52, epe=1.75, lr=0.000010, samples/sec=7.9, sec/step=1.016, eta=2 days, 5:46:10\n",
      "2019-04-24 15:17:28 Iter 9500 [Train]: loss=77.22, epe=1.77, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 6:05:15\n",
      "2019-04-24 15:19:13 Iter 9600 [Train]: loss=70.53, epe=1.57, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 6:03:08\n",
      "2019-04-24 15:20:58 Iter 9700 [Train]: loss=72.90, epe=1.62, lr=0.000010, samples/sec=7.9, sec/step=1.017, eta=2 days, 5:47:01\n",
      "2019-04-24 15:22:44 Iter 9800 [Train]: loss=71.77, epe=1.63, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 6:00:56\n",
      "2019-04-24 15:24:30 Iter 9900 [Train]: loss=77.81, epe=1.75, lr=0.000010, samples/sec=7.8, sec/step=1.026, eta=2 days, 6:09:18\n",
      "2019-04-24 15:26:15 Iter 10000 [Train]: loss=69.73, epe=1.57, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 6:05:20\n",
      "2019-04-24 15:26:17 Iter 10000 [Val]: loss=72.26, epe=1.72\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-10000\n",
      "2019-04-24 15:28:11 Iter 10100 [Train]: loss=72.02, epe=1.61, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 5:54:08\n",
      "2019-04-24 15:29:57 Iter 10200 [Train]: loss=72.03, epe=1.61, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 6:00:31\n",
      "2019-04-24 15:31:43 Iter 10300 [Train]: loss=72.80, epe=1.64, lr=0.000010, samples/sec=7.8, sec/step=1.028, eta=2 days, 6:11:23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-24 15:33:29 Iter 10400 [Train]: loss=75.36, epe=1.69, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 5:58:41\n",
      "2019-04-24 15:35:14 Iter 10500 [Train]: loss=67.52, epe=1.47, lr=0.000010, samples/sec=7.8, sec/step=1.020, eta=2 days, 5:40:50\n",
      "2019-04-24 15:36:59 Iter 10600 [Train]: loss=76.81, epe=1.73, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 5:43:23\n",
      "2019-04-24 15:38:44 Iter 10700 [Train]: loss=74.37, epe=1.65, lr=0.000010, samples/sec=7.9, sec/step=1.016, eta=2 days, 5:25:27\n",
      "2019-04-24 15:40:29 Iter 10800 [Train]: loss=76.59, epe=1.79, lr=0.000010, samples/sec=7.9, sec/step=1.018, eta=2 days, 5:30:32\n",
      "2019-04-24 15:42:15 Iter 10900 [Train]: loss=73.96, epe=1.66, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 5:48:47\n",
      "2019-04-24 15:44:01 Iter 11000 [Train]: loss=66.58, epe=1.46, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 5:37:27\n",
      "2019-04-24 15:44:02 Iter 11000 [Val]: loss=77.45, epe=1.94\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-11000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-11000\n",
      "2019-04-24 15:45:56 Iter 11100 [Train]: loss=75.38, epe=1.69, lr=0.000010, samples/sec=7.9, sec/step=1.019, eta=2 days, 5:27:44\n",
      "2019-04-24 15:47:41 Iter 11200 [Train]: loss=71.84, epe=1.59, lr=0.000010, samples/sec=7.8, sec/step=1.020, eta=2 days, 5:29:03\n",
      "2019-04-24 15:49:27 Iter 11300 [Train]: loss=71.84, epe=1.62, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 5:39:52\n",
      "2019-04-24 15:51:12 Iter 11400 [Train]: loss=73.96, epe=1.64, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 5:31:41\n",
      "2019-04-24 15:52:58 Iter 11500 [Train]: loss=73.96, epe=1.67, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 5:38:44\n",
      "2019-04-24 15:54:45 Iter 11600 [Train]: loss=73.98, epe=1.66, lr=0.000010, samples/sec=7.7, sec/step=1.039, eta=2 days, 6:22:14\n",
      "2019-04-24 15:56:30 Iter 11700 [Train]: loss=72.86, epe=1.63, lr=0.000010, samples/sec=7.9, sec/step=1.016, eta=2 days, 5:07:41\n",
      "2019-04-24 15:58:15 Iter 11800 [Train]: loss=68.55, epe=1.50, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 5:23:51\n",
      "2019-04-24 16:00:01 Iter 11900 [Train]: loss=70.66, epe=1.57, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 5:20:06\n",
      "2019-04-24 16:01:46 Iter 12000 [Train]: loss=71.61, epe=1.60, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 5:27:32\n",
      "2019-04-24 16:01:48 Iter 12000 [Val]: loss=65.96, epe=1.53\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-12000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-12000\n",
      "2019-04-24 16:03:41 Iter 12100 [Train]: loss=68.66, epe=1.52, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 5:18:12\n",
      "2019-04-24 16:05:27 Iter 12200 [Train]: loss=73.92, epe=1.69, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 5:16:47\n",
      "2019-04-24 16:07:13 Iter 12300 [Train]: loss=71.93, epe=1.59, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 5:23:46\n",
      "2019-04-24 16:08:58 Iter 12400 [Train]: loss=76.56, epe=1.72, lr=0.000010, samples/sec=7.8, sec/step=1.020, eta=2 days, 5:09:40\n",
      "2019-04-24 16:10:43 Iter 12500 [Train]: loss=69.32, epe=1.51, lr=0.000010, samples/sec=7.9, sec/step=1.016, eta=2 days, 4:54:49\n",
      "2019-04-24 16:12:28 Iter 12600 [Train]: loss=69.91, epe=1.55, lr=0.000010, samples/sec=7.8, sec/step=1.020, eta=2 days, 5:04:30\n",
      "2019-04-24 16:14:14 Iter 12700 [Train]: loss=73.49, epe=1.64, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 5:19:50\n",
      "2019-04-24 16:16:00 Iter 12800 [Train]: loss=68.60, epe=1.51, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 5:18:40\n",
      "2019-04-24 16:17:46 Iter 12900 [Train]: loss=73.62, epe=1.62, lr=0.000010, samples/sec=7.8, sec/step=1.027, eta=2 days, 5:22:16\n",
      "2019-04-24 16:19:32 Iter 13000 [Train]: loss=69.31, epe=1.56, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 5:15:30\n",
      "2019-04-24 16:19:33 Iter 13000 [Val]: loss=56.08, epe=1.23\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-13000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-13000\n",
      "2019-04-24 16:21:27 Iter 13100 [Train]: loss=69.50, epe=1.54, lr=0.000010, samples/sec=7.9, sec/step=1.018, eta=2 days, 4:51:28\n",
      "2019-04-24 16:23:12 Iter 13200 [Train]: loss=69.68, epe=1.54, lr=0.000010, samples/sec=7.9, sec/step=1.017, eta=2 days, 4:45:21\n",
      "2019-04-24 16:24:57 Iter 13300 [Train]: loss=69.68, epe=1.52, lr=0.000010, samples/sec=7.8, sec/step=1.020, eta=2 days, 4:52:46\n",
      "2019-04-24 16:26:42 Iter 13400 [Train]: loss=69.69, epe=1.53, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 4:55:04\n",
      "2019-04-24 16:28:28 Iter 13500 [Train]: loss=71.82, epe=1.61, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 5:04:24\n",
      "2019-04-24 16:30:14 Iter 13600 [Train]: loss=70.40, epe=1.56, lr=0.000010, samples/sec=7.8, sec/step=1.026, eta=2 days, 5:07:22\n",
      "2019-04-24 16:32:00 Iter 13700 [Train]: loss=71.01, epe=1.57, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 5:01:10\n",
      "2019-04-24 16:33:45 Iter 13800 [Train]: loss=70.43, epe=1.56, lr=0.000010, samples/sec=7.9, sec/step=1.019, eta=2 days, 4:42:08\n",
      "2019-04-24 16:35:30 Iter 13900 [Train]: loss=66.73, epe=1.45, lr=0.000010, samples/sec=7.8, sec/step=1.020, eta=2 days, 4:44:46\n",
      "2019-04-24 16:37:15 Iter 14000 [Train]: loss=71.69, epe=1.59, lr=0.000010, samples/sec=7.9, sec/step=1.018, eta=2 days, 4:34:44\n",
      "2019-04-24 16:37:17 Iter 14000 [Val]: loss=59.75, epe=1.33\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-14000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-14000\n",
      "2019-04-24 16:39:11 Iter 14100 [Train]: loss=67.66, epe=1.50, lr=0.000010, samples/sec=7.8, sec/step=1.020, eta=2 days, 4:38:47\n",
      "2019-04-24 16:40:56 Iter 14200 [Train]: loss=70.41, epe=1.58, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 4:45:06\n",
      "2019-04-24 16:42:42 Iter 14300 [Train]: loss=70.61, epe=1.55, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 4:38:58\n",
      "2019-04-24 16:44:27 Iter 14400 [Train]: loss=65.95, epe=1.45, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 4:37:16\n",
      "2019-04-24 16:46:13 Iter 14500 [Train]: loss=70.43, epe=1.56, lr=0.000010, samples/sec=7.8, sec/step=1.029, eta=2 days, 5:00:28\n",
      "2019-04-24 16:47:59 Iter 14600 [Train]: loss=70.46, epe=1.55, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 4:48:40\n",
      "2019-04-24 16:49:44 Iter 14700 [Train]: loss=67.76, epe=1.50, lr=0.000010, samples/sec=7.8, sec/step=1.019, eta=2 days, 4:27:33\n",
      "2019-04-24 16:51:30 Iter 14800 [Train]: loss=72.48, epe=1.61, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 4:44:48\n",
      "2019-04-24 16:53:15 Iter 14900 [Train]: loss=70.49, epe=1.57, lr=0.000010, samples/sec=7.8, sec/step=1.020, eta=2 days, 4:25:47\n",
      "2019-04-24 16:55:01 Iter 15000 [Train]: loss=69.78, epe=1.54, lr=0.000010, samples/sec=7.9, sec/step=1.019, eta=2 days, 4:20:37\n",
      "2019-04-24 16:55:02 Iter 15000 [Val]: loss=65.81, epe=1.55\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-15000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-15000\n",
      "2019-04-24 16:56:56 Iter 15100 [Train]: loss=65.40, epe=1.43, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 4:27:19\n",
      "2019-04-24 16:58:41 Iter 15200 [Train]: loss=77.03, epe=1.72, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 4:33:57\n",
      "2019-04-24 17:00:26 Iter 15300 [Train]: loss=67.58, epe=1.49, lr=0.000010, samples/sec=7.9, sec/step=1.019, eta=2 days, 4:16:37\n",
      "2019-04-24 17:02:12 Iter 15400 [Train]: loss=67.32, epe=1.47, lr=0.000010, samples/sec=7.8, sec/step=1.023, eta=2 days, 4:28:14\n",
      "2019-04-24 17:03:57 Iter 15500 [Train]: loss=72.08, epe=1.60, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 4:18:43\n",
      "2019-04-24 17:05:43 Iter 15600 [Train]: loss=65.88, epe=1.44, lr=0.000010, samples/sec=7.8, sec/step=1.019, eta=2 days, 4:12:34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-24 17:07:29 Iter 15700 [Train]: loss=73.98, epe=1.65, lr=0.000010, samples/sec=7.8, sec/step=1.027, eta=2 days, 4:35:35\n",
      "2019-04-24 17:09:14 Iter 15800 [Train]: loss=71.07, epe=1.59, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 4:16:49\n",
      "2019-04-24 17:11:00 Iter 15900 [Train]: loss=65.59, epe=1.44, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 4:11:31\n",
      "2019-04-24 17:12:46 Iter 16000 [Train]: loss=67.66, epe=1.48, lr=0.000010, samples/sec=7.8, sec/step=1.029, eta=2 days, 4:36:37\n",
      "2019-04-24 17:12:47 Iter 16000 [Val]: loss=62.00, epe=1.41\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-16000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-16000\n",
      "2019-04-24 17:14:42 Iter 16100 [Train]: loss=69.21, epe=1.51, lr=0.000010, samples/sec=7.8, sec/step=1.027, eta=2 days, 4:27:09\n",
      "2019-04-24 17:16:27 Iter 16200 [Train]: loss=64.78, epe=1.41, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 4:17:19\n",
      "2019-04-24 17:18:13 Iter 16300 [Train]: loss=70.42, epe=1.56, lr=0.000010, samples/sec=7.8, sec/step=1.026, eta=2 days, 4:20:11\n",
      "2019-04-24 17:19:59 Iter 16400 [Train]: loss=69.25, epe=1.52, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 4:16:08\n",
      "2019-04-24 17:21:44 Iter 16500 [Train]: loss=70.05, epe=1.54, lr=0.000010, samples/sec=7.9, sec/step=1.019, eta=2 days, 3:56:41\n",
      "2019-04-24 17:23:29 Iter 16600 [Train]: loss=66.33, epe=1.47, lr=0.000010, samples/sec=7.8, sec/step=1.020, eta=2 days, 3:59:15\n",
      "2019-04-24 17:25:15 Iter 16700 [Train]: loss=70.76, epe=1.54, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 4:08:47\n",
      "2019-04-24 17:27:01 Iter 16800 [Train]: loss=66.03, epe=1.45, lr=0.000010, samples/sec=7.8, sec/step=1.027, eta=2 days, 4:15:28\n",
      "2019-04-24 17:28:47 Iter 16900 [Train]: loss=70.26, epe=1.54, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 3:57:30\n",
      "2019-04-24 17:30:32 Iter 17000 [Train]: loss=68.00, epe=1.51, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 3:55:40\n",
      "2019-04-24 17:30:33 Iter 17000 [Val]: loss=58.98, epe=1.35\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-17000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-17000\n",
      "2019-04-24 17:32:28 Iter 17100 [Train]: loss=67.57, epe=1.47, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 4:04:31\n",
      "2019-04-24 17:34:13 Iter 17200 [Train]: loss=69.66, epe=1.54, lr=0.000010, samples/sec=7.8, sec/step=1.019, eta=2 days, 3:45:37\n",
      "2019-04-24 17:35:59 Iter 17300 [Train]: loss=70.44, epe=1.56, lr=0.000010, samples/sec=7.8, sec/step=1.026, eta=2 days, 4:02:48\n",
      "2019-04-24 17:37:45 Iter 17400 [Train]: loss=70.06, epe=1.54, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 3:48:58\n",
      "2019-04-24 17:39:30 Iter 17500 [Train]: loss=67.53, epe=1.47, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 3:49:49\n",
      "2019-04-24 17:41:16 Iter 17600 [Train]: loss=66.42, epe=1.45, lr=0.000010, samples/sec=7.8, sec/step=1.027, eta=2 days, 4:01:19\n",
      "2019-04-24 17:43:02 Iter 17700 [Train]: loss=64.42, epe=1.39, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 3:43:03\n",
      "2019-04-24 17:44:47 Iter 17800 [Train]: loss=71.31, epe=1.58, lr=0.000010, samples/sec=7.8, sec/step=1.020, eta=2 days, 3:36:02\n",
      "2019-04-24 17:46:32 Iter 17900 [Train]: loss=67.10, epe=1.47, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 3:42:20\n",
      "2019-04-24 17:48:18 Iter 18000 [Train]: loss=66.50, epe=1.45, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 3:39:32\n",
      "2019-04-24 17:48:19 Iter 18000 [Val]: loss=58.46, epe=1.32\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-18000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-18000\n",
      "2019-04-24 17:50:13 Iter 18100 [Train]: loss=72.96, epe=1.63, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 3:46:04\n",
      "2019-04-24 17:51:59 Iter 18200 [Train]: loss=68.30, epe=1.51, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 3:33:55\n",
      "2019-04-24 17:53:44 Iter 18300 [Train]: loss=65.52, epe=1.41, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 3:40:31\n",
      "2019-04-24 17:55:30 Iter 18400 [Train]: loss=74.26, epe=1.66, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 3:41:46\n",
      "2019-04-24 17:57:15 Iter 18500 [Train]: loss=67.48, epe=1.47, lr=0.000010, samples/sec=7.8, sec/step=1.020, eta=2 days, 3:26:26\n",
      "2019-04-24 17:59:01 Iter 18600 [Train]: loss=71.56, epe=1.59, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 3:26:43\n",
      "2019-04-24 18:00:46 Iter 18700 [Train]: loss=64.64, epe=1.40, lr=0.000010, samples/sec=7.8, sec/step=1.019, eta=2 days, 3:19:39\n",
      "2019-04-24 18:02:31 Iter 18800 [Train]: loss=68.00, epe=1.48, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 3:22:23\n",
      "2019-04-24 18:04:17 Iter 18900 [Train]: loss=64.90, epe=1.41, lr=0.000010, samples/sec=7.8, sec/step=1.020, eta=2 days, 3:19:29\n",
      "2019-04-24 18:06:02 Iter 19000 [Train]: loss=68.45, epe=1.50, lr=0.000010, samples/sec=7.8, sec/step=1.023, eta=2 days, 3:26:20\n",
      "2019-04-24 18:06:04 Iter 19000 [Val]: loss=64.46, epe=1.50\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-19000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-19000\n",
      "2019-04-24 18:07:58 Iter 19100 [Train]: loss=71.54, epe=1.57, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 3:17:34\n",
      "2019-04-24 18:09:43 Iter 19200 [Train]: loss=63.89, epe=1.37, lr=0.000010, samples/sec=7.8, sec/step=1.023, eta=2 days, 3:23:06\n",
      "2019-04-24 18:11:29 Iter 19300 [Train]: loss=65.79, epe=1.43, lr=0.000010, samples/sec=7.8, sec/step=1.026, eta=2 days, 3:28:58\n",
      "2019-04-24 18:13:14 Iter 19400 [Train]: loss=69.00, epe=1.51, lr=0.000010, samples/sec=7.9, sec/step=1.017, eta=2 days, 3:00:47\n",
      "2019-04-24 18:14:59 Iter 19500 [Train]: loss=64.56, epe=1.40, lr=0.000010, samples/sec=7.9, sec/step=1.016, eta=2 days, 2:56:29\n",
      "2019-04-24 18:16:45 Iter 19600 [Train]: loss=69.90, epe=1.53, lr=0.000010, samples/sec=7.8, sec/step=1.023, eta=2 days, 3:14:57\n",
      "2019-04-24 18:18:30 Iter 19700 [Train]: loss=67.32, epe=1.47, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 3:15:50\n",
      "2019-04-24 18:20:16 Iter 19800 [Train]: loss=64.68, epe=1.41, lr=0.000010, samples/sec=7.8, sec/step=1.023, eta=2 days, 3:13:39\n",
      "2019-04-24 18:22:01 Iter 19900 [Train]: loss=66.97, epe=1.46, lr=0.000010, samples/sec=7.8, sec/step=1.020, eta=2 days, 3:02:28\n",
      "2019-04-24 18:23:47 Iter 20000 [Train]: loss=63.46, epe=1.35, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 3:05:02\n",
      "2019-04-24 18:23:48 Iter 20000 [Val]: loss=62.42, epe=1.45\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-20000\n",
      "2019-04-24 18:25:44 Iter 20100 [Train]: loss=66.30, epe=1.44, lr=0.000010, samples/sec=7.8, sec/step=1.030, eta=2 days, 3:27:07\n",
      "2019-04-24 18:27:29 Iter 20200 [Train]: loss=69.59, epe=1.52, lr=0.000010, samples/sec=7.9, sec/step=1.018, eta=2 days, 2:51:48\n",
      "2019-04-24 18:29:15 Iter 20300 [Train]: loss=62.94, epe=1.35, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 3:07:43\n",
      "2019-04-24 18:31:01 Iter 20400 [Train]: loss=66.04, epe=1.42, lr=0.000010, samples/sec=7.8, sec/step=1.026, eta=2 days, 3:10:42\n",
      "2019-04-24 18:32:47 Iter 20500 [Train]: loss=70.55, epe=1.55, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 2:58:26\n",
      "2019-04-24 18:34:32 Iter 20600 [Train]: loss=62.22, epe=1.35, lr=0.000010, samples/sec=7.8, sec/step=1.023, eta=2 days, 2:57:44\n",
      "2019-04-24 18:36:18 Iter 20700 [Train]: loss=67.37, epe=1.47, lr=0.000010, samples/sec=7.8, sec/step=1.029, eta=2 days, 3:14:48\n",
      "2019-04-24 18:38:05 Iter 20800 [Train]: loss=67.03, epe=1.45, lr=0.000010, samples/sec=7.8, sec/step=1.029, eta=2 days, 3:13:36\n",
      "2019-04-24 18:39:50 Iter 20900 [Train]: loss=65.35, epe=1.42, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 2:51:30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-24 18:41:36 Iter 21000 [Train]: loss=61.93, epe=1.32, lr=0.000010, samples/sec=7.8, sec/step=1.026, eta=2 days, 2:59:45\n",
      "2019-04-24 18:41:37 Iter 21000 [Val]: loss=65.14, epe=1.52\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-21000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-21000\n",
      "2019-04-24 18:43:32 Iter 21100 [Train]: loss=70.19, epe=1.54, lr=0.000010, samples/sec=7.8, sec/step=1.029, eta=2 days, 3:07:22\n",
      "2019-04-24 18:45:17 Iter 21200 [Train]: loss=65.48, epe=1.42, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 2:45:50\n",
      "2019-04-24 18:47:03 Iter 21300 [Train]: loss=66.04, epe=1.42, lr=0.000010, samples/sec=7.8, sec/step=1.026, eta=2 days, 2:55:52\n",
      "2019-04-24 18:48:49 Iter 21400 [Train]: loss=62.76, epe=1.36, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 2:49:23\n",
      "2019-04-24 18:50:34 Iter 21500 [Train]: loss=70.38, epe=1.54, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 2:41:42\n",
      "2019-04-24 18:52:20 Iter 21600 [Train]: loss=67.20, epe=1.46, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 2:44:45\n",
      "2019-04-24 18:54:06 Iter 21700 [Train]: loss=66.12, epe=1.44, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 2:35:18\n",
      "2019-04-24 18:55:51 Iter 21800 [Train]: loss=65.57, epe=1.42, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 2:33:32\n",
      "2019-04-24 18:57:37 Iter 21900 [Train]: loss=67.37, epe=1.46, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 2:34:53\n",
      "2019-04-24 18:59:23 Iter 22000 [Train]: loss=62.04, epe=1.34, lr=0.000010, samples/sec=7.7, sec/step=1.036, eta=2 days, 3:13:03\n",
      "2019-04-24 18:59:25 Iter 22000 [Val]: loss=68.67, epe=1.69\n",
      "Saving model...\n",
      "... model wasn't saved -- its score (1.69) doesn't outperform other checkpoints\n",
      "2019-04-24 19:01:16 Iter 22100 [Train]: loss=64.77, epe=1.38, lr=0.000010, samples/sec=7.8, sec/step=1.030, eta=2 days, 2:53:24\n",
      "2019-04-24 19:03:01 Iter 22200 [Train]: loss=66.49, epe=1.44, lr=0.000010, samples/sec=7.9, sec/step=1.015, eta=2 days, 2:06:29\n",
      "2019-04-24 19:04:46 Iter 22300 [Train]: loss=63.77, epe=1.36, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 2:22:38\n",
      "2019-04-24 19:06:32 Iter 22400 [Train]: loss=66.12, epe=1.44, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 2:32:31\n",
      "2019-04-24 19:08:17 Iter 22500 [Train]: loss=67.69, epe=1.48, lr=0.000010, samples/sec=7.9, sec/step=1.018, eta=2 days, 2:10:35\n",
      "2019-04-24 19:10:03 Iter 22600 [Train]: loss=68.22, epe=1.50, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 2:27:20\n",
      "2019-04-24 19:11:49 Iter 22700 [Train]: loss=64.07, epe=1.37, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 2:27:36\n",
      "2019-04-24 19:13:34 Iter 22800 [Train]: loss=67.43, epe=1.45, lr=0.000010, samples/sec=7.8, sec/step=1.019, eta=2 days, 2:10:49\n",
      "2019-04-24 19:15:20 Iter 22900 [Train]: loss=63.39, epe=1.37, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 2:26:22\n",
      "2019-04-24 19:17:04 Iter 23000 [Train]: loss=63.67, epe=1.37, lr=0.000010, samples/sec=7.9, sec/step=1.010, eta=2 days, 1:40:57\n",
      "2019-04-24 19:17:06 Iter 23000 [Val]: loss=55.47, epe=1.25\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-23000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-23000\n",
      "2019-04-24 19:19:00 Iter 23100 [Train]: loss=68.75, epe=1.48, lr=0.000010, samples/sec=7.8, sec/step=1.026, eta=2 days, 2:25:18\n",
      "2019-04-24 19:20:45 Iter 23200 [Train]: loss=62.86, epe=1.35, lr=0.000010, samples/sec=7.8, sec/step=1.020, eta=2 days, 2:04:43\n",
      "2019-04-24 19:22:31 Iter 23300 [Train]: loss=68.37, epe=1.50, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 2:10:42\n",
      "2019-04-24 19:24:17 Iter 23400 [Train]: loss=62.76, epe=1.35, lr=0.000010, samples/sec=7.8, sec/step=1.026, eta=2 days, 2:20:40\n",
      "2019-04-24 19:26:03 Iter 23500 [Train]: loss=66.78, epe=1.45, lr=0.000010, samples/sec=7.8, sec/step=1.026, eta=2 days, 2:18:02\n",
      "2019-04-24 19:27:48 Iter 23600 [Train]: loss=66.86, epe=1.47, lr=0.000010, samples/sec=7.8, sec/step=1.023, eta=2 days, 2:06:19\n",
      "2019-04-24 19:29:34 Iter 23700 [Train]: loss=63.93, epe=1.39, lr=0.000010, samples/sec=7.8, sec/step=1.027, eta=2 days, 2:16:21\n",
      "2019-04-24 19:31:20 Iter 23800 [Train]: loss=65.92, epe=1.42, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 2:06:41\n",
      "2019-04-24 19:33:05 Iter 23900 [Train]: loss=64.37, epe=1.39, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 1:55:29\n",
      "2019-04-24 19:34:51 Iter 24000 [Train]: loss=66.39, epe=1.43, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 1:54:22\n",
      "2019-04-24 19:34:52 Iter 24000 [Val]: loss=60.08, epe=1.41\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-24000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-24000\n",
      "2019-04-24 19:36:47 Iter 24100 [Train]: loss=67.34, epe=1.50, lr=0.000010, samples/sec=7.8, sec/step=1.026, eta=2 days, 2:08:07\n",
      "2019-04-24 19:38:33 Iter 24200 [Train]: loss=63.21, epe=1.36, lr=0.000010, samples/sec=7.8, sec/step=1.028, eta=2 days, 2:10:49\n",
      "2019-04-24 19:40:18 Iter 24300 [Train]: loss=63.59, epe=1.37, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 1:57:38\n",
      "2019-04-24 19:42:03 Iter 24400 [Train]: loss=65.94, epe=1.43, lr=0.000010, samples/sec=7.9, sec/step=1.016, eta=2 days, 1:34:46\n",
      "2019-04-24 19:43:49 Iter 24500 [Train]: loss=62.74, epe=1.34, lr=0.000010, samples/sec=7.8, sec/step=1.020, eta=2 days, 1:44:06\n",
      "2019-04-24 19:45:34 Iter 24600 [Train]: loss=63.41, epe=1.36, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 1:45:51\n",
      "2019-04-24 19:47:20 Iter 24700 [Train]: loss=64.26, epe=1.38, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 1:54:31\n",
      "2019-04-24 19:49:05 Iter 24800 [Train]: loss=65.05, epe=1.40, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 1:40:02\n",
      "2019-04-24 19:50:51 Iter 24900 [Train]: loss=66.17, epe=1.43, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 1:38:47\n",
      "2019-04-24 19:52:36 Iter 25000 [Train]: loss=62.89, epe=1.33, lr=0.000010, samples/sec=7.9, sec/step=1.016, eta=2 days, 1:24:28\n",
      "2019-04-24 19:52:37 Iter 25000 [Val]: loss=55.02, epe=1.25\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-25000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-25000\n",
      "2019-04-24 19:54:32 Iter 25100 [Train]: loss=63.70, epe=1.37, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 1:39:11\n",
      "2019-04-24 19:56:17 Iter 25200 [Train]: loss=63.61, epe=1.36, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 1:42:09\n",
      "2019-04-24 19:58:03 Iter 25300 [Train]: loss=59.56, epe=1.26, lr=0.000010, samples/sec=7.8, sec/step=1.023, eta=2 days, 1:37:14\n",
      "2019-04-24 19:59:49 Iter 25400 [Train]: loss=72.01, epe=1.56, lr=0.000010, samples/sec=7.8, sec/step=1.026, eta=2 days, 1:46:35\n",
      "2019-04-24 20:01:34 Iter 25500 [Train]: loss=59.69, epe=1.26, lr=0.000010, samples/sec=7.8, sec/step=1.020, eta=2 days, 1:25:56\n",
      "2019-04-24 20:03:20 Iter 25600 [Train]: loss=67.96, epe=1.49, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 1:37:51\n",
      "2019-04-24 20:05:06 Iter 25700 [Train]: loss=61.99, epe=1.30, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 1:37:57\n",
      "2019-04-24 20:06:52 Iter 25800 [Train]: loss=65.70, epe=1.43, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 1:34:39\n",
      "2019-04-24 20:08:38 Iter 25900 [Train]: loss=63.58, epe=1.36, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 1:33:18\n",
      "2019-04-24 20:10:23 Iter 26000 [Train]: loss=64.50, epe=1.38, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 1:29:50\n",
      "2019-04-24 20:10:25 Iter 26000 [Val]: loss=52.71, epe=1.15\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-26000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-26000\n",
      "2019-04-24 20:12:19 Iter 26100 [Train]: loss=66.93, epe=1.46, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 1:28:03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-24 20:14:04 Iter 26200 [Train]: loss=63.49, epe=1.37, lr=0.000010, samples/sec=7.8, sec/step=1.020, eta=2 days, 1:13:43\n",
      "2019-04-24 20:15:50 Iter 26300 [Train]: loss=63.26, epe=1.37, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 1:25:09\n",
      "2019-04-24 20:17:36 Iter 26400 [Train]: loss=65.74, epe=1.41, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 1:23:04\n",
      "2019-04-24 20:19:22 Iter 26500 [Train]: loss=63.48, epe=1.36, lr=0.000010, samples/sec=7.8, sec/step=1.023, eta=2 days, 1:17:26\n",
      "2019-04-24 20:21:07 Iter 26600 [Train]: loss=65.74, epe=1.43, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 1:23:31\n",
      "2019-04-24 20:22:53 Iter 26700 [Train]: loss=57.51, epe=1.20, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 1:12:01\n",
      "2019-04-24 20:24:39 Iter 26800 [Train]: loss=70.97, epe=1.54, lr=0.000010, samples/sec=7.8, sec/step=1.027, eta=2 days, 1:24:27\n",
      "2019-04-24 20:26:25 Iter 26900 [Train]: loss=63.61, epe=1.37, lr=0.000010, samples/sec=7.8, sec/step=1.021, eta=2 days, 1:05:04\n",
      "2019-04-24 20:28:10 Iter 27000 [Train]: loss=61.25, epe=1.32, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 1:14:05\n",
      "2019-04-24 20:28:11 Iter 27000 [Val]: loss=49.34, epe=1.06\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-27000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-27000\n",
      "2019-04-24 20:30:06 Iter 27100 [Train]: loss=62.75, epe=1.35, lr=0.000010, samples/sec=7.8, sec/step=1.020, eta=2 days, 0:58:36\n",
      "2019-04-24 20:31:52 Iter 27200 [Train]: loss=63.38, epe=1.37, lr=0.000010, samples/sec=7.8, sec/step=1.030, eta=2 days, 1:26:48\n",
      "2019-04-24 20:33:38 Iter 27300 [Train]: loss=63.08, epe=1.34, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 1:06:37\n",
      "2019-04-24 20:35:23 Iter 27400 [Train]: loss=62.83, epe=1.35, lr=0.000010, samples/sec=7.9, sec/step=1.017, eta=2 days, 0:46:11\n",
      "2019-04-24 20:37:09 Iter 27500 [Train]: loss=65.00, epe=1.40, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 1:03:41\n",
      "2019-04-24 20:38:54 Iter 27600 [Train]: loss=62.43, epe=1.34, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 1:02:18\n",
      "2019-04-24 20:40:39 Iter 27700 [Train]: loss=64.71, epe=1.38, lr=0.000010, samples/sec=7.9, sec/step=1.014, eta=2 days, 0:32:30\n",
      "2019-04-24 20:42:24 Iter 27800 [Train]: loss=63.54, epe=1.36, lr=0.000010, samples/sec=7.9, sec/step=1.015, eta=2 days, 0:31:52\n",
      "2019-04-24 20:44:09 Iter 27900 [Train]: loss=59.91, epe=1.26, lr=0.000010, samples/sec=7.8, sec/step=1.022, eta=2 days, 0:51:31\n",
      "2019-04-24 20:45:54 Iter 28000 [Train]: loss=66.65, epe=1.45, lr=0.000010, samples/sec=7.9, sec/step=1.016, eta=2 days, 0:32:07\n",
      "2019-04-24 20:45:56 Iter 28000 [Val]: loss=57.04, epe=1.28\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-28000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-28000\n",
      "2019-04-24 20:47:50 Iter 28100 [Train]: loss=64.08, epe=1.38, lr=0.000010, samples/sec=7.8, sec/step=1.025, eta=2 days, 0:56:33\n",
      "2019-04-24 20:49:36 Iter 28200 [Train]: loss=66.93, epe=1.46, lr=0.000010, samples/sec=7.8, sec/step=1.024, eta=2 days, 0:53:02\n",
      "2019-04-24 20:51:26 Iter 28300 [Train]: loss=61.58, epe=1.31, lr=0.000010, samples/sec=7.6, sec/step=1.059, eta=2 days, 2:29:30\n",
      "2019-04-24 20:53:18 Iter 28400 [Train]: loss=64.72, epe=1.38, lr=0.000010, samples/sec=7.5, sec/step=1.065, eta=2 days, 2:44:44\n",
      "2019-04-24 20:55:17 Iter 28500 [Train]: loss=59.76, epe=1.28, lr=0.000010, samples/sec=7.3, sec/step=1.089, eta=2 days, 3:52:21\n",
      "2019-04-24 20:57:07 Iter 28600 [Train]: loss=61.84, epe=1.31, lr=0.000010, samples/sec=7.6, sec/step=1.054, eta=2 days, 2:09:56\n",
      "2019-04-24 20:58:57 Iter 28700 [Train]: loss=62.05, epe=1.32, lr=0.000010, samples/sec=7.6, sec/step=1.056, eta=2 days, 2:13:44\n",
      "2019-04-24 21:00:47 Iter 28800 [Train]: loss=67.01, epe=1.44, lr=0.000010, samples/sec=7.5, sec/step=1.061, eta=2 days, 2:28:00\n",
      "2019-04-24 21:02:38 Iter 28900 [Train]: loss=63.65, epe=1.38, lr=0.000010, samples/sec=7.5, sec/step=1.067, eta=2 days, 2:42:37\n",
      "2019-04-24 21:04:28 Iter 29000 [Train]: loss=61.42, epe=1.31, lr=0.000010, samples/sec=7.6, sec/step=1.055, eta=2 days, 2:06:13\n",
      "2019-04-24 21:04:29 Iter 29000 [Val]: loss=54.15, epe=1.21\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-29000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-29000\n",
      "2019-04-24 21:06:31 Iter 29100 [Train]: loss=63.37, epe=1.34, lr=0.000010, samples/sec=7.6, sec/step=1.057, eta=2 days, 2:09:35\n",
      "2019-04-24 21:08:20 Iter 29200 [Train]: loss=58.75, epe=1.25, lr=0.000010, samples/sec=7.6, sec/step=1.055, eta=2 days, 2:03:53\n",
      "2019-04-24 21:10:11 Iter 29300 [Train]: loss=64.79, epe=1.40, lr=0.000010, samples/sec=7.5, sec/step=1.067, eta=2 days, 2:35:36\n",
      "2019-04-24 21:12:01 Iter 29400 [Train]: loss=60.90, epe=1.30, lr=0.000010, samples/sec=7.6, sec/step=1.059, eta=2 days, 2:10:14\n",
      "2019-04-24 21:13:50 Iter 29500 [Train]: loss=62.49, epe=1.33, lr=0.000010, samples/sec=7.6, sec/step=1.047, eta=2 days, 1:35:57\n",
      "2019-04-24 21:15:40 Iter 29600 [Train]: loss=68.15, epe=1.49, lr=0.000010, samples/sec=7.5, sec/step=1.063, eta=2 days, 2:19:33\n",
      "2019-04-24 21:17:30 Iter 29700 [Train]: loss=58.33, epe=1.22, lr=0.000010, samples/sec=7.6, sec/step=1.056, eta=2 days, 1:57:08\n",
      "2019-04-24 21:19:23 Iter 29800 [Train]: loss=64.39, epe=1.40, lr=0.000010, samples/sec=7.4, sec/step=1.082, eta=2 days, 3:09:57\n",
      "2019-04-24 21:21:12 Iter 29900 [Train]: loss=64.39, epe=1.36, lr=0.000010, samples/sec=7.6, sec/step=1.052, eta=2 days, 1:43:36\n",
      "2019-04-24 21:23:01 Iter 30000 [Train]: loss=61.37, epe=1.31, lr=0.000010, samples/sec=7.6, sec/step=1.055, eta=2 days, 1:48:30\n",
      "2019-04-24 21:23:03 Iter 30000 [Val]: loss=65.93, epe=1.63\n",
      "Saving model...\n",
      "... model wasn't saved -- its score (1.63) doesn't outperform other checkpoints\n",
      "2019-04-24 21:25:00 Iter 30100 [Train]: loss=61.26, epe=1.30, lr=0.000010, samples/sec=7.6, sec/step=1.053, eta=2 days, 1:40:36\n",
      "2019-04-24 21:26:50 Iter 30200 [Train]: loss=66.76, epe=1.45, lr=0.000010, samples/sec=7.6, sec/step=1.051, eta=2 days, 1:33:23\n",
      "2019-04-24 21:28:39 Iter 30300 [Train]: loss=62.61, epe=1.35, lr=0.000010, samples/sec=7.6, sec/step=1.051, eta=2 days, 1:32:54\n",
      "2019-04-24 21:30:29 Iter 30400 [Train]: loss=64.11, epe=1.36, lr=0.000010, samples/sec=7.5, sec/step=1.060, eta=2 days, 1:56:33\n",
      "2019-04-24 21:32:19 Iter 30500 [Train]: loss=60.76, epe=1.28, lr=0.000010, samples/sec=7.6, sec/step=1.058, eta=2 days, 1:48:38\n",
      "2019-04-24 21:34:09 Iter 30600 [Train]: loss=65.13, epe=1.42, lr=0.000010, samples/sec=7.6, sec/step=1.056, eta=2 days, 1:42:28\n",
      "2019-04-24 21:35:59 Iter 30700 [Train]: loss=60.87, epe=1.29, lr=0.000010, samples/sec=7.6, sec/step=1.059, eta=2 days, 1:48:40\n",
      "2019-04-24 21:37:51 Iter 30800 [Train]: loss=60.28, epe=1.28, lr=0.000010, samples/sec=7.5, sec/step=1.073, eta=2 days, 2:27:10\n",
      "2019-04-24 21:39:41 Iter 30900 [Train]: loss=64.76, epe=1.39, lr=0.000010, samples/sec=7.6, sec/step=1.056, eta=2 days, 1:36:29\n",
      "2019-04-24 21:41:30 Iter 31000 [Train]: loss=61.56, epe=1.31, lr=0.000010, samples/sec=7.6, sec/step=1.056, eta=2 days, 1:33:39\n",
      "2019-04-24 21:41:32 Iter 31000 [Val]: loss=59.01, epe=1.31\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-31000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-31000\n",
      "2019-04-24 21:43:34 Iter 31100 [Train]: loss=62.84, epe=1.35, lr=0.000010, samples/sec=7.5, sec/step=1.064, eta=2 days, 1:54:37\n",
      "2019-04-24 21:45:26 Iter 31200 [Train]: loss=60.60, epe=1.30, lr=0.000010, samples/sec=7.5, sec/step=1.072, eta=2 days, 2:17:17\n",
      "2019-04-24 21:47:18 Iter 31300 [Train]: loss=62.90, epe=1.33, lr=0.000010, samples/sec=7.5, sec/step=1.068, eta=2 days, 2:02:36\n",
      "2019-04-24 21:49:07 Iter 31400 [Train]: loss=63.28, epe=1.36, lr=0.000010, samples/sec=7.6, sec/step=1.054, eta=2 days, 1:20:57\n",
      "2019-04-24 21:50:57 Iter 31500 [Train]: loss=62.32, epe=1.34, lr=0.000010, samples/sec=7.6, sec/step=1.058, eta=2 days, 1:31:41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-24 21:52:48 Iter 31600 [Train]: loss=61.82, epe=1.31, lr=0.000010, samples/sec=7.5, sec/step=1.062, eta=2 days, 1:40:30\n",
      "2019-04-24 21:54:40 Iter 31700 [Train]: loss=61.30, epe=1.32, lr=0.000010, samples/sec=7.4, sec/step=1.075, eta=2 days, 2:15:45\n",
      "2019-04-24 21:56:30 Iter 31800 [Train]: loss=64.75, epe=1.40, lr=0.000010, samples/sec=7.6, sec/step=1.054, eta=2 days, 1:15:54\n",
      "2019-04-24 21:58:20 Iter 31900 [Train]: loss=63.29, epe=1.35, lr=0.000010, samples/sec=7.5, sec/step=1.067, eta=2 days, 1:48:52\n",
      "2019-04-24 22:00:01 Iter 32000 [Train]: loss=61.55, epe=1.30, lr=0.000010, samples/sec=8.2, sec/step=0.971, eta=1 day, 21:18:36\n",
      "2019-04-24 22:00:02 Iter 32000 [Val]: loss=52.63, epe=1.18\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-32000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-32000\n",
      "2019-04-24 22:01:46 Iter 32100 [Train]: loss=59.82, epe=1.27, lr=0.000010, samples/sec=8.9, sec/step=0.904, eta=1 day, 18:09:09\n",
      "2019-04-24 22:03:20 Iter 32200 [Train]: loss=64.85, epe=1.39, lr=0.000010, samples/sec=8.9, sec/step=0.903, eta=1 day, 18:06:44\n",
      "2019-04-24 22:04:54 Iter 32300 [Train]: loss=62.52, epe=1.34, lr=0.000010, samples/sec=8.8, sec/step=0.906, eta=1 day, 18:12:37\n",
      "2019-04-24 22:06:28 Iter 32400 [Train]: loss=60.43, epe=1.27, lr=0.000010, samples/sec=8.8, sec/step=0.909, eta=1 day, 18:20:02\n",
      "2019-04-24 22:08:02 Iter 32500 [Train]: loss=60.65, epe=1.29, lr=0.000010, samples/sec=8.8, sec/step=0.908, eta=1 day, 18:14:27\n",
      "2019-04-24 22:09:36 Iter 32600 [Train]: loss=64.39, epe=1.40, lr=0.000010, samples/sec=8.9, sec/step=0.904, eta=1 day, 18:01:48\n",
      "2019-04-24 22:11:11 Iter 32700 [Train]: loss=65.63, epe=1.42, lr=0.000010, samples/sec=8.8, sec/step=0.907, eta=1 day, 18:09:28\n",
      "2019-04-24 22:12:45 Iter 32800 [Train]: loss=57.79, epe=1.22, lr=0.000010, samples/sec=8.8, sec/step=0.906, eta=1 day, 18:04:09\n",
      "2019-04-24 22:15:27 Iter 32900 [Train]: loss=57.73, epe=1.21, lr=0.000010, samples/sec=5.8, sec/step=1.385, eta=2 days, 16:18:27\n",
      "2019-04-24 22:19:42 Iter 33000 [Train]: loss=66.05, epe=1.43, lr=0.000010, samples/sec=3.9, sec/step=2.073, eta=4 days, 0:10:52\n",
      "2019-04-24 22:19:45 Iter 33000 [Val]: loss=56.31, epe=1.25\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-33000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-33000\n",
      "2019-04-24 22:24:26 Iter 33100 [Train]: loss=62.82, epe=1.32, lr=0.000010, samples/sec=3.8, sec/step=2.129, eta=4 days, 2:41:40\n",
      "2019-04-24 22:28:44 Iter 33200 [Train]: loss=62.19, epe=1.35, lr=0.000010, samples/sec=3.8, sec/step=2.105, eta=4 days, 1:30:48\n",
      "2019-04-24 22:32:56 Iter 33300 [Train]: loss=62.05, epe=1.32, lr=0.000010, samples/sec=3.8, sec/step=2.094, eta=4 days, 0:58:03\n",
      "2019-04-24 22:37:17 Iter 33400 [Train]: loss=62.78, epe=1.35, lr=0.000010, samples/sec=3.8, sec/step=2.113, eta=4 days, 1:48:26\n",
      "2019-04-24 22:41:50 Iter 33500 [Train]: loss=60.46, epe=1.28, lr=0.000010, samples/sec=3.7, sec/step=2.177, eta=4 days, 4:40:13\n",
      "2019-04-24 22:46:12 Iter 33600 [Train]: loss=60.37, epe=1.29, lr=0.000010, samples/sec=3.6, sec/step=2.198, eta=4 days, 5:35:13\n",
      "2019-04-24 22:50:21 Iter 33700 [Train]: loss=67.81, epe=1.46, lr=0.000010, samples/sec=3.9, sec/step=2.031, eta=3 days, 21:49:55\n",
      "2019-04-24 22:54:37 Iter 33800 [Train]: loss=58.72, epe=1.24, lr=0.000010, samples/sec=3.8, sec/step=2.105, eta=4 days, 1:09:37\n",
      "2019-04-24 22:58:57 Iter 33900 [Train]: loss=65.04, epe=1.39, lr=0.000010, samples/sec=3.7, sec/step=2.136, eta=4 days, 2:33:30\n",
      "2019-04-24 23:03:17 Iter 34000 [Train]: loss=58.48, epe=1.22, lr=0.000010, samples/sec=3.8, sec/step=2.132, eta=4 days, 2:18:36\n",
      "2019-04-24 23:03:20 Iter 34000 [Val]: loss=55.03, epe=1.23\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-34000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-34000\n",
      "2019-04-24 23:08:00 Iter 34100 [Train]: loss=60.90, epe=1.28, lr=0.000010, samples/sec=3.8, sec/step=2.113, eta=4 days, 1:21:17\n",
      "2019-04-24 23:12:26 Iter 34200 [Train]: loss=60.90, epe=1.30, lr=0.000010, samples/sec=3.6, sec/step=2.195, eta=4 days, 5:04:21\n",
      "2019-04-24 23:16:42 Iter 34300 [Train]: loss=62.95, epe=1.35, lr=0.000010, samples/sec=3.7, sec/step=2.144, eta=4 days, 2:40:07\n",
      "2019-04-24 23:21:16 Iter 34400 [Train]: loss=58.76, epe=1.23, lr=0.000010, samples/sec=3.6, sec/step=2.242, eta=4 days, 7:07:07\n",
      "2019-04-24 23:25:37 Iter 34500 [Train]: loss=64.42, epe=1.37, lr=0.000010, samples/sec=3.7, sec/step=2.189, eta=4 days, 4:38:49\n",
      "2019-04-24 23:30:01 Iter 34600 [Train]: loss=64.01, epe=1.37, lr=0.000010, samples/sec=3.7, sec/step=2.185, eta=4 days, 4:22:02\n",
      "2019-04-24 23:34:28 Iter 34700 [Train]: loss=57.40, epe=1.22, lr=0.000010, samples/sec=3.7, sec/step=2.171, eta=4 days, 3:41:30\n",
      "2019-04-24 23:38:49 Iter 34800 [Train]: loss=60.91, epe=1.28, lr=0.000010, samples/sec=3.7, sec/step=2.162, eta=4 days, 3:12:19\n",
      "2019-04-24 23:43:04 Iter 34900 [Train]: loss=60.70, epe=1.29, lr=0.000010, samples/sec=3.8, sec/step=2.092, eta=3 days, 23:56:12\n",
      "2019-04-24 23:47:19 Iter 35000 [Train]: loss=64.88, epe=1.41, lr=0.000010, samples/sec=3.7, sec/step=2.135, eta=4 days, 1:50:58\n",
      "2019-04-24 23:47:22 Iter 35000 [Val]: loss=55.53, epe=1.24\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-35000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-35000\n",
      "2019-04-24 23:52:17 Iter 35100 [Train]: loss=57.75, epe=1.23, lr=0.000010, samples/sec=3.8, sec/step=2.117, eta=4 days, 0:58:36\n",
      "2019-04-24 23:56:43 Iter 35200 [Train]: loss=61.45, epe=1.30, lr=0.000010, samples/sec=3.6, sec/step=2.194, eta=4 days, 4:27:18\n",
      "2019-04-25 00:00:56 Iter 35300 [Train]: loss=65.25, epe=1.40, lr=0.000010, samples/sec=3.8, sec/step=2.087, eta=3 days, 23:28:45\n",
      "2019-04-25 00:05:28 Iter 35400 [Train]: loss=56.25, epe=1.18, lr=0.000010, samples/sec=3.7, sec/step=2.143, eta=4 days, 1:59:33\n",
      "2019-04-25 00:09:47 Iter 35500 [Train]: loss=61.11, epe=1.29, lr=0.000010, samples/sec=3.8, sec/step=2.080, eta=3 days, 23:01:30\n",
      "2019-04-25 00:14:21 Iter 35600 [Train]: loss=65.05, epe=1.40, lr=0.000010, samples/sec=3.6, sec/step=2.238, eta=4 days, 6:13:24\n",
      "2019-04-25 00:18:44 Iter 35700 [Train]: loss=60.09, epe=1.26, lr=0.000010, samples/sec=3.8, sec/step=2.125, eta=4 days, 0:58:52\n",
      "2019-04-25 00:23:10 Iter 35800 [Train]: loss=59.59, epe=1.26, lr=0.000010, samples/sec=3.7, sec/step=2.166, eta=4 days, 2:48:47\n",
      "2019-04-25 00:27:38 Iter 35900 [Train]: loss=62.37, epe=1.34, lr=0.000010, samples/sec=3.6, sec/step=2.230, eta=4 days, 5:38:39\n",
      "2019-04-25 00:32:00 Iter 36000 [Train]: loss=58.34, epe=1.22, lr=0.000010, samples/sec=3.7, sec/step=2.188, eta=4 days, 3:39:39\n",
      "2019-04-25 00:32:03 Iter 36000 [Val]: loss=52.11, epe=1.14\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-36000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-36000\n",
      "2019-04-25 00:36:56 Iter 36100 [Train]: loss=62.75, epe=1.33, lr=0.000010, samples/sec=3.7, sec/step=2.157, eta=4 days, 2:13:27\n",
      "2019-04-25 00:41:08 Iter 36200 [Train]: loss=59.11, epe=1.24, lr=0.000010, samples/sec=3.9, sec/step=2.077, eta=3 days, 22:30:19\n",
      "2019-04-25 00:45:32 Iter 36300 [Train]: loss=63.02, epe=1.35, lr=0.000010, samples/sec=3.9, sec/step=2.072, eta=3 days, 22:13:54\n",
      "2019-04-25 00:50:06 Iter 36400 [Train]: loss=60.19, epe=1.27, lr=0.000010, samples/sec=3.7, sec/step=2.177, eta=4 days, 2:54:43\n",
      "2019-04-25 00:54:37 Iter 36500 [Train]: loss=60.12, epe=1.29, lr=0.000010, samples/sec=3.7, sec/step=2.190, eta=4 days, 3:27:31\n",
      "2019-04-25 00:59:02 Iter 36600 [Train]: loss=62.44, epe=1.33, lr=0.000010, samples/sec=3.7, sec/step=2.139, eta=4 days, 1:05:49\n",
      "2019-04-25 01:03:30 Iter 36700 [Train]: loss=60.83, epe=1.29, lr=0.000010, samples/sec=3.7, sec/step=2.147, eta=4 days, 1:23:15\n",
      "2019-04-25 01:08:12 Iter 36800 [Train]: loss=61.03, epe=1.30, lr=0.000010, samples/sec=3.5, sec/step=2.271, eta=4 days, 6:58:22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-25 01:12:31 Iter 36900 [Train]: loss=58.99, epe=1.23, lr=0.000010, samples/sec=3.9, sec/step=2.062, eta=3 days, 21:26:26\n",
      "2019-04-25 01:16:58 Iter 37000 [Train]: loss=62.88, epe=1.34, lr=0.000010, samples/sec=3.8, sec/step=2.108, eta=3 days, 23:27:14\n",
      "2019-04-25 01:17:01 Iter 37000 [Val]: loss=61.07, epe=1.44\n",
      "Saving model...\n",
      "... model wasn't saved -- its score (1.44) doesn't outperform other checkpoints\n",
      "2019-04-25 01:21:38 Iter 37100 [Train]: loss=56.03, epe=1.16, lr=0.000010, samples/sec=3.7, sec/step=2.163, eta=4 days, 1:53:35\n",
      "2019-04-25 01:26:14 Iter 37200 [Train]: loss=62.50, epe=1.35, lr=0.000010, samples/sec=3.5, sec/step=2.260, eta=4 days, 6:12:51\n",
      "2019-04-25 01:30:39 Iter 37300 [Train]: loss=62.16, epe=1.33, lr=0.000010, samples/sec=3.8, sec/step=2.116, eta=3 days, 23:38:57\n",
      "2019-04-25 01:35:06 Iter 37400 [Train]: loss=56.99, epe=1.21, lr=0.000010, samples/sec=3.7, sec/step=2.166, eta=4 days, 1:49:06\n",
      "2019-04-25 01:39:37 Iter 37500 [Train]: loss=64.72, epe=1.38, lr=0.000010, samples/sec=3.6, sec/step=2.238, eta=4 days, 5:00:10\n",
      "2019-04-25 01:44:03 Iter 37600 [Train]: loss=57.74, epe=1.21, lr=0.000010, samples/sec=3.7, sec/step=2.172, eta=4 days, 1:57:33\n",
      "2019-04-25 01:48:19 Iter 37700 [Train]: loss=63.04, epe=1.35, lr=0.000010, samples/sec=3.9, sec/step=2.074, eta=3 days, 21:28:54\n",
      "2019-04-25 01:52:34 Iter 37800 [Train]: loss=58.82, epe=1.25, lr=0.000010, samples/sec=3.9, sec/step=2.037, eta=3 days, 19:47:21\n",
      "2019-04-25 01:57:07 Iter 37900 [Train]: loss=62.60, epe=1.34, lr=0.000010, samples/sec=3.6, sec/step=2.204, eta=4 days, 3:15:29\n",
      "2019-04-25 02:01:43 Iter 38000 [Train]: loss=58.46, epe=1.23, lr=0.000010, samples/sec=3.6, sec/step=2.193, eta=4 days, 2:41:13\n",
      "2019-04-25 02:01:48 Iter 38000 [Val]: loss=56.59, epe=1.27\n",
      "Saving model...\n",
      "... model wasn't saved -- its score (1.27) doesn't outperform other checkpoints\n",
      "2019-04-25 02:06:15 Iter 38100 [Train]: loss=60.26, epe=1.28, lr=0.000010, samples/sec=3.9, sec/step=2.063, eta=3 days, 20:46:06\n",
      "2019-04-25 02:10:40 Iter 38200 [Train]: loss=63.10, epe=1.34, lr=0.000010, samples/sec=3.7, sec/step=2.144, eta=4 days, 0:22:31\n",
      "2019-04-25 02:15:10 Iter 38300 [Train]: loss=59.48, epe=1.27, lr=0.000010, samples/sec=3.7, sec/step=2.143, eta=4 days, 0:15:08\n",
      "2019-04-25 02:19:29 Iter 38400 [Train]: loss=60.53, epe=1.28, lr=0.000010, samples/sec=3.9, sec/step=2.045, eta=3 days, 19:48:59\n",
      "2019-04-25 02:23:54 Iter 38500 [Train]: loss=61.56, epe=1.30, lr=0.000010, samples/sec=3.8, sec/step=2.118, eta=3 days, 23:00:39\n",
      "2019-04-25 02:28:02 Iter 38600 [Train]: loss=58.69, epe=1.25, lr=0.000010, samples/sec=4.0, sec/step=2.011, eta=3 days, 18:09:08\n",
      "2019-04-25 02:31:54 Iter 38700 [Train]: loss=63.31, epe=1.34, lr=0.000010, samples/sec=4.2, sec/step=1.885, eta=3 days, 12:28:12\n",
      "2019-04-25 02:36:02 Iter 38800 [Train]: loss=58.05, epe=1.23, lr=0.000010, samples/sec=4.0, sec/step=2.005, eta=3 days, 17:46:27\n",
      "2019-04-25 02:39:55 Iter 38900 [Train]: loss=58.23, epe=1.25, lr=0.000010, samples/sec=4.1, sec/step=1.932, eta=3 days, 14:26:25\n",
      "2019-04-25 02:43:52 Iter 39000 [Train]: loss=65.14, epe=1.39, lr=0.000010, samples/sec=4.1, sec/step=1.962, eta=3 days, 15:45:29\n",
      "2019-04-25 02:43:54 Iter 39000 [Val]: loss=53.04, epe=1.18\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-39000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-39000\n",
      "2019-04-25 02:48:08 Iter 39100 [Train]: loss=56.82, epe=1.19, lr=0.000010, samples/sec=4.2, sec/step=1.897, eta=3 days, 12:46:13\n",
      "2019-04-25 02:52:08 Iter 39200 [Train]: loss=61.52, epe=1.31, lr=0.000010, samples/sec=4.0, sec/step=2.000, eta=3 days, 17:21:09\n",
      "2019-04-25 02:56:16 Iter 39300 [Train]: loss=58.36, epe=1.21, lr=0.000010, samples/sec=3.9, sec/step=2.029, eta=3 days, 18:33:06\n",
      "2019-04-25 03:00:14 Iter 39400 [Train]: loss=63.09, epe=1.37, lr=0.000010, samples/sec=4.1, sec/step=1.945, eta=3 days, 14:46:17\n",
      "2019-04-25 03:03:52 Iter 39500 [Train]: loss=58.59, epe=1.22, lr=0.000010, samples/sec=4.4, sec/step=1.823, eta=3 days, 9:17:38\n",
      "2019-04-25 03:07:36 Iter 39600 [Train]: loss=61.33, epe=1.31, lr=0.000010, samples/sec=4.2, sec/step=1.883, eta=3 days, 11:54:56\n",
      "2019-04-25 03:11:18 Iter 39700 [Train]: loss=58.19, epe=1.23, lr=0.000010, samples/sec=4.3, sec/step=1.867, eta=3 days, 11:07:17\n",
      "2019-04-25 03:14:54 Iter 39800 [Train]: loss=64.50, epe=1.36, lr=0.000010, samples/sec=4.4, sec/step=1.816, eta=3 days, 8:49:19\n",
      "2019-04-25 03:18:29 Iter 39900 [Train]: loss=58.11, epe=1.24, lr=0.000010, samples/sec=4.5, sec/step=1.780, eta=3 days, 7:09:54\n",
      "2019-04-25 03:22:11 Iter 40000 [Train]: loss=59.29, epe=1.25, lr=0.000010, samples/sec=4.3, sec/step=1.859, eta=3 days, 10:36:58\n",
      "2019-04-25 03:22:14 Iter 40000 [Val]: loss=57.42, epe=1.32\n",
      "Saving model...\n",
      "... model wasn't saved -- its score (1.32) doesn't outperform other checkpoints\n",
      "2019-04-25 03:26:11 Iter 40100 [Train]: loss=59.00, epe=1.24, lr=0.000010, samples/sec=4.3, sec/step=1.840, eta=3 days, 9:44:33\n",
      "2019-04-25 03:29:36 Iter 40200 [Train]: loss=64.60, epe=1.38, lr=0.000010, samples/sec=4.6, sec/step=1.738, eta=3 days, 5:08:20\n",
      "2019-04-25 03:32:52 Iter 40300 [Train]: loss=56.80, epe=1.19, lr=0.000010, samples/sec=4.6, sec/step=1.722, eta=3 days, 4:24:00\n",
      "2019-04-25 03:36:09 Iter 40400 [Train]: loss=58.48, epe=1.23, lr=0.000010, samples/sec=4.7, sec/step=1.685, eta=3 days, 2:40:52\n",
      "2019-04-25 03:39:25 Iter 40500 [Train]: loss=61.38, epe=1.30, lr=0.000010, samples/sec=4.8, sec/step=1.660, eta=3 days, 1:32:48\n",
      "2019-04-25 03:42:45 Iter 40600 [Train]: loss=62.08, epe=1.33, lr=0.000010, samples/sec=4.7, sec/step=1.701, eta=3 days, 3:19:04\n",
      "2019-04-25 03:46:01 Iter 40700 [Train]: loss=58.33, epe=1.21, lr=0.000010, samples/sec=4.7, sec/step=1.712, eta=3 days, 3:45:45\n",
      "2019-04-25 03:49:26 Iter 40800 [Train]: loss=61.89, epe=1.33, lr=0.000010, samples/sec=4.6, sec/step=1.735, eta=3 days, 4:44:04\n",
      "2019-04-25 03:52:57 Iter 40900 [Train]: loss=59.33, epe=1.23, lr=0.000010, samples/sec=4.6, sec/step=1.750, eta=3 days, 5:21:02\n",
      "2019-04-25 03:56:12 Iter 41000 [Train]: loss=59.30, epe=1.26, lr=0.000010, samples/sec=4.8, sec/step=1.672, eta=3 days, 1:50:03\n",
      "2019-04-25 03:56:14 Iter 41000 [Val]: loss=53.83, epe=1.18\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-41000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-41000\n",
      "2019-04-25 03:59:52 Iter 41100 [Train]: loss=61.29, epe=1.29, lr=0.000010, samples/sec=4.8, sec/step=1.674, eta=3 days, 1:53:07\n",
      "2019-04-25 04:03:12 Iter 41200 [Train]: loss=60.29, epe=1.28, lr=0.000010, samples/sec=4.8, sec/step=1.683, eta=3 days, 2:14:29\n",
      "2019-04-25 04:06:36 Iter 41300 [Train]: loss=61.28, epe=1.31, lr=0.000010, samples/sec=4.6, sec/step=1.727, eta=3 days, 4:07:11\n",
      "2019-04-25 04:09:51 Iter 41400 [Train]: loss=57.23, epe=1.20, lr=0.000010, samples/sec=4.8, sec/step=1.657, eta=3 days, 1:00:41\n",
      "2019-04-25 04:13:02 Iter 41500 [Train]: loss=59.92, epe=1.25, lr=0.000010, samples/sec=4.8, sec/step=1.667, eta=3 days, 1:22:54\n",
      "2019-04-25 04:16:21 Iter 41600 [Train]: loss=61.27, epe=1.30, lr=0.000010, samples/sec=4.7, sec/step=1.698, eta=3 days, 2:43:19\n",
      "2019-04-25 04:19:37 Iter 41700 [Train]: loss=59.18, epe=1.25, lr=0.000010, samples/sec=4.8, sec/step=1.667, eta=3 days, 1:17:18\n",
      "2019-04-25 04:22:54 Iter 41800 [Train]: loss=60.91, epe=1.28, lr=0.000010, samples/sec=4.7, sec/step=1.709, eta=3 days, 3:05:40\n",
      "2019-04-25 04:26:07 Iter 41900 [Train]: loss=57.72, epe=1.21, lr=0.000010, samples/sec=4.8, sec/step=1.662, eta=3 days, 1:00:10\n",
      "2019-04-25 04:29:25 Iter 42000 [Train]: loss=59.55, epe=1.25, lr=0.000010, samples/sec=4.8, sec/step=1.672, eta=3 days, 1:22:21\n",
      "2019-04-25 04:29:27 Iter 42000 [Val]: loss=63.19, epe=1.57\n",
      "Saving model...\n",
      "... model wasn't saved -- its score (1.57) doesn't outperform other checkpoints\n",
      "2019-04-25 04:32:54 Iter 42100 [Train]: loss=61.58, epe=1.32, lr=0.000010, samples/sec=4.8, sec/step=1.650, eta=3 days, 0:21:57\n",
      "2019-04-25 04:36:06 Iter 42200 [Train]: loss=58.66, epe=1.24, lr=0.000010, samples/sec=5.0, sec/step=1.600, eta=2 days, 22:08:24\n",
      "2019-04-25 04:39:15 Iter 42300 [Train]: loss=59.48, epe=1.25, lr=0.000010, samples/sec=4.9, sec/step=1.626, eta=2 days, 23:13:22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-25 04:42:27 Iter 42400 [Train]: loss=58.13, epe=1.22, lr=0.000010, samples/sec=4.7, sec/step=1.691, eta=3 days, 2:02:46\n",
      "2019-04-25 04:45:44 Iter 42500 [Train]: loss=61.24, epe=1.31, lr=0.000010, samples/sec=4.8, sec/step=1.680, eta=3 days, 1:29:09\n",
      "2019-04-25 04:48:56 Iter 42600 [Train]: loss=58.65, epe=1.24, lr=0.000010, samples/sec=4.8, sec/step=1.650, eta=3 days, 0:07:23\n",
      "2019-04-25 04:52:14 Iter 42700 [Train]: loss=60.86, epe=1.30, lr=0.000010, samples/sec=4.7, sec/step=1.698, eta=3 days, 2:11:57\n",
      "2019-04-25 04:55:31 Iter 42800 [Train]: loss=60.58, epe=1.28, lr=0.000010, samples/sec=4.7, sec/step=1.695, eta=3 days, 2:01:55\n",
      "2019-04-25 04:58:56 Iter 42900 [Train]: loss=60.70, epe=1.29, lr=0.000010, samples/sec=4.6, sec/step=1.742, eta=3 days, 4:01:30\n",
      "2019-04-25 05:02:14 Iter 43000 [Train]: loss=59.82, epe=1.27, lr=0.000010, samples/sec=4.8, sec/step=1.659, eta=3 days, 0:20:03\n",
      "2019-04-25 05:02:16 Iter 43000 [Val]: loss=68.42, epe=1.70\n",
      "Saving model...\n",
      "... model wasn't saved -- its score (1.70) doesn't outperform other checkpoints\n",
      "2019-04-25 05:05:44 Iter 43100 [Train]: loss=60.70, epe=1.29, lr=0.000010, samples/sec=4.9, sec/step=1.649, eta=2 days, 23:51:34\n",
      "2019-04-25 05:08:57 Iter 43200 [Train]: loss=56.06, epe=1.16, lr=0.000010, samples/sec=4.9, sec/step=1.643, eta=2 days, 23:33:03\n",
      "2019-04-25 05:12:15 Iter 43300 [Train]: loss=61.61, epe=1.29, lr=0.000010, samples/sec=4.8, sec/step=1.666, eta=3 days, 0:30:44\n",
      "2019-04-25 05:15:38 Iter 43400 [Train]: loss=59.79, epe=1.25, lr=0.000010, samples/sec=4.6, sec/step=1.724, eta=3 days, 2:59:36\n",
      "2019-04-25 05:19:00 Iter 43500 [Train]: loss=60.10, epe=1.27, lr=0.000010, samples/sec=4.8, sec/step=1.672, eta=3 days, 0:40:35\n",
      "2019-04-25 05:22:18 Iter 43600 [Train]: loss=61.14, epe=1.30, lr=0.000010, samples/sec=4.8, sec/step=1.661, eta=3 days, 0:10:52\n",
      "2019-04-25 05:25:32 Iter 43700 [Train]: loss=57.24, epe=1.21, lr=0.000010, samples/sec=4.8, sec/step=1.679, eta=3 days, 0:53:16\n",
      "2019-04-25 05:28:49 Iter 43800 [Train]: loss=63.71, epe=1.35, lr=0.000010, samples/sec=4.9, sec/step=1.647, eta=2 days, 23:27:31\n",
      "2019-04-25 05:32:10 Iter 43900 [Train]: loss=57.74, epe=1.22, lr=0.000010, samples/sec=4.6, sec/step=1.742, eta=3 days, 3:32:08\n",
      "2019-04-25 05:35:36 Iter 44000 [Train]: loss=60.93, epe=1.29, lr=0.000010, samples/sec=4.6, sec/step=1.737, eta=3 days, 3:15:35\n",
      "2019-04-25 05:35:38 Iter 44000 [Val]: loss=52.26, epe=1.14\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-44000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-44000\n",
      "2019-04-25 05:39:19 Iter 44100 [Train]: loss=55.14, epe=1.15, lr=0.000010, samples/sec=4.8, sec/step=1.670, eta=3 days, 0:20:10\n",
      "2019-04-25 05:42:31 Iter 44200 [Train]: loss=59.37, epe=1.25, lr=0.000010, samples/sec=4.8, sec/step=1.663, eta=2 days, 23:57:14\n",
      "2019-04-25 05:45:58 Iter 44300 [Train]: loss=61.28, epe=1.28, lr=0.000010, samples/sec=4.6, sec/step=1.722, eta=3 days, 2:28:18\n",
      "2019-04-25 05:49:13 Iter 44400 [Train]: loss=56.92, epe=1.19, lr=0.000010, samples/sec=4.8, sec/step=1.667, eta=3 days, 0:03:01\n",
      "2019-04-25 05:52:33 Iter 44500 [Train]: loss=57.14, epe=1.20, lr=0.000010, samples/sec=4.6, sec/step=1.740, eta=3 days, 3:08:46\n",
      "2019-04-25 05:55:55 Iter 44600 [Train]: loss=57.22, epe=1.20, lr=0.000010, samples/sec=4.7, sec/step=1.698, eta=3 days, 1:17:29\n",
      "2019-04-25 05:59:14 Iter 44700 [Train]: loss=62.49, epe=1.32, lr=0.000010, samples/sec=4.7, sec/step=1.708, eta=3 days, 1:41:39\n",
      "2019-04-25 06:02:29 Iter 44800 [Train]: loss=58.37, epe=1.24, lr=0.000010, samples/sec=4.8, sec/step=1.658, eta=2 days, 23:27:26\n",
      "2019-04-25 06:05:49 Iter 44900 [Train]: loss=60.62, epe=1.29, lr=0.000010, samples/sec=4.7, sec/step=1.698, eta=3 days, 1:09:53\n",
      "2019-04-25 06:09:03 Iter 45000 [Train]: loss=58.09, epe=1.22, lr=0.000010, samples/sec=4.8, sec/step=1.665, eta=2 days, 23:42:00\n",
      "2019-04-25 06:09:05 Iter 45000 [Val]: loss=58.49, epe=1.35\n",
      "Saving model...\n",
      "... model wasn't saved -- its score (1.35) doesn't outperform other checkpoints\n",
      "2019-04-25 06:12:44 Iter 45100 [Train]: loss=59.61, epe=1.26, lr=0.000010, samples/sec=4.6, sec/step=1.744, eta=3 days, 3:03:15\n",
      "2019-04-25 06:16:06 Iter 45200 [Train]: loss=56.82, epe=1.19, lr=0.000010, samples/sec=4.8, sec/step=1.680, eta=3 days, 0:14:45\n",
      "2019-04-25 06:19:27 Iter 45300 [Train]: loss=58.62, epe=1.23, lr=0.000010, samples/sec=4.7, sec/step=1.708, eta=3 days, 1:25:01\n",
      "2019-04-25 06:22:44 Iter 45400 [Train]: loss=57.44, epe=1.22, lr=0.000010, samples/sec=4.8, sec/step=1.657, eta=2 days, 23:09:31\n",
      "2019-04-25 06:26:04 Iter 45500 [Train]: loss=59.39, epe=1.25, lr=0.000010, samples/sec=4.7, sec/step=1.694, eta=3 days, 0:42:00\n",
      "2019-04-25 06:29:25 Iter 45600 [Train]: loss=58.26, epe=1.23, lr=0.000010, samples/sec=4.8, sec/step=1.684, eta=3 days, 0:12:18\n",
      "2019-04-25 06:32:48 Iter 45700 [Train]: loss=59.38, epe=1.25, lr=0.000010, samples/sec=4.8, sec/step=1.682, eta=3 days, 0:04:29\n",
      "2019-04-25 06:36:10 Iter 45800 [Train]: loss=58.19, epe=1.22, lr=0.000010, samples/sec=4.8, sec/step=1.671, eta=2 days, 23:35:37\n",
      "2019-04-25 06:39:31 Iter 45900 [Train]: loss=59.15, epe=1.24, lr=0.000010, samples/sec=4.7, sec/step=1.684, eta=3 days, 0:06:05\n",
      "2019-04-25 06:42:58 Iter 46000 [Train]: loss=56.63, epe=1.18, lr=0.000010, samples/sec=4.7, sec/step=1.697, eta=3 days, 0:35:51\n",
      "2019-04-25 06:43:00 Iter 46000 [Val]: loss=52.67, epe=1.15\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-46000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-46000\n",
      "2019-04-25 06:46:46 Iter 46100 [Train]: loss=60.10, epe=1.26, lr=0.000010, samples/sec=4.6, sec/step=1.754, eta=3 days, 2:59:46\n",
      "2019-04-25 06:50:12 Iter 46200 [Train]: loss=58.88, epe=1.25, lr=0.000010, samples/sec=4.7, sec/step=1.687, eta=3 days, 0:04:12\n",
      "2019-04-25 06:53:36 Iter 46300 [Train]: loss=56.76, epe=1.18, lr=0.000010, samples/sec=4.7, sec/step=1.705, eta=3 days, 0:46:42\n",
      "2019-04-25 06:56:46 Iter 46400 [Train]: loss=57.95, epe=1.21, lr=0.000010, samples/sec=4.9, sec/step=1.619, eta=2 days, 21:03:30\n",
      "2019-04-25 07:00:07 Iter 46500 [Train]: loss=58.13, epe=1.22, lr=0.000010, samples/sec=4.7, sec/step=1.691, eta=3 days, 0:06:30\n",
      "2019-04-25 07:03:28 Iter 46600 [Train]: loss=60.86, epe=1.30, lr=0.000010, samples/sec=4.8, sec/step=1.669, eta=2 days, 23:06:24\n",
      "2019-04-25 07:06:47 Iter 46700 [Train]: loss=57.66, epe=1.18, lr=0.000010, samples/sec=4.8, sec/step=1.672, eta=2 days, 23:12:16\n",
      "2019-04-25 07:09:55 Iter 46800 [Train]: loss=60.67, epe=1.28, lr=0.000010, samples/sec=5.0, sec/step=1.608, eta=2 days, 20:24:49\n",
      "2019-04-25 07:13:14 Iter 46900 [Train]: loss=56.80, epe=1.20, lr=0.000010, samples/sec=4.8, sec/step=1.672, eta=2 days, 23:06:07\n",
      "2019-04-25 07:16:30 Iter 47000 [Train]: loss=60.00, epe=1.25, lr=0.000010, samples/sec=4.9, sec/step=1.643, eta=2 days, 21:49:31\n",
      "2019-04-25 07:16:33 Iter 47000 [Val]: loss=50.76, epe=1.10\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-47000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-47000\n",
      "2019-04-25 07:20:14 Iter 47100 [Train]: loss=61.44, epe=1.30, lr=0.000010, samples/sec=4.8, sec/step=1.660, eta=2 days, 22:29:23\n",
      "2019-04-25 07:23:29 Iter 47200 [Train]: loss=56.06, epe=1.17, lr=0.000010, samples/sec=4.8, sec/step=1.659, eta=2 days, 22:25:23\n",
      "2019-04-25 07:26:46 Iter 47300 [Train]: loss=58.94, epe=1.25, lr=0.000010, samples/sec=4.9, sec/step=1.637, eta=2 days, 21:26:05\n",
      "2019-04-25 07:30:08 Iter 47400 [Train]: loss=57.89, epe=1.21, lr=0.000010, samples/sec=4.8, sec/step=1.680, eta=2 days, 23:13:36\n",
      "2019-04-25 07:33:26 Iter 47500 [Train]: loss=57.39, epe=1.19, lr=0.000010, samples/sec=4.9, sec/step=1.639, eta=2 days, 21:24:55\n",
      "2019-04-25 07:36:45 Iter 47600 [Train]: loss=60.92, epe=1.29, lr=0.000010, samples/sec=4.8, sec/step=1.665, eta=2 days, 22:29:08\n",
      "2019-04-25 07:40:06 Iter 47700 [Train]: loss=57.10, epe=1.18, lr=0.000010, samples/sec=4.8, sec/step=1.663, eta=2 days, 22:20:53\n",
      "2019-04-25 07:43:27 Iter 47800 [Train]: loss=57.04, epe=1.20, lr=0.000010, samples/sec=4.8, sec/step=1.684, eta=2 days, 23:10:39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-25 07:46:45 Iter 47900 [Train]: loss=58.51, epe=1.24, lr=0.000010, samples/sec=4.9, sec/step=1.642, eta=2 days, 21:21:54\n",
      "2019-04-25 07:50:14 Iter 48000 [Train]: loss=58.52, epe=1.23, lr=0.000010, samples/sec=4.8, sec/step=1.678, eta=2 days, 22:50:41\n",
      "2019-04-25 07:50:17 Iter 48000 [Val]: loss=61.27, epe=1.45\n",
      "Saving model...\n",
      "... model wasn't saved -- its score (1.45) doesn't outperform other checkpoints\n",
      "2019-04-25 07:53:43 Iter 48100 [Train]: loss=61.69, epe=1.31, lr=0.000010, samples/sec=5.0, sec/step=1.611, eta=2 days, 19:58:44\n",
      "2019-04-25 07:57:06 Iter 48200 [Train]: loss=58.74, epe=1.25, lr=0.000010, samples/sec=4.9, sec/step=1.627, eta=2 days, 20:35:53\n",
      "2019-04-25 08:00:38 Iter 48300 [Train]: loss=57.57, epe=1.21, lr=0.000010, samples/sec=4.7, sec/step=1.709, eta=3 days, 0:00:34\n",
      "2019-04-25 08:04:02 Iter 48400 [Train]: loss=57.91, epe=1.21, lr=0.000010, samples/sec=4.8, sec/step=1.672, eta=2 days, 22:25:39\n",
      "2019-04-25 08:07:17 Iter 48500 [Train]: loss=59.97, epe=1.27, lr=0.000010, samples/sec=4.8, sec/step=1.660, eta=2 days, 21:51:58\n",
      "2019-04-25 08:10:34 Iter 48600 [Train]: loss=55.15, epe=1.14, lr=0.000010, samples/sec=5.0, sec/step=1.615, eta=2 days, 19:55:25\n",
      "2019-04-25 08:14:00 Iter 48700 [Train]: loss=60.01, epe=1.26, lr=0.000010, samples/sec=4.8, sec/step=1.677, eta=2 days, 22:28:18\n",
      "2019-04-25 08:17:17 Iter 48800 [Train]: loss=56.98, epe=1.19, lr=0.000010, samples/sec=4.7, sec/step=1.684, eta=2 days, 22:44:28\n",
      "2019-04-25 08:20:34 Iter 48900 [Train]: loss=58.60, epe=1.23, lr=0.000010, samples/sec=5.0, sec/step=1.605, eta=2 days, 19:20:54\n",
      "2019-04-25 08:23:51 Iter 49000 [Train]: loss=60.74, epe=1.27, lr=0.000010, samples/sec=4.8, sec/step=1.655, eta=2 days, 21:24:37\n",
      "2019-04-25 08:23:53 Iter 49000 [Val]: loss=51.67, epe=1.13\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-49000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-49000\n",
      "2019-04-25 08:27:33 Iter 49100 [Train]: loss=56.91, epe=1.18, lr=0.000010, samples/sec=4.7, sec/step=1.696, eta=2 days, 23:06:09\n",
      "2019-04-25 08:30:48 Iter 49200 [Train]: loss=57.36, epe=1.20, lr=0.000010, samples/sec=4.8, sec/step=1.653, eta=2 days, 21:14:18\n",
      "2019-04-25 08:34:16 Iter 49300 [Train]: loss=59.07, epe=1.24, lr=0.000010, samples/sec=4.9, sec/step=1.649, eta=2 days, 21:00:40\n",
      "2019-04-25 08:37:30 Iter 49400 [Train]: loss=58.35, epe=1.21, lr=0.000010, samples/sec=4.9, sec/step=1.638, eta=2 days, 20:32:17\n",
      "2019-04-25 08:40:41 Iter 49500 [Train]: loss=59.57, epe=1.27, lr=0.000010, samples/sec=4.9, sec/step=1.631, eta=2 days, 20:10:18\n",
      "2019-04-25 08:43:50 Iter 49600 [Train]: loss=55.04, epe=1.12, lr=0.000010, samples/sec=4.9, sec/step=1.622, eta=2 days, 19:46:58\n",
      "2019-04-25 08:47:19 Iter 49700 [Train]: loss=58.98, epe=1.23, lr=0.000010, samples/sec=4.7, sec/step=1.708, eta=2 days, 23:17:50\n",
      "2019-04-25 08:50:40 Iter 49800 [Train]: loss=57.21, epe=1.20, lr=0.000010, samples/sec=4.8, sec/step=1.652, eta=2 days, 20:54:31\n",
      "2019-04-25 08:53:58 Iter 49900 [Train]: loss=58.68, epe=1.22, lr=0.000010, samples/sec=4.8, sec/step=1.668, eta=2 days, 21:32:34\n",
      "2019-04-25 08:57:19 Iter 50000 [Train]: loss=55.50, epe=1.16, lr=0.000010, samples/sec=4.9, sec/step=1.631, eta=2 days, 19:58:41\n",
      "2019-04-25 08:57:22 Iter 50000 [Val]: loss=56.37, epe=1.25\n",
      "Saving model...\n",
      "... model wasn't saved -- its score (1.25) doesn't outperform other checkpoints\n",
      "2019-04-25 09:00:48 Iter 50100 [Train]: loss=60.61, epe=1.29, lr=0.000010, samples/sec=5.0, sec/step=1.599, eta=2 days, 18:35:36\n",
      "2019-04-25 09:04:07 Iter 50200 [Train]: loss=58.23, epe=1.23, lr=0.000010, samples/sec=4.8, sec/step=1.681, eta=2 days, 21:56:57\n",
      "2019-04-25 09:07:19 Iter 50300 [Train]: loss=56.65, epe=1.18, lr=0.000010, samples/sec=5.0, sec/step=1.611, eta=2 days, 18:58:34\n",
      "2019-04-25 09:10:44 Iter 50400 [Train]: loss=59.02, epe=1.23, lr=0.000010, samples/sec=4.9, sec/step=1.637, eta=2 days, 20:01:25\n",
      "2019-04-25 09:14:00 Iter 50500 [Train]: loss=56.65, epe=1.20, lr=0.000010, samples/sec=4.9, sec/step=1.627, eta=2 days, 19:35:08\n",
      "2019-04-25 09:17:25 Iter 50600 [Train]: loss=61.68, epe=1.32, lr=0.000010, samples/sec=4.6, sec/step=1.729, eta=2 days, 23:45:54\n",
      "2019-04-25 09:20:49 Iter 50700 [Train]: loss=55.68, epe=1.16, lr=0.000010, samples/sec=4.8, sec/step=1.653, eta=2 days, 20:32:34\n",
      "2019-04-25 09:24:06 Iter 50800 [Train]: loss=59.01, epe=1.24, lr=0.000010, samples/sec=4.7, sec/step=1.688, eta=2 days, 21:58:35\n",
      "2019-04-25 09:27:27 Iter 50900 [Train]: loss=58.37, epe=1.22, lr=0.000010, samples/sec=4.9, sec/step=1.646, eta=2 days, 20:09:35\n",
      "2019-04-25 09:30:54 Iter 51000 [Train]: loss=57.88, epe=1.23, lr=0.000010, samples/sec=4.7, sec/step=1.701, eta=2 days, 22:24:36\n",
      "2019-04-25 09:30:56 Iter 51000 [Val]: loss=54.72, epe=1.25\n",
      "Saving model...\n",
      "... model wasn't saved -- its score (1.25) doesn't outperform other checkpoints\n",
      "2019-04-25 09:34:30 Iter 51100 [Train]: loss=58.36, epe=1.23, lr=0.000010, samples/sec=4.8, sec/step=1.671, eta=2 days, 21:06:46\n",
      "2019-04-25 09:37:50 Iter 51200 [Train]: loss=56.38, epe=1.18, lr=0.000010, samples/sec=4.8, sec/step=1.683, eta=2 days, 21:34:03\n",
      "2019-04-25 09:41:08 Iter 51300 [Train]: loss=58.34, epe=1.22, lr=0.000010, samples/sec=4.8, sec/step=1.655, eta=2 days, 20:22:32\n",
      "2019-04-25 09:44:38 Iter 51400 [Train]: loss=59.11, epe=1.23, lr=0.000010, samples/sec=4.6, sec/step=1.751, eta=3 days, 0:16:06\n",
      "2019-04-25 09:47:54 Iter 51500 [Train]: loss=56.52, epe=1.18, lr=0.000010, samples/sec=4.8, sec/step=1.650, eta=2 days, 20:04:53\n",
      "2019-04-25 09:51:14 Iter 51600 [Train]: loss=56.34, epe=1.19, lr=0.000010, samples/sec=4.9, sec/step=1.646, eta=2 days, 19:50:36\n",
      "2019-04-25 09:54:45 Iter 51700 [Train]: loss=57.50, epe=1.20, lr=0.000010, samples/sec=4.7, sec/step=1.710, eta=2 days, 22:26:50\n",
      "2019-04-25 09:58:00 Iter 51800 [Train]: loss=58.40, epe=1.23, lr=0.000010, samples/sec=4.8, sec/step=1.652, eta=2 days, 20:00:36\n",
      "2019-04-25 10:01:23 Iter 51900 [Train]: loss=57.64, epe=1.21, lr=0.000010, samples/sec=4.7, sec/step=1.711, eta=2 days, 22:22:37\n",
      "2019-04-25 10:04:41 Iter 52000 [Train]: loss=57.31, epe=1.20, lr=0.000010, samples/sec=4.7, sec/step=1.693, eta=2 days, 21:37:07\n",
      "2019-04-25 10:04:44 Iter 52000 [Val]: loss=57.37, epe=1.30\n",
      "Saving model...\n",
      "... model wasn't saved -- its score (1.30) doesn't outperform other checkpoints\n",
      "2019-04-25 10:08:16 Iter 52100 [Train]: loss=57.44, epe=1.20, lr=0.000010, samples/sec=4.7, sec/step=1.686, eta=2 days, 21:14:59\n",
      "2019-04-25 10:11:37 Iter 52200 [Train]: loss=55.53, epe=1.15, lr=0.000010, samples/sec=4.7, sec/step=1.716, eta=2 days, 22:26:00\n",
      "2019-04-25 10:15:02 Iter 52300 [Train]: loss=56.93, epe=1.19, lr=0.000010, samples/sec=4.8, sec/step=1.671, eta=2 days, 20:33:06\n",
      "2019-04-25 10:18:25 Iter 52400 [Train]: loss=57.68, epe=1.19, lr=0.000010, samples/sec=4.8, sec/step=1.671, eta=2 days, 20:30:32\n",
      "2019-04-25 10:21:38 Iter 52500 [Train]: loss=60.06, epe=1.27, lr=0.000010, samples/sec=4.9, sec/step=1.618, eta=2 days, 18:17:35\n",
      "2019-04-25 10:24:55 Iter 52600 [Train]: loss=55.29, epe=1.15, lr=0.000010, samples/sec=4.8, sec/step=1.651, eta=2 days, 19:36:08\n",
      "2019-04-25 10:28:20 Iter 52700 [Train]: loss=56.42, epe=1.18, lr=0.000010, samples/sec=4.9, sec/step=1.648, eta=2 days, 19:26:29\n",
      "2019-04-25 10:31:37 Iter 52800 [Train]: loss=54.15, epe=1.12, lr=0.000010, samples/sec=4.9, sec/step=1.649, eta=2 days, 19:26:32\n",
      "2019-04-25 10:35:03 Iter 52900 [Train]: loss=62.11, epe=1.31, lr=0.000010, samples/sec=4.6, sec/step=1.729, eta=2 days, 22:39:10\n",
      "2019-04-25 10:38:38 Iter 53000 [Train]: loss=56.54, epe=1.19, lr=0.000010, samples/sec=4.6, sec/step=1.743, eta=2 days, 23:11:30\n",
      "2019-04-25 10:38:41 Iter 53000 [Val]: loss=57.57, epe=1.32\n",
      "Saving model...\n",
      "... model wasn't saved -- its score (1.32) doesn't outperform other checkpoints\n",
      "2019-04-25 10:42:16 Iter 53100 [Train]: loss=56.95, epe=1.20, lr=0.000010, samples/sec=4.8, sec/step=1.671, eta=2 days, 20:11:49\n",
      "2019-04-25 10:45:41 Iter 53200 [Train]: loss=59.50, epe=1.26, lr=0.000010, samples/sec=4.6, sec/step=1.751, eta=2 days, 23:25:03\n",
      "2019-04-25 10:49:16 Iter 53300 [Train]: loss=52.75, epe=1.09, lr=0.000010, samples/sec=4.6, sec/step=1.720, eta=2 days, 22:06:37\n",
      "2019-04-25 10:52:47 Iter 53400 [Train]: loss=57.55, epe=1.21, lr=0.000010, samples/sec=4.6, sec/step=1.739, eta=2 days, 22:47:44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-25 10:56:05 Iter 53500 [Train]: loss=56.99, epe=1.19, lr=0.000010, samples/sec=4.8, sec/step=1.676, eta=2 days, 20:12:02\n",
      "2019-04-25 10:59:31 Iter 53600 [Train]: loss=56.96, epe=1.18, lr=0.000010, samples/sec=4.8, sec/step=1.670, eta=2 days, 19:53:55\n",
      "2019-04-25 11:02:59 Iter 53700 [Train]: loss=56.55, epe=1.17, lr=0.000010, samples/sec=4.6, sec/step=1.729, eta=2 days, 22:15:06\n",
      "2019-04-25 11:06:13 Iter 53800 [Train]: loss=56.09, epe=1.16, lr=0.000010, samples/sec=4.8, sec/step=1.651, eta=2 days, 19:03:35\n",
      "2019-04-25 11:09:28 Iter 53900 [Train]: loss=61.16, epe=1.29, lr=0.000010, samples/sec=4.9, sec/step=1.645, eta=2 days, 18:45:28\n",
      "2019-04-25 11:12:19 Iter 54000 [Train]: loss=54.50, epe=1.12, lr=0.000010, samples/sec=5.2, sec/step=1.550, eta=2 days, 14:52:47\n",
      "2019-04-25 11:12:20 Iter 54000 [Val]: loss=49.97, epe=1.10\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-54000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-54000\n",
      "2019-04-25 11:15:12 Iter 54100 [Train]: loss=55.93, epe=1.17, lr=0.000010, samples/sec=5.5, sec/step=1.445, eta=2 days, 10:33:56\n",
      "2019-04-25 11:17:42 Iter 54200 [Train]: loss=57.84, epe=1.21, lr=0.000010, samples/sec=5.6, sec/step=1.438, eta=2 days, 10:13:34\n",
      "2019-04-25 11:20:12 Iter 54300 [Train]: loss=53.85, epe=1.12, lr=0.000010, samples/sec=5.6, sec/step=1.431, eta=2 days, 9:54:37\n",
      "2019-04-25 11:22:42 Iter 54400 [Train]: loss=59.07, epe=1.23, lr=0.000010, samples/sec=5.6, sec/step=1.432, eta=2 days, 9:55:18\n",
      "2019-04-25 11:25:15 Iter 54500 [Train]: loss=55.94, epe=1.17, lr=0.000010, samples/sec=5.4, sec/step=1.469, eta=2 days, 11:22:42\n",
      "2019-04-25 11:27:47 Iter 54600 [Train]: loss=59.16, epe=1.25, lr=0.000010, samples/sec=5.5, sec/step=1.459, eta=2 days, 10:54:50\n",
      "2019-04-25 11:30:19 Iter 54700 [Train]: loss=54.53, epe=1.13, lr=0.000010, samples/sec=5.5, sec/step=1.443, eta=2 days, 10:14:48\n",
      "2019-04-25 11:32:51 Iter 54800 [Train]: loss=57.25, epe=1.19, lr=0.000010, samples/sec=5.5, sec/step=1.449, eta=2 days, 10:27:01\n",
      "2019-04-25 11:35:24 Iter 54900 [Train]: loss=55.92, epe=1.17, lr=0.000010, samples/sec=5.5, sec/step=1.465, eta=2 days, 11:01:41\n",
      "2019-04-25 11:37:59 Iter 55000 [Train]: loss=59.19, epe=1.24, lr=0.000010, samples/sec=5.4, sec/step=1.487, eta=2 days, 11:53:32\n",
      "2019-04-25 11:38:01 Iter 55000 [Val]: loss=53.78, epe=1.20\n",
      "Saving model...\n",
      "... model wasn't saved -- its score (1.20) doesn't outperform other checkpoints\n",
      "2019-04-25 11:40:47 Iter 55100 [Train]: loss=57.33, epe=1.20, lr=0.000010, samples/sec=5.4, sec/step=1.492, eta=2 days, 12:03:48\n",
      "2019-04-25 11:43:24 Iter 55200 [Train]: loss=56.89, epe=1.19, lr=0.000010, samples/sec=5.4, sec/step=1.487, eta=2 days, 11:49:03\n",
      "2019-04-25 11:45:59 Iter 55300 [Train]: loss=56.82, epe=1.18, lr=0.000010, samples/sec=5.4, sec/step=1.483, eta=2 days, 11:36:54\n",
      "2019-04-25 11:48:33 Iter 55400 [Train]: loss=56.32, epe=1.17, lr=0.000010, samples/sec=5.5, sec/step=1.468, eta=2 days, 10:57:27\n",
      "2019-04-25 11:51:09 Iter 55500 [Train]: loss=57.97, epe=1.22, lr=0.000010, samples/sec=5.4, sec/step=1.487, eta=2 days, 11:41:10\n",
      "2019-04-25 11:53:46 Iter 55600 [Train]: loss=55.40, epe=1.16, lr=0.000010, samples/sec=5.4, sec/step=1.490, eta=2 days, 11:46:59\n",
      "2019-04-25 11:56:22 Iter 55700 [Train]: loss=60.05, epe=1.27, lr=0.000010, samples/sec=5.4, sec/step=1.490, eta=2 days, 11:44:23\n",
      "2019-04-25 11:58:57 Iter 55800 [Train]: loss=53.77, epe=1.11, lr=0.000010, samples/sec=5.4, sec/step=1.482, eta=2 days, 11:20:43\n",
      "2019-04-25 12:01:33 Iter 55900 [Train]: loss=57.00, epe=1.18, lr=0.000010, samples/sec=5.4, sec/step=1.489, eta=2 days, 11:37:11\n",
      "2019-04-25 12:04:11 Iter 56000 [Train]: loss=56.77, epe=1.19, lr=0.000010, samples/sec=5.3, sec/step=1.502, eta=2 days, 12:03:56\n",
      "2019-04-25 12:04:13 Iter 56000 [Val]: loss=51.43, epe=1.13\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-56000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-56000\n",
      "2019-04-25 12:07:08 Iter 56100 [Train]: loss=58.05, epe=1.23, lr=0.000010, samples/sec=5.4, sec/step=1.486, eta=2 days, 11:24:09\n",
      "2019-04-25 12:09:49 Iter 56200 [Train]: loss=57.09, epe=1.19, lr=0.000010, samples/sec=5.3, sec/step=1.503, eta=2 days, 12:02:04\n",
      "2019-04-25 12:12:29 Iter 56300 [Train]: loss=54.69, epe=1.14, lr=0.000010, samples/sec=5.3, sec/step=1.509, eta=2 days, 12:13:33\n",
      "2019-04-25 12:15:10 Iter 56400 [Train]: loss=56.22, epe=1.16, lr=0.000010, samples/sec=5.3, sec/step=1.523, eta=2 days, 12:45:37\n",
      "2019-04-25 12:17:51 Iter 56500 [Train]: loss=57.28, epe=1.20, lr=0.000010, samples/sec=5.3, sec/step=1.503, eta=2 days, 11:53:51\n",
      "2019-04-25 12:20:31 Iter 56600 [Train]: loss=57.19, epe=1.19, lr=0.000010, samples/sec=5.3, sec/step=1.504, eta=2 days, 11:55:06\n",
      "2019-04-25 12:23:13 Iter 56700 [Train]: loss=58.02, epe=1.22, lr=0.000010, samples/sec=5.3, sec/step=1.498, eta=2 days, 11:37:13\n",
      "2019-04-25 12:25:54 Iter 56800 [Train]: loss=54.54, epe=1.12, lr=0.000010, samples/sec=5.3, sec/step=1.513, eta=2 days, 12:11:05\n",
      "2019-04-25 12:28:35 Iter 56900 [Train]: loss=58.92, epe=1.24, lr=0.000010, samples/sec=5.3, sec/step=1.521, eta=2 days, 12:27:52\n",
      "2019-04-25 12:31:15 Iter 57000 [Train]: loss=53.59, epe=1.11, lr=0.000010, samples/sec=5.3, sec/step=1.518, eta=2 days, 12:16:43\n",
      "2019-04-25 12:31:17 Iter 57000 [Val]: loss=46.23, epe=0.98\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-57000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-57000\n",
      "2019-04-25 12:34:17 Iter 57100 [Train]: loss=56.24, epe=1.17, lr=0.000010, samples/sec=5.2, sec/step=1.532, eta=2 days, 12:48:00\n",
      "2019-04-25 12:36:58 Iter 57200 [Train]: loss=57.39, epe=1.20, lr=0.000010, samples/sec=5.3, sec/step=1.505, eta=2 days, 11:42:12\n",
      "2019-04-25 12:39:39 Iter 57300 [Train]: loss=57.59, epe=1.19, lr=0.000010, samples/sec=5.3, sec/step=1.498, eta=2 days, 11:23:38\n",
      "2019-04-25 12:42:20 Iter 57400 [Train]: loss=58.38, epe=1.23, lr=0.000010, samples/sec=5.3, sec/step=1.519, eta=2 days, 12:10:32\n",
      "2019-04-25 12:44:59 Iter 57500 [Train]: loss=57.18, epe=1.19, lr=0.000010, samples/sec=5.3, sec/step=1.500, eta=2 days, 11:21:40\n",
      "2019-04-25 12:47:36 Iter 57600 [Train]: loss=55.22, epe=1.14, lr=0.000010, samples/sec=5.4, sec/step=1.482, eta=2 days, 10:37:48\n",
      "2019-04-25 12:50:20 Iter 57700 [Train]: loss=54.39, epe=1.12, lr=0.000010, samples/sec=5.2, sec/step=1.527, eta=2 days, 12:21:51\n",
      "2019-04-25 12:53:01 Iter 57800 [Train]: loss=58.33, epe=1.22, lr=0.000010, samples/sec=5.3, sec/step=1.510, eta=2 days, 11:39:29\n",
      "2019-04-25 12:55:39 Iter 57900 [Train]: loss=55.34, epe=1.17, lr=0.000010, samples/sec=5.3, sec/step=1.501, eta=2 days, 11:14:53\n",
      "2019-04-25 12:58:22 Iter 58000 [Train]: loss=56.80, epe=1.17, lr=0.000010, samples/sec=5.2, sec/step=1.527, eta=2 days, 12:14:43\n",
      "2019-04-25 12:58:24 Iter 58000 [Val]: loss=49.40, epe=1.08\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-58000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-58000\n",
      "2019-04-25 13:01:24 Iter 58100 [Train]: loss=56.01, epe=1.15, lr=0.000010, samples/sec=5.3, sec/step=1.502, eta=2 days, 11:11:30\n",
      "2019-04-25 13:04:05 Iter 58200 [Train]: loss=56.75, epe=1.18, lr=0.000010, samples/sec=5.3, sec/step=1.510, eta=2 days, 11:29:06\n",
      "2019-04-25 13:06:45 Iter 58300 [Train]: loss=58.06, epe=1.22, lr=0.000010, samples/sec=5.3, sec/step=1.507, eta=2 days, 11:20:13\n",
      "2019-04-25 13:09:23 Iter 58400 [Train]: loss=54.60, epe=1.12, lr=0.000010, samples/sec=5.3, sec/step=1.498, eta=2 days, 10:54:55\n",
      "2019-04-25 13:12:03 Iter 58500 [Train]: loss=56.75, epe=1.19, lr=0.000010, samples/sec=5.3, sec/step=1.502, eta=2 days, 11:01:12\n",
      "2019-04-25 13:14:44 Iter 58600 [Train]: loss=55.07, epe=1.14, lr=0.000010, samples/sec=5.3, sec/step=1.513, eta=2 days, 11:26:45\n",
      "2019-04-25 13:17:26 Iter 58700 [Train]: loss=58.16, epe=1.22, lr=0.000010, samples/sec=5.3, sec/step=1.502, eta=2 days, 10:56:05\n",
      "2019-04-25 13:20:09 Iter 58800 [Train]: loss=60.10, epe=1.27, lr=0.000010, samples/sec=5.3, sec/step=1.514, eta=2 days, 11:23:18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-25 13:22:52 Iter 58900 [Train]: loss=54.33, epe=1.12, lr=0.000010, samples/sec=5.3, sec/step=1.512, eta=2 days, 11:16:32\n",
      "2019-04-25 13:25:31 Iter 59000 [Train]: loss=55.87, epe=1.16, lr=0.000010, samples/sec=5.3, sec/step=1.505, eta=2 days, 10:56:13\n",
      "2019-04-25 13:25:33 Iter 59000 [Val]: loss=47.28, epe=0.99\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-59000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-59000\n",
      "2019-04-25 13:28:33 Iter 59100 [Train]: loss=57.83, epe=1.21, lr=0.000010, samples/sec=5.3, sec/step=1.496, eta=2 days, 10:33:29\n",
      "2019-04-25 13:31:13 Iter 59200 [Train]: loss=54.50, epe=1.13, lr=0.000010, samples/sec=5.3, sec/step=1.504, eta=2 days, 10:50:32\n",
      "2019-04-25 13:33:52 Iter 59300 [Train]: loss=57.74, epe=1.20, lr=0.000010, samples/sec=5.3, sec/step=1.511, eta=2 days, 11:04:17\n",
      "2019-04-25 13:36:29 Iter 59400 [Train]: loss=55.58, epe=1.15, lr=0.000010, samples/sec=5.5, sec/step=1.468, eta=2 days, 9:19:29\n",
      "2019-04-25 13:39:13 Iter 59500 [Train]: loss=54.15, epe=1.12, lr=0.000010, samples/sec=5.2, sec/step=1.540, eta=2 days, 12:06:10\n",
      "2019-04-25 13:42:00 Iter 59600 [Train]: loss=58.44, epe=1.23, lr=0.000010, samples/sec=5.1, sec/step=1.568, eta=2 days, 13:09:50\n",
      "2019-04-25 13:44:51 Iter 59700 [Train]: loss=56.72, epe=1.19, lr=0.000010, samples/sec=5.0, sec/step=1.603, eta=2 days, 14:28:11\n",
      "2019-04-25 13:47:37 Iter 59800 [Train]: loss=54.75, epe=1.14, lr=0.000010, samples/sec=5.2, sec/step=1.547, eta=2 days, 12:14:11\n",
      "2019-04-25 13:50:22 Iter 59900 [Train]: loss=56.13, epe=1.16, lr=0.000010, samples/sec=5.2, sec/step=1.552, eta=2 days, 12:23:24\n",
      "2019-04-25 13:53:06 Iter 60000 [Train]: loss=57.24, epe=1.20, lr=0.000010, samples/sec=5.1, sec/step=1.556, eta=2 days, 12:30:32\n",
      "2019-04-25 13:53:09 Iter 60000 [Val]: loss=49.24, epe=1.09\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-60000\n",
      "2019-04-25 13:56:17 Iter 60100 [Train]: loss=54.09, epe=1.12, lr=0.000010, samples/sec=5.1, sec/step=1.582, eta=2 days, 13:28:57\n",
      "2019-04-25 13:59:08 Iter 60200 [Train]: loss=58.64, epe=1.24, lr=0.000010, samples/sec=5.1, sec/step=1.572, eta=2 days, 13:03:16\n",
      "2019-04-25 14:01:57 Iter 60300 [Train]: loss=56.09, epe=1.17, lr=0.000010, samples/sec=5.1, sec/step=1.567, eta=2 days, 12:48:41\n",
      "2019-04-25 14:04:49 Iter 60400 [Train]: loss=56.34, epe=1.17, lr=0.000010, samples/sec=5.1, sec/step=1.578, eta=2 days, 13:12:08\n",
      "2019-04-25 14:07:35 Iter 60500 [Train]: loss=55.33, epe=1.14, lr=0.000010, samples/sec=5.1, sec/step=1.558, eta=2 days, 12:22:16\n",
      "2019-04-25 14:10:21 Iter 60600 [Train]: loss=57.39, epe=1.20, lr=0.000010, samples/sec=5.2, sec/step=1.550, eta=2 days, 12:01:31\n",
      "2019-04-25 14:13:07 Iter 60700 [Train]: loss=54.55, epe=1.13, lr=0.000010, samples/sec=5.1, sec/step=1.562, eta=2 days, 12:26:24\n",
      "2019-04-25 14:15:52 Iter 60800 [Train]: loss=58.92, epe=1.24, lr=0.000010, samples/sec=5.2, sec/step=1.537, eta=2 days, 11:24:43\n",
      "2019-04-25 14:18:40 Iter 60900 [Train]: loss=55.63, epe=1.15, lr=0.000010, samples/sec=5.1, sec/step=1.560, eta=2 days, 12:17:29\n",
      "2019-04-25 14:21:26 Iter 61000 [Train]: loss=54.98, epe=1.14, lr=0.000010, samples/sec=5.1, sec/step=1.566, eta=2 days, 12:27:43\n",
      "2019-04-25 14:21:28 Iter 61000 [Val]: loss=53.68, epe=1.19\n",
      "Saving model...\n",
      "... model wasn't saved -- its score (1.19) doesn't outperform other checkpoints\n",
      "2019-04-25 14:24:27 Iter 61100 [Train]: loss=54.05, epe=1.13, lr=0.000010, samples/sec=5.1, sec/step=1.574, eta=2 days, 12:43:17\n",
      "2019-04-25 14:27:12 Iter 61200 [Train]: loss=58.24, epe=1.21, lr=0.000010, samples/sec=5.1, sec/step=1.558, eta=2 days, 12:03:46\n",
      "2019-04-25 14:30:00 Iter 61300 [Train]: loss=53.93, epe=1.12, lr=0.000010, samples/sec=5.1, sec/step=1.560, eta=2 days, 12:05:59\n",
      "2019-04-25 14:32:45 Iter 61400 [Train]: loss=57.41, epe=1.22, lr=0.000010, samples/sec=5.2, sec/step=1.544, eta=2 days, 11:26:58\n",
      "2019-04-25 14:35:27 Iter 61500 [Train]: loss=53.22, epe=1.09, lr=0.000010, samples/sec=5.2, sec/step=1.531, eta=2 days, 10:54:26\n",
      "2019-04-25 14:38:10 Iter 61600 [Train]: loss=57.25, epe=1.18, lr=0.000010, samples/sec=5.2, sec/step=1.533, eta=2 days, 10:56:28\n",
      "2019-04-25 14:40:55 Iter 61700 [Train]: loss=57.17, epe=1.18, lr=0.000010, samples/sec=5.2, sec/step=1.536, eta=2 days, 11:00:13\n",
      "2019-04-25 14:43:36 Iter 61800 [Train]: loss=53.28, epe=1.10, lr=0.000010, samples/sec=5.2, sec/step=1.529, eta=2 days, 10:41:13\n",
      "2019-04-25 14:46:20 Iter 61900 [Train]: loss=57.69, epe=1.20, lr=0.000010, samples/sec=5.2, sec/step=1.536, eta=2 days, 10:54:59\n",
      "2019-04-25 14:49:05 Iter 62000 [Train]: loss=53.96, epe=1.12, lr=0.000010, samples/sec=5.1, sec/step=1.554, eta=2 days, 11:33:16\n",
      "2019-04-25 14:49:07 Iter 62000 [Val]: loss=57.42, epe=1.32\n",
      "Saving model...\n",
      "... model wasn't saved -- its score (1.32) doesn't outperform other checkpoints\n",
      "2019-04-25 14:52:00 Iter 62100 [Train]: loss=55.16, epe=1.15, lr=0.000010, samples/sec=5.2, sec/step=1.546, eta=2 days, 11:13:42\n",
      "2019-04-25 14:54:39 Iter 62200 [Train]: loss=56.30, epe=1.18, lr=0.000010, samples/sec=5.3, sec/step=1.507, eta=2 days, 9:40:33\n",
      "2019-04-25 14:57:23 Iter 62300 [Train]: loss=55.30, epe=1.15, lr=0.000010, samples/sec=5.1, sec/step=1.559, eta=2 days, 11:38:50\n",
      "2019-04-25 15:00:07 Iter 62400 [Train]: loss=54.57, epe=1.13, lr=0.000010, samples/sec=5.2, sec/step=1.546, eta=2 days, 11:06:10\n",
      "2019-04-25 15:02:49 Iter 62500 [Train]: loss=54.75, epe=1.14, lr=0.000010, samples/sec=5.3, sec/step=1.523, eta=2 days, 10:09:32\n",
      "2019-04-25 15:05:34 Iter 62600 [Train]: loss=56.42, epe=1.18, lr=0.000010, samples/sec=5.2, sec/step=1.540, eta=2 days, 10:46:49\n",
      "2019-04-25 15:08:18 Iter 62700 [Train]: loss=54.82, epe=1.14, lr=0.000010, samples/sec=5.2, sec/step=1.551, eta=2 days, 11:10:16\n",
      "2019-04-25 15:11:01 Iter 62800 [Train]: loss=57.39, epe=1.21, lr=0.000010, samples/sec=5.2, sec/step=1.549, eta=2 days, 11:01:44\n",
      "2019-04-25 15:13:46 Iter 62900 [Train]: loss=52.19, epe=1.08, lr=0.000010, samples/sec=5.2, sec/step=1.553, eta=2 days, 11:07:31\n",
      "2019-04-25 15:16:28 Iter 63000 [Train]: loss=57.29, epe=1.21, lr=0.000010, samples/sec=5.2, sec/step=1.536, eta=2 days, 10:26:37\n",
      "2019-04-25 15:16:30 Iter 63000 [Val]: loss=51.21, epe=1.11\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-63000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-63000\n",
      "2019-04-25 15:19:33 Iter 63100 [Train]: loss=55.63, epe=1.18, lr=0.000010, samples/sec=5.2, sec/step=1.542, eta=2 days, 10:38:52\n",
      "2019-04-25 15:22:20 Iter 63200 [Train]: loss=58.21, epe=1.20, lr=0.000010, samples/sec=5.2, sec/step=1.547, eta=2 days, 10:46:12\n",
      "2019-04-25 15:25:03 Iter 63300 [Train]: loss=54.28, epe=1.13, lr=0.000010, samples/sec=5.2, sec/step=1.531, eta=2 days, 10:07:57\n",
      "2019-04-25 15:27:46 Iter 63400 [Train]: loss=57.59, epe=1.21, lr=0.000010, samples/sec=5.2, sec/step=1.525, eta=2 days, 9:52:29\n",
      "2019-04-25 15:30:29 Iter 63500 [Train]: loss=52.67, epe=1.07, lr=0.000010, samples/sec=5.2, sec/step=1.542, eta=2 days, 10:28:55\n",
      "2019-04-25 15:33:12 Iter 63600 [Train]: loss=56.37, epe=1.17, lr=0.000010, samples/sec=5.2, sec/step=1.550, eta=2 days, 10:42:36\n",
      "2019-04-25 15:35:55 Iter 63700 [Train]: loss=56.31, epe=1.18, lr=0.000010, samples/sec=5.2, sec/step=1.534, eta=2 days, 10:03:41\n",
      "2019-04-25 15:38:38 Iter 63800 [Train]: loss=54.86, epe=1.12, lr=0.000010, samples/sec=5.3, sec/step=1.518, eta=2 days, 9:26:45\n",
      "2019-04-25 15:41:20 Iter 63900 [Train]: loss=58.63, epe=1.23, lr=0.000010, samples/sec=5.2, sec/step=1.540, eta=2 days, 10:13:36\n",
      "2019-04-25 15:44:05 Iter 64000 [Train]: loss=55.15, epe=1.14, lr=0.000010, samples/sec=5.2, sec/step=1.542, eta=2 days, 10:14:53\n",
      "2019-04-25 15:44:08 Iter 64000 [Val]: loss=55.14, epe=1.29\n",
      "Saving model...\n",
      "... model wasn't saved -- its score (1.29) doesn't outperform other checkpoints\n",
      "2019-04-25 15:47:22 Iter 64100 [Train]: loss=57.93, epe=1.20, lr=0.000010, samples/sec=5.0, sec/step=1.604, eta=2 days, 12:34:00\n",
      "2019-04-25 15:50:04 Iter 64200 [Train]: loss=54.88, epe=1.14, lr=0.000010, samples/sec=5.2, sec/step=1.525, eta=2 days, 9:31:49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-25 15:52:47 Iter 64300 [Train]: loss=55.06, epe=1.15, lr=0.000010, samples/sec=5.3, sec/step=1.524, eta=2 days, 9:26:10\n",
      "2019-04-25 15:55:31 Iter 64400 [Train]: loss=55.35, epe=1.14, lr=0.000010, samples/sec=5.2, sec/step=1.538, eta=2 days, 9:54:56\n",
      "2019-04-25 15:58:13 Iter 64500 [Train]: loss=56.21, epe=1.17, lr=0.000010, samples/sec=5.2, sec/step=1.534, eta=2 days, 9:44:45\n",
      "2019-04-25 16:00:55 Iter 64600 [Train]: loss=55.93, epe=1.16, lr=0.000010, samples/sec=5.2, sec/step=1.527, eta=2 days, 9:27:00\n",
      "2019-04-25 16:03:33 Iter 64700 [Train]: loss=55.93, epe=1.17, lr=0.000010, samples/sec=5.4, sec/step=1.493, eta=2 days, 8:07:42\n",
      "2019-04-25 16:06:27 Iter 64800 [Train]: loss=53.67, epe=1.11, lr=0.000010, samples/sec=5.0, sec/step=1.594, eta=2 days, 11:52:49\n",
      "2019-04-25 16:09:25 Iter 64900 [Train]: loss=58.24, epe=1.23, lr=0.000010, samples/sec=4.9, sec/step=1.621, eta=2 days, 12:49:30\n",
      "2019-04-25 16:12:28 Iter 65000 [Train]: loss=57.97, epe=1.23, lr=0.000010, samples/sec=4.9, sec/step=1.646, eta=2 days, 13:42:49\n",
      "2019-04-25 16:12:30 Iter 65000 [Val]: loss=52.90, epe=1.18\n",
      "Saving model...\n",
      "... model wasn't saved -- its score (1.18) doesn't outperform other checkpoints\n",
      "2019-04-25 16:15:51 Iter 65100 [Train]: loss=52.87, epe=1.09, lr=0.000010, samples/sec=4.8, sec/step=1.655, eta=2 days, 14:01:59\n",
      "2019-04-25 16:18:51 Iter 65200 [Train]: loss=55.38, epe=1.14, lr=0.000010, samples/sec=4.9, sec/step=1.630, eta=2 days, 13:01:48\n",
      "2019-04-25 16:21:51 Iter 65300 [Train]: loss=54.26, epe=1.13, lr=0.000010, samples/sec=4.9, sec/step=1.626, eta=2 days, 12:51:03\n",
      "2019-04-25 16:24:49 Iter 65400 [Train]: loss=54.73, epe=1.13, lr=0.000010, samples/sec=5.0, sec/step=1.612, eta=2 days, 12:16:06\n",
      "2019-04-25 16:27:49 Iter 65500 [Train]: loss=54.50, epe=1.14, lr=0.000010, samples/sec=4.9, sec/step=1.627, eta=2 days, 12:46:35\n",
      "2019-04-25 16:30:53 Iter 65600 [Train]: loss=56.42, epe=1.16, lr=0.000010, samples/sec=4.9, sec/step=1.637, eta=2 days, 13:07:23\n",
      "2019-04-25 16:33:56 Iter 65700 [Train]: loss=56.67, epe=1.17, lr=0.000010, samples/sec=4.9, sec/step=1.644, eta=2 days, 13:18:53\n",
      "2019-04-25 16:37:02 Iter 65800 [Train]: loss=52.13, epe=1.07, lr=0.000010, samples/sec=4.9, sec/step=1.635, eta=2 days, 12:57:51\n",
      "2019-04-25 16:39:55 Iter 65900 [Train]: loss=56.64, epe=1.19, lr=0.000010, samples/sec=5.1, sec/step=1.582, eta=2 days, 10:55:51\n",
      "2019-04-25 16:42:58 Iter 66000 [Train]: loss=56.24, epe=1.17, lr=0.000010, samples/sec=4.8, sec/step=1.650, eta=2 days, 13:24:53\n",
      "2019-04-25 16:43:00 Iter 66000 [Val]: loss=50.84, epe=1.13\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-66000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-sintel_finetuned/pwcnet.ckpt-66000\n",
      "2019-04-25 16:46:42 Iter 66100 [Train]: loss=55.08, epe=1.14, lr=0.000010, samples/sec=4.8, sec/step=1.666, eta=2 days, 13:58:49\n",
      "2019-04-25 16:49:44 Iter 66200 [Train]: loss=54.40, epe=1.12, lr=0.000010, samples/sec=4.9, sec/step=1.631, eta=2 days, 12:36:42\n",
      "2019-04-25 16:52:45 Iter 66300 [Train]: loss=55.57, epe=1.15, lr=0.000010, samples/sec=4.9, sec/step=1.621, eta=2 days, 12:11:02\n",
      "2019-04-25 16:55:46 Iter 66400 [Train]: loss=54.82, epe=1.14, lr=0.000010, samples/sec=4.9, sec/step=1.623, eta=2 days, 12:13:58\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b1d44099aeaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/tfoptflow/tfoptflow/model_pwcnet.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m                 \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_tnsr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_adapt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_tnsr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_adapt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                 \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m                 \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_hat_train_tnsr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m                 \u001b[0mduration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostproc_y_hat_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# y_hat: [107.0802, 5.8556495, None]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "nn.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the training curves for the run above:\n",
    "\n",
    "![](img/pwcnet-sm-6-2-cyclic-chairsthingsmix_finetuned/loss.png)\n",
    "![](img/pwcnet-sm-6-2-cyclic-chairsthingsmix_finetuned/epe.png)\n",
    "![](img/pwcnet-sm-6-2-cyclic-chairsthingsmix_finetuned/lr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the predictions issued by the model for a few validation samples:\n",
    "\n",
    "![](img/pwcnet-sm-6-2-cyclic-chairsthingsmix_finetuned/val1.png)\n",
    "![](img/pwcnet-sm-6-2-cyclic-chairsthingsmix_finetuned/val2.png)\n",
    "![](img/pwcnet-sm-6-2-cyclic-chairsthingsmix_finetuned/val3.png)\n",
    "![](img/pwcnet-sm-6-2-cyclic-chairsthingsmix_finetuned/val4.png)\n",
    "![](img/pwcnet-sm-6-2-cyclic-chairsthingsmix_finetuned/val5.png)\n",
    "![](img/pwcnet-sm-6-2-cyclic-chairsthingsmix_finetuned/val6.png)\n",
    "![](img/pwcnet-sm-6-2-cyclic-chairsthingsmix_finetuned/val7.png)\n",
    "![](img/pwcnet-sm-6-2-cyclic-chairsthingsmix_finetuned/val8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
