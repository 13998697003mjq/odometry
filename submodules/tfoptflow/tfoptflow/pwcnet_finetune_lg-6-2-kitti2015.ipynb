{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PWC-Net-small model finetuning (with cyclical learning rate schedule)\n",
    "=======================================================\n",
    "\n",
    "In this notebook we:\n",
    "- Use a small model (no dense or residual connections), 6 level pyramid, uspample level 2 by 4 as the final flow prediction\n",
    "- Train the PWC-Net-small model on a mix of the `FlyingChairs` and `FlyingThings3DHalfRes` dataset using a Cyclic<sub>short</sub> schedule of our own\n",
    "- Let the Cyclic<sub>short</sub> schedule oscillate between `2e-05` and `1e-06` for 200,000 steps\n",
    "- Switch to the \"robust\" loss described in the paper, instead of the \"multiscale\" loss used during training\n",
    "\n",
    "Below, look for `TODO` references and customize this notebook based on your own needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "[2018a]<a name=\"2018a\"></a> Sun et al. 2018. PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume. [[arXiv]](https://arxiv.org/abs/1709.02371) [[web]](http://research.nvidia.com/publication/2018-02_PWC-Net%3A-CNNs-for) [[PyTorch (Official)]](https://github.com/NVlabs/PWC-Net/tree/master/PyTorch) [[Caffe (Official)]](https://github.com/NVlabs/PWC-Net/tree/master/Caffe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pwcnet_finetune.ipynb\n",
    "\n",
    "PWC-Net model finetuning.\n",
    "\n",
    "Written by Phil Ferriere\n",
    "\n",
    "Licensed under the MIT License (see LICENSE for details)\n",
    "\n",
    "Tensorboard:\n",
    "    [win] tensorboard --logdir=E:\\\\repos\\\\tf-optflow\\\\tfoptflow\\\\pwcnet-sm-6-2-cyclic-chairsthingsmix_finetuned\n",
    "    [ubu] tensorboard --logdir=/media/EDrive/repos/tf-optflow/tfoptflow/pwcnet-sm-6-2-cyclic-chairsthingsmix_finetuned\n",
    "\"\"\"\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import sys\n",
    "from copy import deepcopy\n",
    "from dataset_mpisintel import MPISintelDataset\n",
    "from dataset_base import _DEFAULT_DS_TUNE_OPTIONS\n",
    "from dataset_flyingchairs import FlyingChairsDataset\n",
    "from dataset_flyingthings3d import FlyingThings3DHalfResDataset\n",
    "from dataset_kitti import KITTIDataset\n",
    "from dataset_mixer import MixedDataset\n",
    "from model_pwcnet import ModelPWCNet, _DEFAULT_PWCNET_FINETUNE_OPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 20 14:16:25 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.40.04    Driver Version: 418.40.04    CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla P40           Off  | 00000000:04:00.0 Off |                  Off |\r\n",
      "| N/A   38C    P0    50W / 250W |    225MiB / 24451MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla P40           Off  | 00000000:06:00.0 Off |                  Off |\r\n",
      "| N/A   37C    P0    49W / 250W |    223MiB / 24451MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  Tesla P40           Off  | 00000000:07:00.0 Off |                  Off |\r\n",
      "| N/A   38C    P0    49W / 250W |   5735MiB / 24451MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0     42362      C   ...0/user/f.konokhov/miniconda3/bin/python   213MiB |\r\n",
      "|    1     42362      C   ...0/user/f.konokhov/miniconda3/bin/python   211MiB |\r\n",
      "|    2     42362      C   ...0/user/f.konokhov/miniconda3/bin/python  5723MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Set this first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: You MUST set dataset_root to the correct path on your machine!\n",
    "if sys.platform.startswith(\"win\"):\n",
    "    _DATASET_ROOT = 'E:/datasets/'\n",
    "else:\n",
    "    _DATASET_ROOT = '/Vol1/dbstore/datasets/KITTI_stereo_2015/data/' #'/Vol1/dbstore/datasets/sintel/'\n",
    "    _DATASET_ROOT2 = '/Vol1/dbstore/datasets/sintel/'\n",
    "    _MPISINTEL_ROOT = _DATASET_ROOT2 + 'Sintel_color'\n",
    "\n",
    "    \n",
    "# TODO: You MUST adjust the settings below based on the number of GPU(s) used for training\n",
    "# Set controller device and devices\n",
    "# A one-gpu setup would be something like controller='/device:GPU:0' and gpu_devices=['/device:GPU:0']\n",
    "# Here, we use a dual-GPU setup, as shown below\n",
    "# gpu_devices = ['/device:GPU:0', '/device:GPU:1']\n",
    "# controller = '/device:CPU:0'\n",
    "gpu_devices = ['/device:GPU:0' ,'/device:GPU:1', '/device:GPU:2']\n",
    "controller = '/device:GPU:1'\n",
    "\n",
    "# TODO: You MUST adjust this setting below based on the amount of memory on your GPU(s)\n",
    "# Batch size\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune on `sintel` mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: You MUST set the batch size based on the capabilities of your GPU(s) \n",
    "#  Load train dataset\n",
    "ds_opts = deepcopy(_DEFAULT_DS_TUNE_OPTIONS)\n",
    "ds_opts['in_memory'] = False                          # Too many samples to keep in memory at once, so don't preload them\n",
    "ds_opts['aug_type'] = 'basic'                         # Apply all supported augmentations\n",
    "ds_opts['batch_size'] = batch_size * len(gpu_devices) # Use a multiple of 8; here, 16 for dual-GPU mode (Titan X & 1080 Ti)\n",
    "#ds1 = FlyingChairsDataset(mode='train_with_val', ds_root=_FLYINGCHAIRS_ROOT, options=ds_opts)\n",
    "ds_opts['type'] = 'noc'\n",
    "#ds = MixedDataset(mode='train_with_val', datasets=[ds1, ds2], options=ds_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_opts['crop_preproc'] = (320,896)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = MPISintelDataset(mode='train_with_val', ds_root=_MPISINTEL_ROOT, options=ds_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = KITTIDataset(mode = 'train_with_val', ds_root=_DATASET_ROOT, options=ds_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_opts['type'] = 'final'\n",
    "ds2 = MPISintelDataset(mode='train_with_val', ds_root=_MPISINTEL_ROOT, options=ds_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_full = MixedDataset(mode='train_with_val', datasets=[ds, ds2], options=ds_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Configuration:\n",
      "  verbose              False\n",
      "  in_memory            False\n",
      "  crop_preproc         (320, 896)\n",
      "  scale_preproc        None\n",
      "  type                 final\n",
      "  tb_test_imgs         False\n",
      "  random_seed          1969\n",
      "  val_split            0.03\n",
      "  aug_type             basic\n",
      "  aug_labels           True\n",
      "  fliplr               0.5\n",
      "  flipud               0.5\n",
      "  translate            (0.5, 0.05)\n",
      "  scale                (0.5, 0.05)\n",
      "  batch_size           6\n",
      "  mode                 train_with_val\n",
      "  train size           1203\n",
      "  val size             38\n"
     ]
    }
   ],
   "source": [
    "# Display dataset configuration\n",
    "ds_full.print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = './models/pwcnet-lg-6-2-multisteps-chairsthingsmix/pwcnet.ckpt-595000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the default options\n",
    "nn_opts = deepcopy(_DEFAULT_PWCNET_FINETUNE_OPTIONS)\n",
    "nn_opts['verbose'] = True\n",
    "nn_opts['ckpt_path'] = './models/pwcnet-lg-6-2-multisteps-chairsthingsmix/pwcnet.ckpt-595000'\n",
    "nn_opts['ckpt_dir'] = './pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/'\n",
    "nn_opts['batch_size'] = 8\n",
    "nn_opts['x_shape'] = [2, ds_opts['crop_preproc'][0], ds_opts['crop_preproc'][1], 3]\n",
    "nn_opts['y_shape'] = [ds_opts['crop_preproc'][0], ds_opts['crop_preproc'][1], 2]\n",
    "nn_opts['use_tf_data'] = True # Use tf.data reader\n",
    "nn_opts['gpu_devices'] = gpu_devices\n",
    "nn_opts['controller'] = controller\n",
    "\n",
    "# Use the PWC-Net-small model in quarter-resolution mode\n",
    "nn_opts['use_dense_cx'] = True\n",
    "nn_opts['use_res_cx'] = True\n",
    "nn_opts['pyr_lvls'] = 6\n",
    "nn_opts['flow_pred_lvl'] = 2\n",
    "\n",
    "# Robust loss as described doesn't work, so try the following:\n",
    "nn_opts['loss_fn'] = 'loss_multiscale' # 'loss_multiscale' # 'loss_robust' # 'loss_robust'\n",
    "nn_opts['q'] = 1. # 0.4 # 1. # 0.4 # 1.\n",
    "nn_opts['epsilon'] = 0. # 0.01 # 0. # 0.01 # 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rate schedule. This schedule is for a single GPU using a batch size of 8.\n",
    "# Below,we adjust the schedule to the size of the batch and the number of GPUs.\n",
    "nn_opts['lr_policy'] = 'multisteps'\n",
    "nn_opts['init_lr'] = 1e-05\n",
    "nn_opts['lr_boundaries'] = [80000, 120000, 160000, 200000]\n",
    "nn_opts['lr_values'] = [1e-05, 5e-06, 2.5e-06, 1.25e-06, 6.25e-07]\n",
    "nn_opts['max_steps'] = 200000\n",
    "nn_opts['sparse_gt_flow'] = True\n",
    "# Below,we adjust the schedule to the size of the batch and our number of GPUs (2).\n",
    "nn_opts['max_steps'] = int(nn_opts['max_steps'] * 8 / 8)\n",
    "nn_opts['cyclic_lr_stepsize'] = int(nn_opts['cyclic_lr_stepsize'] * 8 / 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model towers...\n",
      "  Building tower_0...\n",
      "  ...tower_0 built.\n",
      "  Building tower_1...\n",
      "  ...tower_1 built.\n",
      "  Building tower_2...\n",
      "  ...tower_2 built.\n",
      "... model towers built.\n",
      "Initializing from pre-trained model at ./models/pwcnet-lg-6-2-multisteps-chairsthingsmix/pwcnet.ckpt-595000 for finetuning...\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./models/pwcnet-lg-6-2-multisteps-chairsthingsmix/pwcnet.ckpt-595000\n",
      "... model initialized\n",
      "\n",
      "Model Configuration:\n",
      "  verbose                True\n",
      "  ckpt_path              ./models/pwcnet-lg-6-2-multisteps-chairsthingsmix/pwcnet.ckpt-595000\n",
      "  ckpt_dir               ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/\n",
      "  max_to_keep            10\n",
      "  x_dtype                <dtype: 'float32'>\n",
      "  x_shape                [2, 320, 896, 3]\n",
      "  y_dtype                <dtype: 'float32'>\n",
      "  y_shape                [320, 896, 2]\n",
      "  train_mode             fine-tune\n",
      "  adapt_info             None\n",
      "  sparse_gt_flow         True\n",
      "  display_step           100\n",
      "  snapshot_step          1000\n",
      "  val_step               1000\n",
      "  val_batch_size         -1\n",
      "  tb_val_imgs            top_flow\n",
      "  tb_test_imgs           None\n",
      "  gpu_devices            ['/device:GPU:0', '/device:GPU:1', '/device:GPU:2']\n",
      "  controller             /device:GPU:1\n",
      "  use_tf_data            True\n",
      "  use_mixed_precision    False\n",
      "  loss_scaler            128.0\n",
      "  batch_size             8\n",
      "  lr_policy              multisteps\n",
      "  max_steps              200000\n",
      "  lr_boundaries          [80000, 120000, 160000, 200000]\n",
      "  lr_values              [1e-05, 5e-06, 2.5e-06, 1.25e-06, 6.25e-07]\n",
      "  loss_fn                loss_multiscale\n",
      "  alphas                 [0.32, 0.08, 0.02, 0.01, 0.005]\n",
      "  gamma                  0.0004\n",
      "  q                      1.0\n",
      "  epsilon                0.0\n",
      "  pyr_lvls               6\n",
      "  flow_pred_lvl          2\n",
      "  search_range           4\n",
      "  use_dense_cx           True\n",
      "  use_res_cx             True\n",
      "  mode                   train_with_val\n",
      "  trainable params       14079050\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model and display the model configuration\n",
    "nn = ModelPWCNet(mode='train_with_val', options=nn_opts, dataset=ds)\n",
    "nn.print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start finetuning...\n",
      "2019-05-20 16:18:41 Iter 100 [Train]: loss=242.27, epe=7.77, lr=0.000010, samples/sec=13.2, sec/step=1.819, eta=4 days, 5:00:20\n",
      "2019-05-20 16:21:10 Iter 200 [Train]: loss=180.80, epe=5.78, lr=0.000010, samples/sec=17.2, sec/step=1.391, eta=3 days, 5:13:32\n",
      "2019-05-20 16:23:40 Iter 300 [Train]: loss=169.97, epe=5.41, lr=0.000010, samples/sec=17.1, sec/step=1.403, eta=3 days, 5:49:48\n",
      "2019-05-20 16:26:10 Iter 400 [Train]: loss=162.70, epe=5.17, lr=0.000010, samples/sec=17.2, sec/step=1.397, eta=3 days, 5:28:02\n",
      "2019-05-20 16:28:41 Iter 500 [Train]: loss=158.94, epe=5.03, lr=0.000010, samples/sec=16.9, sec/step=1.417, eta=3 days, 6:30:24\n",
      "2019-05-20 16:31:11 Iter 600 [Train]: loss=156.78, epe=4.96, lr=0.000010, samples/sec=17.2, sec/step=1.399, eta=3 days, 5:30:29\n",
      "2019-05-20 16:33:40 Iter 700 [Train]: loss=155.79, epe=4.93, lr=0.000010, samples/sec=17.2, sec/step=1.392, eta=3 days, 5:04:56\n",
      "2019-05-20 16:36:10 Iter 800 [Train]: loss=153.57, epe=4.85, lr=0.000010, samples/sec=17.1, sec/step=1.402, eta=3 days, 5:34:07\n",
      "2019-05-20 16:38:42 Iter 900 [Train]: loss=152.68, epe=4.83, lr=0.000010, samples/sec=16.8, sec/step=1.428, eta=3 days, 6:57:04\n",
      "2019-05-20 16:41:12 Iter 1000 [Train]: loss=150.72, epe=4.76, lr=0.000010, samples/sec=17.2, sec/step=1.396, eta=3 days, 5:11:37\n",
      "2019-05-20 16:41:12 Iter 1000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-1000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-1000\n",
      "2019-05-20 16:43:58 Iter 1100 [Train]: loss=150.67, epe=4.76, lr=0.000010, samples/sec=17.1, sec/step=1.408, eta=3 days, 5:46:09\n",
      "2019-05-20 16:46:30 Iter 1200 [Train]: loss=149.88, epe=4.73, lr=0.000010, samples/sec=16.9, sec/step=1.423, eta=3 days, 6:35:15\n",
      "2019-05-20 16:49:03 Iter 1300 [Train]: loss=149.28, epe=4.72, lr=0.000010, samples/sec=16.7, sec/step=1.434, eta=3 days, 7:08:01\n",
      "2019-05-20 16:51:32 Iter 1400 [Train]: loss=149.18, epe=4.71, lr=0.000010, samples/sec=17.2, sec/step=1.393, eta=3 days, 4:50:09\n",
      "2019-05-20 16:54:00 Iter 1500 [Train]: loss=147.79, epe=4.67, lr=0.000010, samples/sec=17.3, sec/step=1.387, eta=3 days, 4:28:24\n",
      "2019-05-20 16:56:28 Iter 1600 [Train]: loss=147.19, epe=4.65, lr=0.000010, samples/sec=17.4, sec/step=1.381, eta=3 days, 4:05:31\n",
      "2019-05-20 16:58:58 Iter 1700 [Train]: loss=148.32, epe=4.69, lr=0.000010, samples/sec=17.1, sec/step=1.406, eta=3 days, 5:26:36\n",
      "2019-05-20 17:01:28 Iter 1800 [Train]: loss=145.74, epe=4.60, lr=0.000010, samples/sec=17.2, sec/step=1.398, eta=3 days, 4:59:40\n",
      "2019-05-20 17:04:01 Iter 1900 [Train]: loss=146.02, epe=4.62, lr=0.000010, samples/sec=16.7, sec/step=1.435, eta=3 days, 6:56:46\n",
      "2019-05-20 17:06:29 Iter 2000 [Train]: loss=146.65, epe=4.63, lr=0.000010, samples/sec=17.4, sec/step=1.382, eta=3 days, 4:02:14\n",
      "2019-05-20 17:06:29 Iter 2000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-2000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-2000\n",
      "2019-05-20 17:09:13 Iter 2100 [Train]: loss=146.03, epe=4.62, lr=0.000010, samples/sec=16.9, sec/step=1.420, eta=3 days, 6:02:36\n",
      "2019-05-20 17:11:41 Iter 2200 [Train]: loss=145.51, epe=4.60, lr=0.000010, samples/sec=17.2, sec/step=1.393, eta=3 days, 4:32:44\n",
      "2019-05-20 17:14:12 Iter 2300 [Train]: loss=145.56, epe=4.60, lr=0.000010, samples/sec=17.1, sec/step=1.407, eta=3 days, 5:17:22\n",
      "2019-05-20 17:16:42 Iter 2400 [Train]: loss=146.08, epe=4.62, lr=0.000010, samples/sec=17.1, sec/step=1.403, eta=3 days, 5:02:11\n",
      "2019-05-20 17:19:12 Iter 2500 [Train]: loss=145.05, epe=4.59, lr=0.000010, samples/sec=17.0, sec/step=1.411, eta=3 days, 5:25:57\n",
      "2019-05-20 17:21:44 Iter 2600 [Train]: loss=144.12, epe=4.56, lr=0.000010, samples/sec=16.9, sec/step=1.419, eta=3 days, 5:50:04\n",
      "2019-05-20 17:24:10 Iter 2700 [Train]: loss=144.28, epe=4.57, lr=0.000010, samples/sec=17.6, sec/step=1.366, eta=3 days, 2:53:28\n",
      "2019-05-20 17:26:38 Iter 2800 [Train]: loss=144.32, epe=4.56, lr=0.000010, samples/sec=17.4, sec/step=1.381, eta=3 days, 3:39:18\n",
      "2019-05-20 17:29:07 Iter 2900 [Train]: loss=144.10, epe=4.56, lr=0.000010, samples/sec=17.2, sec/step=1.397, eta=3 days, 4:28:13\n",
      "2019-05-20 17:31:37 Iter 3000 [Train]: loss=143.40, epe=4.54, lr=0.000010, samples/sec=17.2, sec/step=1.395, eta=3 days, 4:20:28\n",
      "2019-05-20 17:31:37 Iter 3000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-3000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-3000\n",
      "2019-05-20 17:34:19 Iter 3100 [Train]: loss=144.35, epe=4.57, lr=0.000010, samples/sec=17.0, sec/step=1.408, eta=3 days, 5:01:59\n",
      "2019-05-20 17:36:50 Iter 3200 [Train]: loss=143.27, epe=4.53, lr=0.000010, samples/sec=17.0, sec/step=1.413, eta=3 days, 5:14:58\n",
      "2019-05-20 17:39:18 Iter 3300 [Train]: loss=142.82, epe=4.52, lr=0.000010, samples/sec=17.3, sec/step=1.387, eta=3 days, 3:48:27\n",
      "2019-05-20 17:41:47 Iter 3400 [Train]: loss=143.09, epe=4.53, lr=0.000010, samples/sec=17.2, sec/step=1.393, eta=3 days, 4:03:15\n",
      "2019-05-20 17:44:18 Iter 3500 [Train]: loss=142.68, epe=4.51, lr=0.000010, samples/sec=17.0, sec/step=1.411, eta=3 days, 5:01:20\n",
      "2019-05-20 17:46:49 Iter 3600 [Train]: loss=142.20, epe=4.50, lr=0.000010, samples/sec=17.0, sec/step=1.411, eta=3 days, 4:57:35\n",
      "2019-05-20 17:49:21 Iter 3700 [Train]: loss=142.88, epe=4.52, lr=0.000010, samples/sec=16.9, sec/step=1.422, eta=3 days, 5:32:26\n",
      "2019-05-20 17:51:48 Iter 3800 [Train]: loss=142.11, epe=4.50, lr=0.000010, samples/sec=17.4, sec/step=1.379, eta=3 days, 3:09:34\n",
      "2019-05-20 17:54:18 Iter 3900 [Train]: loss=141.80, epe=4.49, lr=0.000010, samples/sec=17.1, sec/step=1.405, eta=3 days, 4:33:08\n",
      "2019-05-20 17:56:45 Iter 4000 [Train]: loss=142.71, epe=4.52, lr=0.000010, samples/sec=17.4, sec/step=1.377, eta=3 days, 2:57:52\n",
      "2019-05-20 17:56:45 Iter 4000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-4000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-4000\n",
      "2019-05-20 17:59:28 Iter 4100 [Train]: loss=141.03, epe=4.46, lr=0.000010, samples/sec=16.9, sec/step=1.421, eta=3 days, 5:19:23\n",
      "2019-05-20 18:01:59 Iter 4200 [Train]: loss=142.17, epe=4.50, lr=0.000010, samples/sec=16.9, sec/step=1.419, eta=3 days, 5:09:23\n",
      "2019-05-20 18:04:31 Iter 4300 [Train]: loss=141.75, epe=4.49, lr=0.000010, samples/sec=16.9, sec/step=1.419, eta=3 days, 5:09:55\n",
      "2019-05-20 18:07:01 Iter 4400 [Train]: loss=141.42, epe=4.48, lr=0.000010, samples/sec=17.1, sec/step=1.404, eta=3 days, 4:18:35\n",
      "2019-05-20 18:09:30 Iter 4500 [Train]: loss=141.18, epe=4.47, lr=0.000010, samples/sec=17.2, sec/step=1.393, eta=3 days, 3:40:16\n",
      "2019-05-20 18:11:58 Iter 4600 [Train]: loss=140.86, epe=4.45, lr=0.000010, samples/sec=17.4, sec/step=1.381, eta=3 days, 2:58:50\n",
      "2019-05-20 18:14:26 Iter 4700 [Train]: loss=141.64, epe=4.49, lr=0.000010, samples/sec=17.3, sec/step=1.387, eta=3 days, 3:15:09\n",
      "2019-05-20 18:16:56 Iter 4800 [Train]: loss=140.34, epe=4.44, lr=0.000010, samples/sec=17.1, sec/step=1.401, eta=3 days, 3:58:57\n",
      "2019-05-20 18:19:26 Iter 4900 [Train]: loss=139.94, epe=4.43, lr=0.000010, samples/sec=17.1, sec/step=1.405, eta=3 days, 4:10:11\n",
      "2019-05-20 18:21:59 Iter 5000 [Train]: loss=140.52, epe=4.45, lr=0.000010, samples/sec=16.7, sec/step=1.438, eta=3 days, 5:54:58\n",
      "2019-05-20 18:21:59 Iter 5000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-5000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-5000\n",
      "2019-05-20 18:24:43 Iter 5100 [Train]: loss=140.30, epe=4.44, lr=0.000010, samples/sec=16.9, sec/step=1.424, eta=3 days, 5:05:31\n",
      "2019-05-20 18:27:16 Iter 5200 [Train]: loss=140.47, epe=4.45, lr=0.000010, samples/sec=16.8, sec/step=1.432, eta=3 days, 5:30:03\n",
      "2019-05-20 18:29:46 Iter 5300 [Train]: loss=139.17, epe=4.41, lr=0.000010, samples/sec=17.1, sec/step=1.407, eta=3 days, 4:04:30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-20 18:32:13 Iter 5400 [Train]: loss=140.32, epe=4.44, lr=0.000010, samples/sec=17.4, sec/step=1.382, eta=3 days, 2:41:33\n",
      "2019-05-20 18:34:43 Iter 5500 [Train]: loss=139.50, epe=4.42, lr=0.000010, samples/sec=17.1, sec/step=1.405, eta=3 days, 3:53:25\n",
      "2019-05-20 18:37:12 Iter 5600 [Train]: loss=139.44, epe=4.42, lr=0.000010, samples/sec=17.2, sec/step=1.394, eta=3 days, 3:18:00\n",
      "2019-05-20 18:39:42 Iter 5700 [Train]: loss=138.41, epe=4.39, lr=0.000010, samples/sec=17.1, sec/step=1.403, eta=3 days, 3:42:35\n",
      "2019-05-20 18:42:12 Iter 5800 [Train]: loss=138.47, epe=4.38, lr=0.000010, samples/sec=17.1, sec/step=1.405, eta=3 days, 3:47:55\n",
      "2019-05-20 18:44:43 Iter 5900 [Train]: loss=139.15, epe=4.40, lr=0.000010, samples/sec=17.0, sec/step=1.412, eta=3 days, 4:06:48\n",
      "2019-05-20 18:47:14 Iter 6000 [Train]: loss=139.13, epe=4.41, lr=0.000010, samples/sec=17.0, sec/step=1.414, eta=3 days, 4:12:15\n",
      "2019-05-20 18:47:14 Iter 6000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-6000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-6000\n",
      "2019-05-20 18:49:55 Iter 6100 [Train]: loss=138.94, epe=4.40, lr=0.000010, samples/sec=17.3, sec/step=1.390, eta=3 days, 2:53:35\n",
      "2019-05-20 18:52:25 Iter 6200 [Train]: loss=139.18, epe=4.41, lr=0.000010, samples/sec=17.1, sec/step=1.402, eta=3 days, 3:28:51\n",
      "2019-05-20 18:54:55 Iter 6300 [Train]: loss=138.54, epe=4.38, lr=0.000010, samples/sec=17.0, sec/step=1.413, eta=3 days, 4:00:11\n",
      "2019-05-20 18:57:24 Iter 6400 [Train]: loss=138.58, epe=4.38, lr=0.000010, samples/sec=17.3, sec/step=1.387, eta=3 days, 2:34:03\n",
      "2019-05-20 18:59:53 Iter 6500 [Train]: loss=138.11, epe=4.37, lr=0.000010, samples/sec=17.2, sec/step=1.393, eta=3 days, 2:53:26\n",
      "2019-05-20 19:02:24 Iter 6600 [Train]: loss=139.35, epe=4.41, lr=0.000010, samples/sec=16.9, sec/step=1.417, eta=3 days, 4:07:19\n",
      "2019-05-20 19:04:55 Iter 6700 [Train]: loss=137.74, epe=4.36, lr=0.000010, samples/sec=17.0, sec/step=1.410, eta=3 days, 3:41:14\n",
      "2019-05-20 19:07:26 Iter 6800 [Train]: loss=137.92, epe=4.36, lr=0.000010, samples/sec=17.0, sec/step=1.415, eta=3 days, 3:55:12\n",
      "2019-05-20 19:09:54 Iter 6900 [Train]: loss=138.36, epe=4.38, lr=0.000010, samples/sec=17.4, sec/step=1.380, eta=3 days, 2:01:25\n",
      "2019-05-20 19:12:25 Iter 7000 [Train]: loss=137.35, epe=4.35, lr=0.000010, samples/sec=17.0, sec/step=1.412, eta=3 days, 3:41:56\n",
      "2019-05-20 19:12:25 Iter 7000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-7000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-7000\n",
      "2019-05-20 19:15:07 Iter 7100 [Train]: loss=137.80, epe=4.35, lr=0.000010, samples/sec=17.0, sec/step=1.413, eta=3 days, 3:42:58\n",
      "2019-05-20 19:17:36 Iter 7200 [Train]: loss=136.79, epe=4.33, lr=0.000010, samples/sec=17.2, sec/step=1.399, eta=3 days, 2:54:19\n",
      "2019-05-20 19:20:05 Iter 7300 [Train]: loss=138.02, epe=4.37, lr=0.000010, samples/sec=17.2, sec/step=1.395, eta=3 days, 2:40:02\n",
      "2019-05-20 19:22:36 Iter 7400 [Train]: loss=137.06, epe=4.33, lr=0.000010, samples/sec=17.1, sec/step=1.404, eta=3 days, 3:07:47\n",
      "2019-05-20 19:25:07 Iter 7500 [Train]: loss=136.69, epe=4.32, lr=0.000010, samples/sec=16.9, sec/step=1.417, eta=3 days, 3:45:31\n",
      "2019-05-20 19:27:37 Iter 7600 [Train]: loss=137.21, epe=4.34, lr=0.000010, samples/sec=17.1, sec/step=1.403, eta=3 days, 2:57:26\n",
      "2019-05-20 19:30:06 Iter 7700 [Train]: loss=136.89, epe=4.33, lr=0.000010, samples/sec=17.2, sec/step=1.392, eta=3 days, 2:22:54\n",
      "2019-05-20 19:32:34 Iter 7800 [Train]: loss=136.52, epe=4.32, lr=0.000010, samples/sec=17.2, sec/step=1.392, eta=3 days, 2:18:40\n",
      "2019-05-20 19:35:03 Iter 7900 [Train]: loss=137.06, epe=4.34, lr=0.000010, samples/sec=17.3, sec/step=1.387, eta=3 days, 2:00:11\n",
      "2019-05-20 19:37:35 Iter 8000 [Train]: loss=135.50, epe=4.28, lr=0.000010, samples/sec=16.9, sec/step=1.423, eta=3 days, 3:54:13\n",
      "2019-05-20 19:37:35 Iter 8000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-8000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-8000\n",
      "2019-05-20 19:40:16 Iter 8100 [Train]: loss=136.52, epe=4.32, lr=0.000010, samples/sec=16.9, sec/step=1.419, eta=3 days, 3:38:34\n",
      "2019-05-20 19:42:44 Iter 8200 [Train]: loss=136.40, epe=4.32, lr=0.000010, samples/sec=17.4, sec/step=1.379, eta=3 days, 1:28:56\n",
      "2019-05-20 19:45:13 Iter 8300 [Train]: loss=136.50, epe=4.32, lr=0.000010, samples/sec=17.3, sec/step=1.388, eta=3 days, 1:53:29\n",
      "2019-05-20 19:47:43 Iter 8400 [Train]: loss=135.74, epe=4.30, lr=0.000010, samples/sec=17.1, sec/step=1.405, eta=3 days, 2:47:58\n",
      "2019-05-20 19:50:12 Iter 8500 [Train]: loss=135.24, epe=4.27, lr=0.000010, samples/sec=17.2, sec/step=1.397, eta=3 days, 2:19:13\n",
      "2019-05-20 19:52:43 Iter 8600 [Train]: loss=136.46, epe=4.31, lr=0.000010, samples/sec=17.1, sec/step=1.406, eta=3 days, 2:43:47\n",
      "2019-05-20 19:55:14 Iter 8700 [Train]: loss=135.81, epe=4.29, lr=0.000010, samples/sec=17.0, sec/step=1.408, eta=3 days, 2:49:37\n",
      "2019-05-20 19:57:44 Iter 8800 [Train]: loss=135.97, epe=4.30, lr=0.000010, samples/sec=17.1, sec/step=1.404, eta=3 days, 2:34:02\n",
      "2019-05-20 20:00:14 Iter 8900 [Train]: loss=135.45, epe=4.28, lr=0.000010, samples/sec=17.1, sec/step=1.405, eta=3 days, 2:33:55\n",
      "2019-05-20 20:02:42 Iter 9000 [Train]: loss=135.73, epe=4.30, lr=0.000010, samples/sec=17.3, sec/step=1.387, eta=3 days, 1:33:44\n",
      "2019-05-20 20:02:42 Iter 9000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-9000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-9000\n",
      "2019-05-20 20:05:26 Iter 9100 [Train]: loss=137.40, epe=4.34, lr=0.000010, samples/sec=17.1, sec/step=1.402, eta=3 days, 2:20:08\n",
      "2019-05-20 20:07:56 Iter 9200 [Train]: loss=134.33, epe=4.25, lr=0.000010, samples/sec=17.1, sec/step=1.402, eta=3 days, 2:17:21\n",
      "2019-05-20 20:10:25 Iter 9300 [Train]: loss=135.74, epe=4.29, lr=0.000010, samples/sec=17.1, sec/step=1.400, eta=3 days, 2:09:16\n",
      "2019-05-20 20:12:53 Iter 9400 [Train]: loss=134.55, epe=4.26, lr=0.000010, samples/sec=17.3, sec/step=1.384, eta=3 days, 1:16:36\n",
      "2019-05-20 20:15:24 Iter 9500 [Train]: loss=135.07, epe=4.28, lr=0.000010, samples/sec=17.0, sec/step=1.413, eta=3 days, 2:47:37\n",
      "2019-05-20 20:17:56 Iter 9600 [Train]: loss=135.48, epe=4.29, lr=0.000010, samples/sec=17.0, sec/step=1.415, eta=3 days, 2:50:18\n",
      "2019-05-20 20:20:25 Iter 9700 [Train]: loss=134.71, epe=4.26, lr=0.000010, samples/sec=17.2, sec/step=1.398, eta=3 days, 1:53:35\n",
      "2019-05-20 20:22:54 Iter 9800 [Train]: loss=135.92, epe=4.30, lr=0.000010, samples/sec=17.2, sec/step=1.393, eta=3 days, 1:36:15\n",
      "2019-05-20 20:25:22 Iter 9900 [Train]: loss=134.40, epe=4.25, lr=0.000010, samples/sec=17.3, sec/step=1.385, eta=3 days, 1:06:45\n",
      "2019-05-20 20:27:54 Iter 10000 [Train]: loss=135.44, epe=4.29, lr=0.000010, samples/sec=17.0, sec/step=1.414, eta=3 days, 2:37:17\n",
      "2019-05-20 20:27:54 Iter 10000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-10000\n",
      "2019-05-20 20:30:36 Iter 10100 [Train]: loss=135.73, epe=4.29, lr=0.000010, samples/sec=17.0, sec/step=1.410, eta=3 days, 2:21:55\n",
      "2019-05-20 20:33:07 Iter 10200 [Train]: loss=135.04, epe=4.27, lr=0.000010, samples/sec=16.8, sec/step=1.425, eta=3 days, 3:06:41\n",
      "2019-05-20 20:35:38 Iter 10300 [Train]: loss=134.20, epe=4.25, lr=0.000010, samples/sec=17.0, sec/step=1.413, eta=3 days, 2:28:34\n",
      "2019-05-20 20:38:10 Iter 10400 [Train]: loss=135.15, epe=4.28, lr=0.000010, samples/sec=16.9, sec/step=1.422, eta=3 days, 2:52:48\n",
      "2019-05-20 20:40:38 Iter 10500 [Train]: loss=135.05, epe=4.27, lr=0.000010, samples/sec=17.4, sec/step=1.380, eta=3 days, 0:38:27\n",
      "2019-05-20 20:43:08 Iter 10600 [Train]: loss=134.24, epe=4.24, lr=0.000010, samples/sec=17.1, sec/step=1.407, eta=3 days, 2:00:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-20 20:45:37 Iter 10700 [Train]: loss=134.26, epe=4.24, lr=0.000010, samples/sec=17.2, sec/step=1.398, eta=3 days, 1:29:51\n",
      "2019-05-20 20:48:07 Iter 10800 [Train]: loss=134.47, epe=4.25, lr=0.000010, samples/sec=17.2, sec/step=1.398, eta=3 days, 1:28:53\n",
      "2019-05-20 20:50:36 Iter 10900 [Train]: loss=134.51, epe=4.26, lr=0.000010, samples/sec=17.2, sec/step=1.397, eta=3 days, 1:23:52\n",
      "2019-05-20 20:53:05 Iter 11000 [Train]: loss=133.82, epe=4.24, lr=0.000010, samples/sec=17.3, sec/step=1.389, eta=3 days, 0:54:57\n",
      "2019-05-20 20:53:05 Iter 11000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-11000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-11000\n",
      "2019-05-20 20:55:46 Iter 11100 [Train]: loss=134.69, epe=4.26, lr=0.000010, samples/sec=17.0, sec/step=1.409, eta=3 days, 1:56:19\n",
      "2019-05-20 20:58:15 Iter 11200 [Train]: loss=134.23, epe=4.25, lr=0.000010, samples/sec=17.3, sec/step=1.389, eta=3 days, 0:51:14\n",
      "2019-05-20 21:00:44 Iter 11300 [Train]: loss=135.03, epe=4.27, lr=0.000010, samples/sec=17.1, sec/step=1.403, eta=3 days, 1:33:22\n",
      "2019-05-20 21:03:16 Iter 11400 [Train]: loss=134.43, epe=4.26, lr=0.000010, samples/sec=16.9, sec/step=1.423, eta=3 days, 2:34:11\n",
      "2019-05-20 21:05:35 Iter 11500 [Train]: loss=133.09, epe=4.21, lr=0.000010, samples/sec=18.6, sec/step=1.292, eta=2 days, 19:39:48\n",
      "2019-05-20 21:07:40 Iter 11600 [Train]: loss=134.34, epe=4.26, lr=0.000010, samples/sec=21.0, sec/step=1.144, eta=2 days, 11:50:50\n",
      "2019-05-20 21:09:44 Iter 11700 [Train]: loss=133.77, epe=4.23, lr=0.000010, samples/sec=20.9, sec/step=1.148, eta=2 days, 12:02:06\n",
      "2019-05-20 21:11:51 Iter 11800 [Train]: loss=134.23, epe=4.25, lr=0.000010, samples/sec=20.4, sec/step=1.179, eta=2 days, 13:36:54\n",
      "2019-05-20 21:13:57 Iter 11900 [Train]: loss=133.90, epe=4.24, lr=0.000010, samples/sec=20.6, sec/step=1.163, eta=2 days, 12:46:24\n",
      "2019-05-20 21:16:02 Iter 12000 [Train]: loss=133.00, epe=4.21, lr=0.000010, samples/sec=20.9, sec/step=1.149, eta=2 days, 11:59:33\n",
      "2019-05-20 21:16:02 Iter 12000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-12000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-12000\n",
      "2019-05-20 21:18:18 Iter 12100 [Train]: loss=134.23, epe=4.25, lr=0.000010, samples/sec=20.7, sec/step=1.158, eta=2 days, 12:26:51\n",
      "2019-05-20 21:20:21 Iter 12200 [Train]: loss=133.90, epe=4.24, lr=0.000010, samples/sec=21.0, sec/step=1.143, eta=2 days, 11:37:55\n",
      "2019-05-20 21:22:27 Iter 12300 [Train]: loss=133.49, epe=4.23, lr=0.000010, samples/sec=20.7, sec/step=1.159, eta=2 days, 12:24:19\n",
      "2019-05-20 21:24:32 Iter 12400 [Train]: loss=133.79, epe=4.24, lr=0.000010, samples/sec=20.8, sec/step=1.152, eta=2 days, 12:03:01\n",
      "2019-05-20 21:26:37 Iter 12500 [Train]: loss=133.76, epe=4.24, lr=0.000010, samples/sec=20.7, sec/step=1.161, eta=2 days, 12:26:40\n",
      "2019-05-20 21:28:44 Iter 12600 [Train]: loss=133.16, epe=4.22, lr=0.000010, samples/sec=20.6, sec/step=1.163, eta=2 days, 12:31:20\n",
      "2019-05-20 21:30:49 Iter 12700 [Train]: loss=133.84, epe=4.24, lr=0.000010, samples/sec=20.8, sec/step=1.155, eta=2 days, 12:05:51\n",
      "2019-05-20 21:32:54 Iter 12800 [Train]: loss=133.57, epe=4.23, lr=0.000010, samples/sec=20.8, sec/step=1.154, eta=2 days, 11:59:53\n",
      "2019-05-20 21:34:59 Iter 12900 [Train]: loss=133.02, epe=4.22, lr=0.000010, samples/sec=20.7, sec/step=1.160, eta=2 days, 12:17:52\n",
      "2019-05-20 21:37:05 Iter 13000 [Train]: loss=133.86, epe=4.24, lr=0.000010, samples/sec=20.7, sec/step=1.159, eta=2 days, 12:11:23\n",
      "2019-05-20 21:37:05 Iter 13000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-13000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-13000\n",
      "2019-05-20 21:39:19 Iter 13100 [Train]: loss=132.83, epe=4.21, lr=0.000010, samples/sec=20.8, sec/step=1.152, eta=2 days, 11:47:26\n",
      "2019-05-20 21:41:24 Iter 13200 [Train]: loss=132.72, epe=4.21, lr=0.000010, samples/sec=20.7, sec/step=1.159, eta=2 days, 12:08:18\n",
      "2019-05-20 21:43:30 Iter 13300 [Train]: loss=133.39, epe=4.23, lr=0.000010, samples/sec=20.8, sec/step=1.152, eta=2 days, 11:45:15\n",
      "2019-05-20 21:45:35 Iter 13400 [Train]: loss=133.18, epe=4.22, lr=0.000010, samples/sec=20.6, sec/step=1.164, eta=2 days, 12:20:24\n",
      "2019-05-20 21:47:42 Iter 13500 [Train]: loss=133.19, epe=4.22, lr=0.000010, samples/sec=20.5, sec/step=1.170, eta=2 days, 12:35:29\n",
      "2019-05-20 21:49:47 Iter 13600 [Train]: loss=133.27, epe=4.22, lr=0.000010, samples/sec=20.8, sec/step=1.152, eta=2 days, 11:37:31\n",
      "2019-05-20 21:51:52 Iter 13700 [Train]: loss=133.46, epe=4.23, lr=0.000010, samples/sec=20.7, sec/step=1.160, eta=2 days, 12:01:23\n",
      "2019-05-20 21:53:57 Iter 13800 [Train]: loss=133.16, epe=4.22, lr=0.000010, samples/sec=20.9, sec/step=1.150, eta=2 days, 11:29:52\n",
      "2019-05-20 21:56:02 Iter 13900 [Train]: loss=133.45, epe=4.23, lr=0.000010, samples/sec=20.7, sec/step=1.157, eta=2 days, 11:49:32\n",
      "2019-05-20 21:58:09 Iter 14000 [Train]: loss=133.45, epe=4.23, lr=0.000010, samples/sec=20.5, sec/step=1.168, eta=2 days, 12:21:59\n",
      "2019-05-20 21:58:09 Iter 14000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-14000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-14000\n",
      "2019-05-20 22:00:25 Iter 14100 [Train]: loss=133.26, epe=4.22, lr=0.000010, samples/sec=20.6, sec/step=1.163, eta=2 days, 12:03:43\n",
      "2019-05-20 22:02:31 Iter 14200 [Train]: loss=133.06, epe=4.22, lr=0.000010, samples/sec=20.6, sec/step=1.162, eta=2 days, 11:59:49\n",
      "2019-05-20 22:04:36 Iter 14300 [Train]: loss=133.17, epe=4.22, lr=0.000010, samples/sec=20.8, sec/step=1.153, eta=2 days, 11:27:11\n",
      "2019-05-20 22:06:42 Iter 14400 [Train]: loss=133.03, epe=4.22, lr=0.000010, samples/sec=20.6, sec/step=1.165, eta=2 days, 12:03:12\n",
      "2019-05-20 22:08:46 Iter 14500 [Train]: loss=132.38, epe=4.20, lr=0.000010, samples/sec=21.0, sec/step=1.143, eta=2 days, 10:55:03\n",
      "2019-05-20 22:10:52 Iter 14600 [Train]: loss=132.48, epe=4.20, lr=0.000010, samples/sec=20.6, sec/step=1.166, eta=2 days, 12:03:02\n",
      "2019-05-20 22:12:57 Iter 14700 [Train]: loss=132.90, epe=4.21, lr=0.000010, samples/sec=20.8, sec/step=1.155, eta=2 days, 11:27:45\n",
      "2019-05-20 22:15:04 Iter 14800 [Train]: loss=133.03, epe=4.22, lr=0.000010, samples/sec=20.5, sec/step=1.170, eta=2 days, 12:11:18\n",
      "2019-05-20 22:17:08 Iter 14900 [Train]: loss=132.14, epe=4.19, lr=0.000010, samples/sec=20.9, sec/step=1.150, eta=2 days, 11:06:16\n",
      "2019-05-20 22:19:13 Iter 15000 [Train]: loss=132.33, epe=4.19, lr=0.000010, samples/sec=20.9, sec/step=1.150, eta=2 days, 11:05:09\n",
      "2019-05-20 22:19:13 Iter 15000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-15000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-15000\n",
      "2019-05-20 22:21:27 Iter 15100 [Train]: loss=133.34, epe=4.22, lr=0.000010, samples/sec=20.9, sec/step=1.146, eta=2 days, 10:50:55\n",
      "2019-05-20 22:23:34 Iter 15200 [Train]: loss=132.49, epe=4.20, lr=0.000010, samples/sec=20.5, sec/step=1.173, eta=2 days, 12:11:40\n",
      "2019-05-20 22:25:37 Iter 15300 [Train]: loss=133.01, epe=4.22, lr=0.000010, samples/sec=21.2, sec/step=1.134, eta=2 days, 10:12:17\n",
      "2019-05-20 22:27:42 Iter 15400 [Train]: loss=133.05, epe=4.22, lr=0.000010, samples/sec=20.9, sec/step=1.148, eta=2 days, 10:50:44\n",
      "2019-05-20 22:29:46 Iter 15500 [Train]: loss=132.19, epe=4.19, lr=0.000010, samples/sec=20.9, sec/step=1.151, eta=2 days, 10:58:24\n",
      "2019-05-20 22:31:54 Iter 15600 [Train]: loss=132.58, epe=4.21, lr=0.000010, samples/sec=20.6, sec/step=1.165, eta=2 days, 11:40:22\n",
      "2019-05-20 22:34:00 Iter 15700 [Train]: loss=133.24, epe=4.22, lr=0.000010, samples/sec=20.5, sec/step=1.168, eta=2 days, 11:48:29\n",
      "2019-05-20 22:36:03 Iter 15800 [Train]: loss=132.23, epe=4.20, lr=0.000010, samples/sec=21.1, sec/step=1.138, eta=2 days, 10:12:47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-20 22:38:10 Iter 15900 [Train]: loss=133.29, epe=4.22, lr=0.000010, samples/sec=20.4, sec/step=1.176, eta=2 days, 12:07:09\n",
      "2019-05-20 22:40:14 Iter 16000 [Train]: loss=132.42, epe=4.20, lr=0.000010, samples/sec=21.0, sec/step=1.142, eta=2 days, 10:22:57\n",
      "2019-05-20 22:40:14 Iter 16000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-16000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-16000\n",
      "2019-05-20 22:42:32 Iter 16100 [Train]: loss=132.53, epe=4.20, lr=0.000010, samples/sec=20.4, sec/step=1.177, eta=2 days, 12:06:02\n",
      "2019-05-20 22:44:36 Iter 16200 [Train]: loss=132.38, epe=4.20, lr=0.000010, samples/sec=20.9, sec/step=1.147, eta=2 days, 10:35:09\n",
      "2019-05-20 22:46:44 Iter 16300 [Train]: loss=132.03, epe=4.19, lr=0.000010, samples/sec=20.3, sec/step=1.183, eta=2 days, 12:21:50\n",
      "2019-05-20 22:48:51 Iter 16400 [Train]: loss=133.00, epe=4.22, lr=0.000010, samples/sec=20.4, sec/step=1.175, eta=2 days, 11:54:38\n",
      "2019-05-20 22:50:58 Iter 16500 [Train]: loss=132.00, epe=4.19, lr=0.000010, samples/sec=20.4, sec/step=1.175, eta=2 days, 11:52:56\n",
      "2019-05-20 22:53:04 Iter 16600 [Train]: loss=131.96, epe=4.19, lr=0.000010, samples/sec=20.8, sec/step=1.155, eta=2 days, 10:49:39\n",
      "2019-05-20 22:55:10 Iter 16700 [Train]: loss=132.06, epe=4.18, lr=0.000010, samples/sec=20.7, sec/step=1.162, eta=2 days, 11:10:20\n",
      "2019-05-20 22:57:13 Iter 16800 [Train]: loss=131.91, epe=4.19, lr=0.000010, samples/sec=21.2, sec/step=1.130, eta=2 days, 9:31:41\n",
      "2019-05-20 22:59:19 Iter 16900 [Train]: loss=132.79, epe=4.21, lr=0.000010, samples/sec=20.6, sec/step=1.167, eta=2 days, 11:21:12\n",
      "2019-05-20 23:01:24 Iter 17000 [Train]: loss=131.68, epe=4.18, lr=0.000010, samples/sec=20.8, sec/step=1.152, eta=2 days, 10:33:45\n",
      "2019-05-20 23:01:24 Iter 17000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-17000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-17000\n",
      "2019-05-20 23:03:40 Iter 17100 [Train]: loss=132.10, epe=4.19, lr=0.000010, samples/sec=20.6, sec/step=1.165, eta=2 days, 11:11:27\n",
      "2019-05-20 23:05:46 Iter 17200 [Train]: loss=132.57, epe=4.20, lr=0.000010, samples/sec=20.6, sec/step=1.165, eta=2 days, 11:09:24\n",
      "2019-05-20 23:07:52 Iter 17300 [Train]: loss=132.31, epe=4.20, lr=0.000010, samples/sec=20.7, sec/step=1.158, eta=2 days, 10:45:56\n",
      "2019-05-20 23:09:57 Iter 17400 [Train]: loss=131.86, epe=4.18, lr=0.000010, samples/sec=20.7, sec/step=1.160, eta=2 days, 10:50:23\n",
      "2019-05-20 23:12:03 Iter 17500 [Train]: loss=132.60, epe=4.21, lr=0.000010, samples/sec=20.7, sec/step=1.160, eta=2 days, 10:48:22\n",
      "2019-05-20 23:14:08 Iter 17600 [Train]: loss=132.62, epe=4.20, lr=0.000010, samples/sec=20.7, sec/step=1.158, eta=2 days, 10:41:11\n",
      "2019-05-20 23:16:14 Iter 17700 [Train]: loss=131.28, epe=4.16, lr=0.000010, samples/sec=20.6, sec/step=1.166, eta=2 days, 11:03:27\n",
      "2019-05-20 23:18:19 Iter 17800 [Train]: loss=132.48, epe=4.20, lr=0.000010, samples/sec=20.9, sec/step=1.147, eta=2 days, 10:01:56\n",
      "2019-05-20 23:20:24 Iter 17900 [Train]: loss=131.76, epe=4.18, lr=0.000010, samples/sec=20.7, sec/step=1.158, eta=2 days, 10:34:07\n",
      "2019-05-20 23:22:29 Iter 18000 [Train]: loss=131.77, epe=4.18, lr=0.000010, samples/sec=20.9, sec/step=1.147, eta=2 days, 9:59:40\n",
      "2019-05-20 23:22:29 Iter 18000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-18000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-18000\n",
      "2019-05-20 23:24:45 Iter 18100 [Train]: loss=132.37, epe=4.20, lr=0.000010, samples/sec=20.6, sec/step=1.163, eta=2 days, 10:44:47\n",
      "2019-05-20 23:26:54 Iter 18200 [Train]: loss=132.01, epe=4.19, lr=0.000010, samples/sec=20.2, sec/step=1.191, eta=2 days, 12:08:46\n",
      "2019-05-20 23:28:59 Iter 18300 [Train]: loss=131.77, epe=4.18, lr=0.000010, samples/sec=20.7, sec/step=1.160, eta=2 days, 10:31:28\n",
      "2019-05-20 23:31:06 Iter 18400 [Train]: loss=131.93, epe=4.19, lr=0.000010, samples/sec=20.6, sec/step=1.164, eta=2 days, 10:44:02\n",
      "2019-05-20 23:33:12 Iter 18500 [Train]: loss=131.58, epe=4.17, lr=0.000010, samples/sec=20.6, sec/step=1.164, eta=2 days, 10:39:52\n",
      "2019-05-20 23:35:18 Iter 18600 [Train]: loss=131.42, epe=4.17, lr=0.000010, samples/sec=20.6, sec/step=1.164, eta=2 days, 10:39:38\n",
      "2019-05-20 23:37:23 Iter 18700 [Train]: loss=131.77, epe=4.18, lr=0.000010, samples/sec=20.7, sec/step=1.158, eta=2 days, 10:18:33\n",
      "2019-05-20 23:39:28 Iter 18800 [Train]: loss=131.87, epe=4.19, lr=0.000010, samples/sec=20.9, sec/step=1.151, eta=2 days, 9:55:24\n",
      "2019-05-20 23:41:35 Iter 18900 [Train]: loss=130.95, epe=4.16, lr=0.000010, samples/sec=20.4, sec/step=1.175, eta=2 days, 11:06:56\n",
      "2019-05-20 23:43:40 Iter 19000 [Train]: loss=131.49, epe=4.18, lr=0.000010, samples/sec=20.8, sec/step=1.152, eta=2 days, 9:56:10\n",
      "2019-05-20 23:43:40 Iter 19000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-19000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-19000\n",
      "2019-05-20 23:45:53 Iter 19100 [Train]: loss=132.11, epe=4.19, lr=0.000010, samples/sec=20.9, sec/step=1.148, eta=2 days, 9:41:23\n",
      "2019-05-20 23:48:00 Iter 19200 [Train]: loss=131.23, epe=4.17, lr=0.000010, samples/sec=20.4, sec/step=1.174, eta=2 days, 10:58:06\n",
      "2019-05-20 23:50:04 Iter 19300 [Train]: loss=131.58, epe=4.18, lr=0.000010, samples/sec=21.0, sec/step=1.144, eta=2 days, 9:24:56\n",
      "2019-05-20 23:52:11 Iter 19400 [Train]: loss=131.32, epe=4.17, lr=0.000010, samples/sec=20.5, sec/step=1.172, eta=2 days, 10:47:06\n",
      "2019-05-20 23:54:16 Iter 19500 [Train]: loss=131.54, epe=4.17, lr=0.000010, samples/sec=20.8, sec/step=1.156, eta=2 days, 9:57:27\n",
      "2019-05-20 23:56:19 Iter 19600 [Train]: loss=131.28, epe=4.17, lr=0.000010, samples/sec=21.1, sec/step=1.137, eta=2 days, 8:59:38\n",
      "2019-05-20 23:58:24 Iter 19700 [Train]: loss=130.62, epe=4.15, lr=0.000010, samples/sec=20.9, sec/step=1.150, eta=2 days, 9:36:22\n",
      "2019-05-21 00:00:28 Iter 19800 [Train]: loss=132.14, epe=4.20, lr=0.000010, samples/sec=20.9, sec/step=1.150, eta=2 days, 9:33:13\n",
      "2019-05-21 00:02:33 Iter 19900 [Train]: loss=131.44, epe=4.17, lr=0.000010, samples/sec=20.8, sec/step=1.152, eta=2 days, 9:38:12\n",
      "2019-05-21 00:04:38 Iter 20000 [Train]: loss=130.66, epe=4.15, lr=0.000010, samples/sec=20.9, sec/step=1.151, eta=2 days, 9:32:09\n",
      "2019-05-21 00:04:38 Iter 20000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-20000\n",
      "2019-05-21 00:06:54 Iter 20100 [Train]: loss=131.47, epe=4.17, lr=0.000010, samples/sec=20.7, sec/step=1.157, eta=2 days, 9:50:24\n",
      "2019-05-21 00:08:59 Iter 20200 [Train]: loss=132.26, epe=4.20, lr=0.000010, samples/sec=20.7, sec/step=1.159, eta=2 days, 9:52:28\n",
      "2019-05-21 00:11:04 Iter 20300 [Train]: loss=131.09, epe=4.17, lr=0.000010, samples/sec=20.8, sec/step=1.157, eta=2 days, 9:44:05\n",
      "2019-05-21 00:13:10 Iter 20400 [Train]: loss=132.21, epe=4.20, lr=0.000010, samples/sec=20.6, sec/step=1.162, eta=2 days, 9:59:41\n",
      "2019-05-21 00:15:16 Iter 20500 [Train]: loss=131.98, epe=4.19, lr=0.000010, samples/sec=20.7, sec/step=1.159, eta=2 days, 9:48:24\n",
      "2019-05-21 00:17:23 Iter 20600 [Train]: loss=131.25, epe=4.17, lr=0.000010, samples/sec=20.5, sec/step=1.170, eta=2 days, 10:18:20\n",
      "2019-05-21 00:19:29 Iter 20700 [Train]: loss=131.70, epe=4.18, lr=0.000010, samples/sec=20.6, sec/step=1.165, eta=2 days, 10:02:09\n",
      "2019-05-21 00:21:34 Iter 20800 [Train]: loss=130.28, epe=4.14, lr=0.000010, samples/sec=20.8, sec/step=1.154, eta=2 days, 9:27:12\n",
      "2019-05-21 00:23:42 Iter 20900 [Train]: loss=130.82, epe=4.15, lr=0.000010, samples/sec=20.3, sec/step=1.182, eta=2 days, 10:48:53\n",
      "2019-05-21 00:25:48 Iter 21000 [Train]: loss=131.72, epe=4.18, lr=0.000010, samples/sec=20.5, sec/step=1.168, eta=2 days, 10:05:07\n",
      "2019-05-21 00:25:48 Iter 21000 [Val]: loss=nan, epe=nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-21000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-21000\n",
      "2019-05-21 00:28:05 Iter 21100 [Train]: loss=130.72, epe=4.15, lr=0.000010, samples/sec=20.6, sec/step=1.163, eta=2 days, 9:47:42\n",
      "2019-05-21 00:30:11 Iter 21200 [Train]: loss=131.43, epe=4.17, lr=0.000010, samples/sec=20.5, sec/step=1.169, eta=2 days, 10:03:07\n",
      "2019-05-21 00:32:19 Iter 21300 [Train]: loss=131.06, epe=4.16, lr=0.000010, samples/sec=20.5, sec/step=1.170, eta=2 days, 10:05:32\n",
      "2019-05-21 00:34:25 Iter 21400 [Train]: loss=130.54, epe=4.15, lr=0.000010, samples/sec=20.6, sec/step=1.164, eta=2 days, 9:44:14\n",
      "2019-05-21 00:36:31 Iter 21500 [Train]: loss=131.70, epe=4.18, lr=0.000010, samples/sec=20.7, sec/step=1.160, eta=2 days, 9:32:06\n",
      "2019-05-21 00:38:37 Iter 21600 [Train]: loss=131.40, epe=4.17, lr=0.000010, samples/sec=20.6, sec/step=1.164, eta=2 days, 9:42:09\n",
      "2019-05-21 00:40:43 Iter 21700 [Train]: loss=131.30, epe=4.17, lr=0.000010, samples/sec=20.6, sec/step=1.166, eta=2 days, 9:44:39\n",
      "2019-05-21 00:42:52 Iter 21800 [Train]: loss=130.59, epe=4.15, lr=0.000010, samples/sec=20.1, sec/step=1.191, eta=2 days, 10:58:03\n",
      "2019-05-21 00:44:58 Iter 21900 [Train]: loss=131.32, epe=4.17, lr=0.000010, samples/sec=20.7, sec/step=1.158, eta=2 days, 9:16:30\n",
      "2019-05-21 00:47:01 Iter 22000 [Train]: loss=130.52, epe=4.15, lr=0.000010, samples/sec=21.0, sec/step=1.141, eta=2 days, 8:24:18\n",
      "2019-05-21 00:47:01 Iter 22000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-22000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-22000\n",
      "2019-05-21 00:49:15 Iter 22100 [Train]: loss=130.77, epe=4.15, lr=0.000010, samples/sec=20.8, sec/step=1.155, eta=2 days, 9:03:22\n",
      "2019-05-21 00:51:20 Iter 22200 [Train]: loss=131.07, epe=4.17, lr=0.000010, samples/sec=20.8, sec/step=1.152, eta=2 days, 8:52:17\n",
      "2019-05-21 00:53:26 Iter 22300 [Train]: loss=131.68, epe=4.19, lr=0.000010, samples/sec=20.6, sec/step=1.166, eta=2 days, 9:33:39\n",
      "2019-05-21 00:55:30 Iter 22400 [Train]: loss=131.24, epe=4.17, lr=0.000010, samples/sec=21.0, sec/step=1.144, eta=2 days, 8:27:00\n",
      "2019-05-21 00:57:36 Iter 22500 [Train]: loss=130.81, epe=4.15, lr=0.000010, samples/sec=20.7, sec/step=1.162, eta=2 days, 9:16:56\n",
      "2019-05-21 00:59:42 Iter 22600 [Train]: loss=130.82, epe=4.16, lr=0.000010, samples/sec=20.6, sec/step=1.163, eta=2 days, 9:17:36\n",
      "2019-05-21 01:01:50 Iter 22700 [Train]: loss=130.50, epe=4.15, lr=0.000010, samples/sec=20.7, sec/step=1.157, eta=2 days, 9:00:00\n",
      "2019-05-21 01:03:56 Iter 22800 [Train]: loss=131.60, epe=4.18, lr=0.000010, samples/sec=20.5, sec/step=1.169, eta=2 days, 9:31:40\n",
      "2019-05-21 01:06:03 Iter 22900 [Train]: loss=131.21, epe=4.17, lr=0.000010, samples/sec=20.4, sec/step=1.178, eta=2 days, 9:57:29\n",
      "2019-05-21 01:08:08 Iter 23000 [Train]: loss=130.58, epe=4.15, lr=0.000010, samples/sec=20.8, sec/step=1.151, eta=2 days, 8:36:13\n",
      "2019-05-21 01:08:08 Iter 23000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-23000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-23000\n",
      "2019-05-21 01:10:24 Iter 23100 [Train]: loss=131.09, epe=4.17, lr=0.000010, samples/sec=20.5, sec/step=1.170, eta=2 days, 9:28:49\n",
      "2019-05-21 01:12:29 Iter 23200 [Train]: loss=130.77, epe=4.16, lr=0.000010, samples/sec=20.9, sec/step=1.151, eta=2 days, 8:30:48\n",
      "2019-05-21 01:14:33 Iter 23300 [Train]: loss=131.01, epe=4.16, lr=0.000010, samples/sec=21.0, sec/step=1.143, eta=2 days, 8:07:10\n",
      "2019-05-21 01:16:37 Iter 23400 [Train]: loss=131.21, epe=4.17, lr=0.000010, samples/sec=20.9, sec/step=1.148, eta=2 days, 8:17:51\n",
      "2019-05-21 01:18:43 Iter 23500 [Train]: loss=130.60, epe=4.15, lr=0.000010, samples/sec=20.7, sec/step=1.160, eta=2 days, 8:53:17\n",
      "2019-05-21 01:20:49 Iter 23600 [Train]: loss=131.21, epe=4.17, lr=0.000010, samples/sec=20.5, sec/step=1.171, eta=2 days, 9:22:27\n",
      "2019-05-21 01:22:54 Iter 23700 [Train]: loss=130.51, epe=4.15, lr=0.000010, samples/sec=20.9, sec/step=1.148, eta=2 days, 8:14:37\n",
      "2019-05-21 01:25:00 Iter 23800 [Train]: loss=130.87, epe=4.16, lr=0.000010, samples/sec=20.6, sec/step=1.164, eta=2 days, 8:58:18\n",
      "2019-05-21 01:27:05 Iter 23900 [Train]: loss=130.58, epe=4.15, lr=0.000010, samples/sec=20.7, sec/step=1.160, eta=2 days, 8:45:25\n",
      "2019-05-21 01:29:12 Iter 24000 [Train]: loss=130.63, epe=4.16, lr=0.000010, samples/sec=20.6, sec/step=1.166, eta=2 days, 8:59:27\n",
      "2019-05-21 01:29:12 Iter 24000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-24000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-24000\n",
      "2019-05-21 01:31:27 Iter 24100 [Train]: loss=130.50, epe=4.15, lr=0.000010, samples/sec=20.6, sec/step=1.168, eta=2 days, 9:03:18\n",
      "2019-05-21 01:33:33 Iter 24200 [Train]: loss=130.53, epe=4.15, lr=0.000010, samples/sec=20.6, sec/step=1.166, eta=2 days, 8:56:34\n",
      "2019-05-21 01:35:39 Iter 24300 [Train]: loss=130.68, epe=4.15, lr=0.000010, samples/sec=20.7, sec/step=1.161, eta=2 days, 8:40:27\n",
      "2019-05-21 01:37:44 Iter 24400 [Train]: loss=130.86, epe=4.16, lr=0.000010, samples/sec=20.8, sec/step=1.152, eta=2 days, 8:11:02\n",
      "2019-05-21 01:39:49 Iter 24500 [Train]: loss=130.97, epe=4.16, lr=0.000010, samples/sec=20.8, sec/step=1.154, eta=2 days, 8:15:00\n",
      "2019-05-21 01:41:55 Iter 24600 [Train]: loss=131.09, epe=4.17, lr=0.000010, samples/sec=20.6, sec/step=1.164, eta=2 days, 8:43:06\n",
      "2019-05-21 01:44:02 Iter 24700 [Train]: loss=131.35, epe=4.17, lr=0.000010, samples/sec=20.5, sec/step=1.174, eta=2 days, 9:08:39\n",
      "2019-05-21 01:46:08 Iter 24800 [Train]: loss=131.12, epe=4.17, lr=0.000010, samples/sec=20.5, sec/step=1.169, eta=2 days, 8:53:49\n",
      "2019-05-21 01:48:16 Iter 24900 [Train]: loss=131.28, epe=4.17, lr=0.000010, samples/sec=20.3, sec/step=1.180, eta=2 days, 9:24:06\n",
      "2019-05-21 01:50:21 Iter 25000 [Train]: loss=130.61, epe=4.15, lr=0.000010, samples/sec=20.7, sec/step=1.158, eta=2 days, 8:17:31\n",
      "2019-05-21 01:50:21 Iter 25000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-25000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-25000\n",
      "2019-05-21 01:52:39 Iter 25100 [Train]: loss=130.39, epe=4.14, lr=0.000010, samples/sec=20.4, sec/step=1.177, eta=2 days, 9:09:47\n",
      "2019-05-21 01:54:44 Iter 25200 [Train]: loss=130.84, epe=4.16, lr=0.000010, samples/sec=20.8, sec/step=1.155, eta=2 days, 8:04:51\n",
      "2019-05-21 01:56:50 Iter 25300 [Train]: loss=130.79, epe=4.16, lr=0.000010, samples/sec=20.6, sec/step=1.166, eta=2 days, 8:34:03\n",
      "2019-05-21 01:58:55 Iter 25400 [Train]: loss=130.32, epe=4.15, lr=0.000010, samples/sec=20.8, sec/step=1.155, eta=2 days, 7:59:38\n",
      "2019-05-21 02:01:02 Iter 25500 [Train]: loss=130.64, epe=4.15, lr=0.000010, samples/sec=20.5, sec/step=1.173, eta=2 days, 8:52:01\n",
      "2019-05-21 02:03:06 Iter 25600 [Train]: loss=130.32, epe=4.15, lr=0.000010, samples/sec=21.1, sec/step=1.136, eta=2 days, 7:01:46\n",
      "2019-05-21 02:05:10 Iter 25700 [Train]: loss=130.26, epe=4.14, lr=0.000010, samples/sec=20.8, sec/step=1.153, eta=2 days, 7:48:29\n",
      "2019-05-21 02:07:14 Iter 25800 [Train]: loss=130.58, epe=4.15, lr=0.000010, samples/sec=21.0, sec/step=1.142, eta=2 days, 7:16:02\n",
      "2019-05-21 02:09:19 Iter 25900 [Train]: loss=130.68, epe=4.15, lr=0.000010, samples/sec=20.8, sec/step=1.155, eta=2 days, 7:51:15\n",
      "2019-05-21 02:11:24 Iter 26000 [Train]: loss=129.78, epe=4.13, lr=0.000010, samples/sec=20.9, sec/step=1.147, eta=2 days, 7:26:53\n",
      "2019-05-21 02:11:24 Iter 26000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-26000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-26000\n",
      "2019-05-21 02:13:40 Iter 26100 [Train]: loss=130.36, epe=4.14, lr=0.000010, samples/sec=20.7, sec/step=1.158, eta=2 days, 7:56:37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 02:15:50 Iter 26200 [Train]: loss=130.25, epe=4.14, lr=0.000010, samples/sec=20.0, sec/step=1.198, eta=2 days, 9:50:18\n",
      "2019-05-21 02:17:57 Iter 26300 [Train]: loss=130.54, epe=4.15, lr=0.000010, samples/sec=20.6, sec/step=1.167, eta=2 days, 8:18:57\n",
      "2019-05-21 02:20:03 Iter 26400 [Train]: loss=130.22, epe=4.14, lr=0.000010, samples/sec=20.7, sec/step=1.161, eta=2 days, 8:00:20\n",
      "2019-05-21 02:22:07 Iter 26500 [Train]: loss=130.72, epe=4.16, lr=0.000010, samples/sec=21.0, sec/step=1.141, eta=2 days, 6:58:55\n",
      "2019-05-21 02:24:12 Iter 26600 [Train]: loss=131.00, epe=4.17, lr=0.000010, samples/sec=20.8, sec/step=1.152, eta=2 days, 7:28:05\n",
      "2019-05-21 02:26:16 Iter 26700 [Train]: loss=130.11, epe=4.14, lr=0.000010, samples/sec=21.0, sec/step=1.144, eta=2 days, 7:04:19\n",
      "2019-05-21 02:28:22 Iter 26800 [Train]: loss=130.35, epe=4.14, lr=0.000010, samples/sec=20.7, sec/step=1.161, eta=2 days, 7:52:52\n",
      "2019-05-21 02:30:27 Iter 26900 [Train]: loss=130.23, epe=4.14, lr=0.000010, samples/sec=20.7, sec/step=1.159, eta=2 days, 7:44:12\n",
      "2019-05-21 02:32:33 Iter 27000 [Train]: loss=130.93, epe=4.17, lr=0.000010, samples/sec=20.8, sec/step=1.155, eta=2 days, 7:29:16\n",
      "2019-05-21 02:32:33 Iter 27000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-27000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-27000\n",
      "2019-05-21 02:34:52 Iter 27100 [Train]: loss=130.33, epe=4.15, lr=0.000010, samples/sec=20.3, sec/step=1.185, eta=2 days, 8:53:25\n",
      "2019-05-21 02:36:59 Iter 27200 [Train]: loss=130.44, epe=4.16, lr=0.000010, samples/sec=20.4, sec/step=1.176, eta=2 days, 8:27:40\n",
      "2019-05-21 02:39:05 Iter 27300 [Train]: loss=130.15, epe=4.14, lr=0.000010, samples/sec=20.6, sec/step=1.165, eta=2 days, 7:53:03\n",
      "2019-05-21 02:41:11 Iter 27400 [Train]: loss=130.45, epe=4.15, lr=0.000010, samples/sec=20.7, sec/step=1.161, eta=2 days, 7:40:50\n",
      "2019-05-21 02:43:17 Iter 27500 [Train]: loss=129.91, epe=4.13, lr=0.000010, samples/sec=20.6, sec/step=1.166, eta=2 days, 7:52:39\n",
      "2019-05-21 02:45:23 Iter 27600 [Train]: loss=131.82, epe=4.19, lr=0.000010, samples/sec=20.8, sec/step=1.156, eta=2 days, 7:22:44\n",
      "2019-05-21 02:47:31 Iter 27700 [Train]: loss=129.75, epe=4.13, lr=0.000010, samples/sec=20.4, sec/step=1.177, eta=2 days, 8:19:00\n",
      "2019-05-21 02:49:40 Iter 27800 [Train]: loss=129.79, epe=4.13, lr=0.000010, samples/sec=20.2, sec/step=1.185, eta=2 days, 8:41:51\n",
      "2019-05-21 02:51:47 Iter 27900 [Train]: loss=130.62, epe=4.16, lr=0.000010, samples/sec=20.5, sec/step=1.168, eta=2 days, 7:51:06\n",
      "2019-05-21 02:53:54 Iter 28000 [Train]: loss=130.32, epe=4.14, lr=0.000010, samples/sec=20.6, sec/step=1.167, eta=2 days, 7:44:32\n",
      "2019-05-21 02:53:54 Iter 28000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-28000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-28000\n",
      "2019-05-21 02:56:10 Iter 28100 [Train]: loss=130.72, epe=4.16, lr=0.000010, samples/sec=20.5, sec/step=1.170, eta=2 days, 7:52:07\n",
      "2019-05-21 02:58:15 Iter 28200 [Train]: loss=130.41, epe=4.15, lr=0.000010, samples/sec=20.8, sec/step=1.152, eta=2 days, 6:59:04\n",
      "2019-05-21 03:00:22 Iter 28300 [Train]: loss=130.47, epe=4.15, lr=0.000010, samples/sec=20.4, sec/step=1.177, eta=2 days, 8:08:42\n",
      "2019-05-21 03:02:28 Iter 28400 [Train]: loss=129.89, epe=4.13, lr=0.000010, samples/sec=20.7, sec/step=1.161, eta=2 days, 7:19:53\n",
      "2019-05-21 03:04:34 Iter 28500 [Train]: loss=130.85, epe=4.16, lr=0.000010, samples/sec=20.6, sec/step=1.164, eta=2 days, 7:26:40\n",
      "2019-05-21 03:06:38 Iter 28600 [Train]: loss=129.72, epe=4.13, lr=0.000010, samples/sec=20.8, sec/step=1.152, eta=2 days, 6:50:42\n",
      "2019-05-21 03:08:46 Iter 28700 [Train]: loss=130.03, epe=4.14, lr=0.000010, samples/sec=20.4, sec/step=1.176, eta=2 days, 7:56:55\n",
      "2019-05-21 03:10:50 Iter 28800 [Train]: loss=130.75, epe=4.16, lr=0.000010, samples/sec=20.8, sec/step=1.154, eta=2 days, 6:52:28\n",
      "2019-05-21 03:12:56 Iter 28900 [Train]: loss=130.32, epe=4.15, lr=0.000010, samples/sec=20.8, sec/step=1.156, eta=2 days, 6:56:27\n",
      "2019-05-21 03:14:59 Iter 29000 [Train]: loss=130.23, epe=4.14, lr=0.000010, samples/sec=21.0, sec/step=1.141, eta=2 days, 6:10:46\n",
      "2019-05-21 03:14:59 Iter 29000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-29000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-29000\n",
      "2019-05-21 03:17:15 Iter 29100 [Train]: loss=130.16, epe=4.14, lr=0.000010, samples/sec=20.7, sec/step=1.159, eta=2 days, 7:01:15\n",
      "2019-05-21 03:19:19 Iter 29200 [Train]: loss=130.30, epe=4.14, lr=0.000010, samples/sec=21.0, sec/step=1.144, eta=2 days, 6:17:15\n",
      "2019-05-21 03:21:24 Iter 29300 [Train]: loss=129.63, epe=4.13, lr=0.000010, samples/sec=20.7, sec/step=1.159, eta=2 days, 6:56:52\n",
      "2019-05-21 03:23:29 Iter 29400 [Train]: loss=130.86, epe=4.17, lr=0.000010, samples/sec=20.9, sec/step=1.151, eta=2 days, 6:32:17\n",
      "2019-05-21 03:25:38 Iter 29500 [Train]: loss=130.61, epe=4.15, lr=0.000010, samples/sec=20.1, sec/step=1.192, eta=2 days, 8:26:56\n",
      "2019-05-21 03:27:44 Iter 29600 [Train]: loss=129.80, epe=4.13, lr=0.000010, samples/sec=20.6, sec/step=1.168, eta=2 days, 7:16:22\n",
      "2019-05-21 03:29:50 Iter 29700 [Train]: loss=130.73, epe=4.16, lr=0.000010, samples/sec=20.8, sec/step=1.156, eta=2 days, 6:40:01\n",
      "2019-05-21 03:31:54 Iter 29800 [Train]: loss=130.03, epe=4.14, lr=0.000010, samples/sec=20.9, sec/step=1.148, eta=2 days, 6:15:59\n",
      "2019-05-21 03:34:00 Iter 29900 [Train]: loss=130.34, epe=4.15, lr=0.000010, samples/sec=20.6, sec/step=1.164, eta=2 days, 7:01:19\n",
      "2019-05-21 03:36:05 Iter 30000 [Train]: loss=130.24, epe=4.15, lr=0.000010, samples/sec=21.0, sec/step=1.145, eta=2 days, 6:05:06\n",
      "2019-05-21 03:36:05 Iter 30000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-30000\n",
      "2019-05-21 03:38:22 Iter 30100 [Train]: loss=130.53, epe=4.15, lr=0.000010, samples/sec=20.8, sec/step=1.156, eta=2 days, 6:33:40\n",
      "2019-05-21 03:40:28 Iter 30200 [Train]: loss=130.42, epe=4.16, lr=0.000010, samples/sec=20.6, sec/step=1.164, eta=2 days, 6:53:36\n",
      "2019-05-21 03:42:34 Iter 30300 [Train]: loss=130.46, epe=4.16, lr=0.000010, samples/sec=20.8, sec/step=1.155, eta=2 days, 6:25:33\n",
      "2019-05-21 03:44:41 Iter 30400 [Train]: loss=129.87, epe=4.13, lr=0.000010, samples/sec=20.6, sec/step=1.164, eta=2 days, 6:50:50\n",
      "2019-05-21 03:46:45 Iter 30500 [Train]: loss=130.73, epe=4.16, lr=0.000010, samples/sec=20.8, sec/step=1.155, eta=2 days, 6:22:31\n",
      "2019-05-21 03:48:53 Iter 30600 [Train]: loss=129.85, epe=4.13, lr=0.000010, samples/sec=20.2, sec/step=1.185, eta=2 days, 7:47:03\n",
      "2019-05-21 03:51:00 Iter 30700 [Train]: loss=130.20, epe=4.15, lr=0.000010, samples/sec=20.5, sec/step=1.169, eta=2 days, 6:58:59\n",
      "2019-05-21 03:53:07 Iter 30800 [Train]: loss=129.79, epe=4.13, lr=0.000010, samples/sec=20.4, sec/step=1.176, eta=2 days, 7:15:15\n",
      "2019-05-21 03:55:13 Iter 30900 [Train]: loss=129.70, epe=4.13, lr=0.000010, samples/sec=20.7, sec/step=1.157, eta=2 days, 6:20:58\n",
      "2019-05-21 03:57:18 Iter 31000 [Train]: loss=130.08, epe=4.14, lr=0.000010, samples/sec=20.8, sec/step=1.153, eta=2 days, 6:07:40\n",
      "2019-05-21 03:57:18 Iter 31000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-31000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-31000\n",
      "2019-05-21 03:59:35 Iter 31100 [Train]: loss=130.27, epe=4.15, lr=0.000010, samples/sec=20.5, sec/step=1.171, eta=2 days, 6:57:07\n",
      "2019-05-21 04:01:40 Iter 31200 [Train]: loss=129.86, epe=4.13, lr=0.000010, samples/sec=20.7, sec/step=1.157, eta=2 days, 6:14:35\n",
      "2019-05-21 04:03:45 Iter 31300 [Train]: loss=130.26, epe=4.15, lr=0.000010, samples/sec=20.8, sec/step=1.153, eta=2 days, 6:01:39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 04:05:50 Iter 31400 [Train]: loss=130.38, epe=4.15, lr=0.000010, samples/sec=20.8, sec/step=1.152, eta=2 days, 5:56:05\n",
      "2019-05-21 04:07:57 Iter 31500 [Train]: loss=129.18, epe=4.11, lr=0.000010, samples/sec=20.5, sec/step=1.169, eta=2 days, 6:42:39\n",
      "2019-05-21 04:10:02 Iter 31600 [Train]: loss=130.06, epe=4.14, lr=0.000010, samples/sec=20.8, sec/step=1.151, eta=2 days, 5:51:38\n",
      "2019-05-21 04:12:08 Iter 31700 [Train]: loss=129.39, epe=4.12, lr=0.000010, samples/sec=20.6, sec/step=1.164, eta=2 days, 6:24:00\n",
      "2019-05-21 04:14:15 Iter 31800 [Train]: loss=129.36, epe=4.12, lr=0.000010, samples/sec=20.4, sec/step=1.177, eta=2 days, 7:00:20\n",
      "2019-05-21 04:16:21 Iter 31900 [Train]: loss=129.23, epe=4.11, lr=0.000010, samples/sec=20.8, sec/step=1.153, eta=2 days, 5:49:07\n",
      "2019-05-21 04:18:27 Iter 32000 [Train]: loss=129.74, epe=4.14, lr=0.000010, samples/sec=20.5, sec/step=1.169, eta=2 days, 6:33:39\n",
      "2019-05-21 04:18:27 Iter 32000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-32000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-32000\n",
      "2019-05-21 04:20:44 Iter 32100 [Train]: loss=129.62, epe=4.13, lr=0.000010, samples/sec=20.4, sec/step=1.178, eta=2 days, 6:56:05\n",
      "2019-05-21 04:22:49 Iter 32200 [Train]: loss=130.17, epe=4.14, lr=0.000010, samples/sec=20.8, sec/step=1.156, eta=2 days, 5:52:27\n",
      "2019-05-21 04:24:55 Iter 32300 [Train]: loss=130.11, epe=4.14, lr=0.000010, samples/sec=20.6, sec/step=1.163, eta=2 days, 6:11:54\n",
      "2019-05-21 04:27:00 Iter 32400 [Train]: loss=129.59, epe=4.12, lr=0.000010, samples/sec=20.7, sec/step=1.161, eta=2 days, 6:01:50\n",
      "2019-05-21 04:29:08 Iter 32500 [Train]: loss=129.68, epe=4.13, lr=0.000010, samples/sec=20.4, sec/step=1.177, eta=2 days, 6:45:52\n",
      "2019-05-21 04:31:11 Iter 32600 [Train]: loss=129.65, epe=4.13, lr=0.000010, samples/sec=21.2, sec/step=1.131, eta=2 days, 4:36:48\n",
      "2019-05-21 04:33:16 Iter 32700 [Train]: loss=130.87, epe=4.17, lr=0.000010, samples/sec=20.7, sec/step=1.161, eta=2 days, 5:55:57\n",
      "2019-05-21 04:35:25 Iter 32800 [Train]: loss=129.43, epe=4.12, lr=0.000010, samples/sec=20.3, sec/step=1.181, eta=2 days, 6:49:58\n",
      "2019-05-21 04:37:30 Iter 32900 [Train]: loss=130.30, epe=4.15, lr=0.000010, samples/sec=20.8, sec/step=1.156, eta=2 days, 5:39:36\n",
      "2019-05-21 04:39:38 Iter 33000 [Train]: loss=129.69, epe=4.13, lr=0.000010, samples/sec=20.4, sec/step=1.174, eta=2 days, 6:27:32\n",
      "2019-05-21 04:39:38 Iter 33000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-33000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-33000\n",
      "2019-05-21 04:41:55 Iter 33100 [Train]: loss=129.68, epe=4.13, lr=0.000010, samples/sec=20.5, sec/step=1.168, eta=2 days, 6:09:10\n",
      "2019-05-21 04:44:01 Iter 33200 [Train]: loss=130.15, epe=4.14, lr=0.000010, samples/sec=20.6, sec/step=1.165, eta=2 days, 5:57:49\n",
      "2019-05-21 04:46:09 Iter 33300 [Train]: loss=130.35, epe=4.15, lr=0.000010, samples/sec=20.3, sec/step=1.185, eta=2 days, 6:52:27\n",
      "2019-05-21 04:48:14 Iter 33400 [Train]: loss=129.31, epe=4.12, lr=0.000010, samples/sec=20.8, sec/step=1.152, eta=2 days, 5:20:03\n",
      "2019-05-21 04:50:19 Iter 33500 [Train]: loss=129.40, epe=4.12, lr=0.000010, samples/sec=20.7, sec/step=1.161, eta=2 days, 5:42:07\n",
      "2019-05-21 04:52:25 Iter 33600 [Train]: loss=129.28, epe=4.12, lr=0.000010, samples/sec=20.7, sec/step=1.160, eta=2 days, 5:36:14\n",
      "2019-05-21 04:54:29 Iter 33700 [Train]: loss=129.81, epe=4.14, lr=0.000010, samples/sec=20.8, sec/step=1.155, eta=2 days, 5:20:04\n",
      "2019-05-21 04:56:34 Iter 33800 [Train]: loss=129.76, epe=4.13, lr=0.000010, samples/sec=20.8, sec/step=1.152, eta=2 days, 5:10:06\n",
      "2019-05-21 04:58:39 Iter 33900 [Train]: loss=130.00, epe=4.14, lr=0.000010, samples/sec=20.8, sec/step=1.154, eta=2 days, 5:16:00\n",
      "2019-05-21 05:00:45 Iter 34000 [Train]: loss=129.77, epe=4.13, lr=0.000010, samples/sec=20.6, sec/step=1.167, eta=2 days, 5:47:46\n",
      "2019-05-21 05:00:45 Iter 34000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-34000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-34000\n",
      "2019-05-21 05:03:02 Iter 34100 [Train]: loss=130.63, epe=4.16, lr=0.000010, samples/sec=20.6, sec/step=1.167, eta=2 days, 5:46:05\n",
      "2019-05-21 05:05:08 Iter 34200 [Train]: loss=129.34, epe=4.13, lr=0.000010, samples/sec=20.6, sec/step=1.165, eta=2 days, 5:38:06\n",
      "2019-05-21 05:07:14 Iter 34300 [Train]: loss=128.89, epe=4.10, lr=0.000010, samples/sec=20.7, sec/step=1.160, eta=2 days, 5:22:57\n",
      "2019-05-21 05:09:22 Iter 34400 [Train]: loss=129.80, epe=4.14, lr=0.000010, samples/sec=20.4, sec/step=1.179, eta=2 days, 6:13:03\n",
      "2019-05-21 05:11:27 Iter 34500 [Train]: loss=129.30, epe=4.12, lr=0.000010, samples/sec=20.7, sec/step=1.158, eta=2 days, 5:14:50\n",
      "2019-05-21 05:13:34 Iter 34600 [Train]: loss=129.80, epe=4.14, lr=0.000010, samples/sec=20.5, sec/step=1.172, eta=2 days, 5:50:47\n",
      "2019-05-21 05:15:41 Iter 34700 [Train]: loss=129.22, epe=4.12, lr=0.000010, samples/sec=20.4, sec/step=1.176, eta=2 days, 6:01:08\n",
      "2019-05-21 05:17:47 Iter 34800 [Train]: loss=129.18, epe=4.12, lr=0.000010, samples/sec=20.7, sec/step=1.158, eta=2 days, 5:07:35\n",
      "2019-05-21 05:19:53 Iter 34900 [Train]: loss=129.57, epe=4.13, lr=0.000010, samples/sec=20.5, sec/step=1.170, eta=2 days, 5:38:22\n",
      "2019-05-21 05:22:00 Iter 35000 [Train]: loss=128.71, epe=4.10, lr=0.000010, samples/sec=20.5, sec/step=1.168, eta=2 days, 5:32:17\n",
      "2019-05-21 05:22:00 Iter 35000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-35000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-35000\n",
      "2019-05-21 05:24:13 Iter 35100 [Train]: loss=129.37, epe=4.12, lr=0.000010, samples/sec=20.9, sec/step=1.147, eta=2 days, 4:31:02\n",
      "2019-05-21 05:26:20 Iter 35200 [Train]: loss=129.28, epe=4.12, lr=0.000010, samples/sec=20.6, sec/step=1.167, eta=2 days, 5:25:41\n",
      "2019-05-21 05:28:24 Iter 35300 [Train]: loss=129.76, epe=4.14, lr=0.000010, samples/sec=20.9, sec/step=1.150, eta=2 days, 4:36:47\n",
      "2019-05-21 05:30:30 Iter 35400 [Train]: loss=128.99, epe=4.11, lr=0.000010, samples/sec=20.6, sec/step=1.165, eta=2 days, 5:16:33\n",
      "2019-05-21 05:32:37 Iter 35500 [Train]: loss=129.76, epe=4.14, lr=0.000010, samples/sec=20.5, sec/step=1.168, eta=2 days, 5:23:04\n",
      "2019-05-21 05:34:42 Iter 35600 [Train]: loss=129.00, epe=4.11, lr=0.000010, samples/sec=20.7, sec/step=1.159, eta=2 days, 4:55:06\n",
      "2019-05-21 05:36:50 Iter 35700 [Train]: loss=129.05, epe=4.11, lr=0.000010, samples/sec=20.2, sec/step=1.186, eta=2 days, 6:07:08\n",
      "2019-05-21 05:38:58 Iter 35800 [Train]: loss=129.58, epe=4.13, lr=0.000010, samples/sec=20.4, sec/step=1.176, eta=2 days, 5:37:20\n",
      "2019-05-21 05:41:04 Iter 35900 [Train]: loss=129.24, epe=4.12, lr=0.000010, samples/sec=20.6, sec/step=1.167, eta=2 days, 5:10:55\n",
      "2019-05-21 05:43:10 Iter 36000 [Train]: loss=129.75, epe=4.13, lr=0.000010, samples/sec=20.6, sec/step=1.163, eta=2 days, 4:59:07\n",
      "2019-05-21 05:43:10 Iter 36000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-36000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-36000\n",
      "2019-05-21 05:45:27 Iter 36100 [Train]: loss=128.90, epe=4.11, lr=0.000010, samples/sec=20.5, sec/step=1.173, eta=2 days, 5:24:06\n",
      "2019-05-21 05:47:32 Iter 36200 [Train]: loss=129.56, epe=4.13, lr=0.000010, samples/sec=20.9, sec/step=1.151, eta=2 days, 4:21:08\n",
      "2019-05-21 05:49:39 Iter 36300 [Train]: loss=129.72, epe=4.14, lr=0.000010, samples/sec=20.5, sec/step=1.172, eta=2 days, 5:18:36\n",
      "2019-05-21 05:51:45 Iter 36400 [Train]: loss=129.70, epe=4.13, lr=0.000010, samples/sec=20.6, sec/step=1.168, eta=2 days, 5:04:10\n",
      "2019-05-21 05:53:54 Iter 36500 [Train]: loss=129.56, epe=4.13, lr=0.000010, samples/sec=20.3, sec/step=1.182, eta=2 days, 5:41:16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 05:55:58 Iter 36600 [Train]: loss=129.22, epe=4.12, lr=0.000010, samples/sec=20.9, sec/step=1.151, eta=2 days, 4:14:44\n",
      "2019-05-21 05:58:05 Iter 36700 [Train]: loss=129.00, epe=4.11, lr=0.000010, samples/sec=20.5, sec/step=1.172, eta=2 days, 5:10:19\n",
      "2019-05-21 06:00:13 Iter 36800 [Train]: loss=129.16, epe=4.11, lr=0.000010, samples/sec=20.3, sec/step=1.180, eta=2 days, 5:29:02\n",
      "2019-05-21 06:02:20 Iter 36900 [Train]: loss=129.17, epe=4.12, lr=0.000010, samples/sec=20.5, sec/step=1.168, eta=2 days, 4:55:14\n",
      "2019-05-21 06:04:25 Iter 37000 [Train]: loss=129.29, epe=4.12, lr=0.000010, samples/sec=20.7, sec/step=1.159, eta=2 days, 4:29:37\n",
      "2019-05-21 06:04:25 Iter 37000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-37000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-37000\n",
      "2019-05-21 06:06:44 Iter 37100 [Train]: loss=129.49, epe=4.13, lr=0.000010, samples/sec=20.4, sec/step=1.177, eta=2 days, 5:15:12\n",
      "2019-05-21 06:08:50 Iter 37200 [Train]: loss=128.46, epe=4.09, lr=0.000010, samples/sec=20.7, sec/step=1.162, eta=2 days, 4:32:54\n",
      "2019-05-21 06:10:55 Iter 37300 [Train]: loss=129.73, epe=4.13, lr=0.000010, samples/sec=20.8, sec/step=1.155, eta=2 days, 4:11:55\n",
      "2019-05-21 06:13:03 Iter 37400 [Train]: loss=128.71, epe=4.10, lr=0.000010, samples/sec=20.3, sec/step=1.179, eta=2 days, 5:16:21\n",
      "2019-05-21 06:15:09 Iter 37500 [Train]: loss=129.49, epe=4.13, lr=0.000010, samples/sec=20.6, sec/step=1.166, eta=2 days, 4:37:05\n",
      "2019-05-21 06:17:16 Iter 37600 [Train]: loss=129.33, epe=4.12, lr=0.000010, samples/sec=20.5, sec/step=1.170, eta=2 days, 4:48:07\n",
      "2019-05-21 06:19:21 Iter 37700 [Train]: loss=129.26, epe=4.12, lr=0.000010, samples/sec=20.6, sec/step=1.165, eta=2 days, 4:31:23\n",
      "2019-05-21 06:21:28 Iter 37800 [Train]: loss=128.94, epe=4.11, lr=0.000010, samples/sec=20.5, sec/step=1.171, eta=2 days, 4:46:38\n",
      "2019-05-21 06:23:36 Iter 37900 [Train]: loss=129.39, epe=4.13, lr=0.000010, samples/sec=20.5, sec/step=1.170, eta=2 days, 4:40:13\n",
      "2019-05-21 06:25:42 Iter 38000 [Train]: loss=128.82, epe=4.11, lr=0.000010, samples/sec=20.5, sec/step=1.169, eta=2 days, 4:36:18\n",
      "2019-05-21 06:25:42 Iter 38000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-38000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-38000\n",
      "2019-05-21 06:28:01 Iter 38100 [Train]: loss=129.68, epe=4.13, lr=0.000010, samples/sec=20.8, sec/step=1.154, eta=2 days, 3:55:09\n",
      "2019-05-21 06:30:11 Iter 38200 [Train]: loss=129.51, epe=4.13, lr=0.000010, samples/sec=20.0, sec/step=1.200, eta=2 days, 5:55:31\n",
      "2019-05-21 06:32:16 Iter 38300 [Train]: loss=128.89, epe=4.11, lr=0.000010, samples/sec=20.8, sec/step=1.153, eta=2 days, 3:47:14\n",
      "2019-05-21 06:34:21 Iter 38400 [Train]: loss=129.44, epe=4.13, lr=0.000010, samples/sec=20.7, sec/step=1.157, eta=2 days, 3:56:30\n",
      "2019-05-21 06:36:31 Iter 38500 [Train]: loss=129.39, epe=4.13, lr=0.000010, samples/sec=20.2, sec/step=1.186, eta=2 days, 5:12:26\n",
      "2019-05-21 06:38:36 Iter 38600 [Train]: loss=128.88, epe=4.11, lr=0.000010, samples/sec=20.8, sec/step=1.156, eta=2 days, 3:50:14\n",
      "2019-05-21 06:40:41 Iter 38700 [Train]: loss=129.25, epe=4.12, lr=0.000010, samples/sec=20.9, sec/step=1.148, eta=2 days, 3:26:56\n",
      "2019-05-21 06:42:47 Iter 38800 [Train]: loss=129.02, epe=4.12, lr=0.000010, samples/sec=20.5, sec/step=1.168, eta=2 days, 4:19:13\n",
      "2019-05-21 06:44:56 Iter 38900 [Train]: loss=129.09, epe=4.12, lr=0.000010, samples/sec=20.3, sec/step=1.183, eta=2 days, 4:56:37\n",
      "2019-05-21 06:47:02 Iter 39000 [Train]: loss=129.12, epe=4.12, lr=0.000010, samples/sec=20.6, sec/step=1.163, eta=2 days, 4:00:09\n",
      "2019-05-21 06:47:02 Iter 39000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-39000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-39000\n",
      "2019-05-21 06:49:19 Iter 39100 [Train]: loss=129.26, epe=4.12, lr=0.000010, samples/sec=20.6, sec/step=1.166, eta=2 days, 4:07:53\n",
      "2019-05-21 06:51:25 Iter 39200 [Train]: loss=129.17, epe=4.12, lr=0.000010, samples/sec=20.7, sec/step=1.161, eta=2 days, 3:51:56\n",
      "2019-05-21 06:53:30 Iter 39300 [Train]: loss=129.92, epe=4.14, lr=0.000010, samples/sec=20.8, sec/step=1.153, eta=2 days, 3:27:10\n",
      "2019-05-21 06:55:36 Iter 39400 [Train]: loss=129.45, epe=4.13, lr=0.000010, samples/sec=20.5, sec/step=1.169, eta=2 days, 4:09:12\n",
      "2019-05-21 06:57:41 Iter 39500 [Train]: loss=129.62, epe=4.13, lr=0.000010, samples/sec=20.9, sec/step=1.147, eta=2 days, 3:09:22\n",
      "2019-05-21 06:59:48 Iter 39600 [Train]: loss=128.83, epe=4.11, lr=0.000010, samples/sec=20.3, sec/step=1.180, eta=2 days, 4:34:05\n",
      "2019-05-21 07:01:53 Iter 39700 [Train]: loss=129.33, epe=4.12, lr=0.000010, samples/sec=20.8, sec/step=1.155, eta=2 days, 3:24:58\n",
      "2019-05-21 07:04:01 Iter 39800 [Train]: loss=128.91, epe=4.11, lr=0.000010, samples/sec=20.4, sec/step=1.176, eta=2 days, 4:20:32\n",
      "2019-05-21 07:06:09 Iter 39900 [Train]: loss=129.24, epe=4.12, lr=0.000010, samples/sec=20.4, sec/step=1.174, eta=2 days, 4:13:53\n",
      "2019-05-21 07:08:17 Iter 40000 [Train]: loss=129.30, epe=4.12, lr=0.000010, samples/sec=20.4, sec/step=1.178, eta=2 days, 4:22:26\n",
      "2019-05-21 07:08:17 Iter 40000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-40000\n",
      "2019-05-21 07:10:36 Iter 40100 [Train]: loss=128.16, epe=4.09, lr=0.000010, samples/sec=20.4, sec/step=1.175, eta=2 days, 4:12:19\n",
      "2019-05-21 07:12:43 Iter 40200 [Train]: loss=129.21, epe=4.12, lr=0.000010, samples/sec=20.5, sec/step=1.169, eta=2 days, 3:54:31\n",
      "2019-05-21 07:14:49 Iter 40300 [Train]: loss=129.15, epe=4.12, lr=0.000010, samples/sec=20.7, sec/step=1.161, eta=2 days, 3:30:03\n",
      "2019-05-21 07:16:53 Iter 40400 [Train]: loss=129.29, epe=4.12, lr=0.000010, samples/sec=20.9, sec/step=1.151, eta=2 days, 3:01:23\n",
      "2019-05-21 07:18:58 Iter 40500 [Train]: loss=129.52, epe=4.13, lr=0.000010, samples/sec=20.7, sec/step=1.160, eta=2 days, 3:24:32\n",
      "2019-05-21 07:21:04 Iter 40600 [Train]: loss=129.13, epe=4.12, lr=0.000010, samples/sec=20.7, sec/step=1.159, eta=2 days, 3:19:18\n",
      "2019-05-21 07:23:09 Iter 40700 [Train]: loss=128.95, epe=4.11, lr=0.000010, samples/sec=20.8, sec/step=1.155, eta=2 days, 3:06:07\n",
      "2019-05-21 07:25:17 Iter 40800 [Train]: loss=128.33, epe=4.09, lr=0.000010, samples/sec=20.3, sec/step=1.184, eta=2 days, 4:22:10\n",
      "2019-05-21 07:27:21 Iter 40900 [Train]: loss=128.63, epe=4.10, lr=0.000010, samples/sec=21.0, sec/step=1.143, eta=2 days, 2:31:10\n",
      "2019-05-21 07:29:27 Iter 41000 [Train]: loss=128.59, epe=4.10, lr=0.000010, samples/sec=20.7, sec/step=1.162, eta=2 days, 3:18:38\n",
      "2019-05-21 07:29:27 Iter 41000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-41000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-41000\n",
      "2019-05-21 07:31:48 Iter 41100 [Train]: loss=129.36, epe=4.13, lr=0.000010, samples/sec=20.3, sec/step=1.183, eta=2 days, 4:13:02\n",
      "2019-05-21 07:33:55 Iter 41200 [Train]: loss=128.79, epe=4.11, lr=0.000010, samples/sec=20.6, sec/step=1.166, eta=2 days, 3:26:32\n",
      "2019-05-21 07:36:02 Iter 41300 [Train]: loss=129.14, epe=4.12, lr=0.000010, samples/sec=20.5, sec/step=1.170, eta=2 days, 3:35:04\n",
      "2019-05-21 07:38:07 Iter 41400 [Train]: loss=128.95, epe=4.11, lr=0.000010, samples/sec=20.8, sec/step=1.153, eta=2 days, 2:48:01\n",
      "2019-05-21 07:40:13 Iter 41500 [Train]: loss=128.88, epe=4.11, lr=0.000010, samples/sec=20.6, sec/step=1.166, eta=2 days, 3:20:01\n",
      "2019-05-21 07:42:20 Iter 41600 [Train]: loss=128.25, epe=4.09, lr=0.000010, samples/sec=20.4, sec/step=1.174, eta=2 days, 3:40:10\n",
      "2019-05-21 07:44:26 Iter 41700 [Train]: loss=129.33, epe=4.12, lr=0.000010, samples/sec=20.6, sec/step=1.163, eta=2 days, 3:09:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 07:46:31 Iter 41800 [Train]: loss=129.09, epe=4.12, lr=0.000010, samples/sec=20.7, sec/step=1.159, eta=2 days, 2:56:36\n",
      "2019-05-21 07:48:38 Iter 41900 [Train]: loss=129.83, epe=4.14, lr=0.000010, samples/sec=20.6, sec/step=1.163, eta=2 days, 3:05:24\n",
      "2019-05-21 07:50:45 Iter 42000 [Train]: loss=128.97, epe=4.11, lr=0.000010, samples/sec=20.5, sec/step=1.173, eta=2 days, 3:29:36\n",
      "2019-05-21 07:50:45 Iter 42000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-42000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-42000\n",
      "2019-05-21 07:53:08 Iter 42100 [Train]: loss=129.44, epe=4.13, lr=0.000010, samples/sec=20.2, sec/step=1.187, eta=2 days, 4:03:55\n",
      "2019-05-21 07:55:18 Iter 42200 [Train]: loss=129.07, epe=4.11, lr=0.000010, samples/sec=20.4, sec/step=1.175, eta=2 days, 3:30:16\n",
      "2019-05-21 07:57:23 Iter 42300 [Train]: loss=129.42, epe=4.13, lr=0.000010, samples/sec=20.9, sec/step=1.149, eta=2 days, 2:19:16\n",
      "2019-05-21 07:59:28 Iter 42400 [Train]: loss=128.82, epe=4.11, lr=0.000010, samples/sec=20.8, sec/step=1.152, eta=2 days, 2:25:23\n",
      "2019-05-21 08:01:34 Iter 42500 [Train]: loss=128.25, epe=4.09, lr=0.000010, samples/sec=20.5, sec/step=1.168, eta=2 days, 3:06:55\n",
      "2019-05-21 08:03:39 Iter 42600 [Train]: loss=129.24, epe=4.13, lr=0.000010, samples/sec=20.9, sec/step=1.151, eta=2 days, 2:19:30\n",
      "2019-05-21 08:05:46 Iter 42700 [Train]: loss=129.29, epe=4.13, lr=0.000010, samples/sec=20.5, sec/step=1.169, eta=2 days, 3:03:59\n",
      "2019-05-21 08:07:52 Iter 42800 [Train]: loss=128.76, epe=4.11, lr=0.000010, samples/sec=20.6, sec/step=1.164, eta=2 days, 2:50:41\n",
      "2019-05-21 08:10:00 Iter 42900 [Train]: loss=128.49, epe=4.10, lr=0.000010, samples/sec=20.3, sec/step=1.185, eta=2 days, 3:42:46\n",
      "2019-05-21 08:12:06 Iter 43000 [Train]: loss=129.76, epe=4.14, lr=0.000010, samples/sec=20.7, sec/step=1.159, eta=2 days, 2:33:53\n",
      "2019-05-21 08:12:06 Iter 43000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-43000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-43000\n",
      "2019-05-21 08:14:28 Iter 43100 [Train]: loss=128.93, epe=4.11, lr=0.000010, samples/sec=20.2, sec/step=1.188, eta=2 days, 3:47:16\n",
      "2019-05-21 08:16:37 Iter 43200 [Train]: loss=128.38, epe=4.09, lr=0.000010, samples/sec=20.2, sec/step=1.186, eta=2 days, 3:40:34\n",
      "2019-05-21 08:18:44 Iter 43300 [Train]: loss=129.41, epe=4.13, lr=0.000010, samples/sec=20.5, sec/step=1.171, eta=2 days, 2:58:30\n",
      "2019-05-21 08:20:50 Iter 43400 [Train]: loss=129.35, epe=4.13, lr=0.000010, samples/sec=20.7, sec/step=1.159, eta=2 days, 2:25:13\n",
      "2019-05-21 08:22:58 Iter 43500 [Train]: loss=128.41, epe=4.10, lr=0.000010, samples/sec=20.3, sec/step=1.185, eta=2 days, 3:30:03\n",
      "2019-05-21 08:25:06 Iter 43600 [Train]: loss=128.95, epe=4.11, lr=0.000010, samples/sec=20.5, sec/step=1.174, eta=2 days, 2:59:04\n",
      "2019-05-21 08:27:12 Iter 43700 [Train]: loss=128.81, epe=4.11, lr=0.000010, samples/sec=20.7, sec/step=1.159, eta=2 days, 2:19:36\n",
      "2019-05-21 08:29:19 Iter 43800 [Train]: loss=128.08, epe=4.09, lr=0.000010, samples/sec=20.4, sec/step=1.175, eta=2 days, 2:58:07\n",
      "2019-05-21 08:31:26 Iter 43900 [Train]: loss=129.11, epe=4.12, lr=0.000010, samples/sec=20.5, sec/step=1.172, eta=2 days, 2:49:21\n",
      "2019-05-21 08:33:33 Iter 44000 [Train]: loss=128.82, epe=4.11, lr=0.000010, samples/sec=20.6, sec/step=1.162, eta=2 days, 2:22:16\n",
      "2019-05-21 08:33:33 Iter 44000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-44000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-44000\n",
      "2019-05-21 08:35:53 Iter 44100 [Train]: loss=129.30, epe=4.12, lr=0.000010, samples/sec=20.6, sec/step=1.166, eta=2 days, 2:30:41\n",
      "2019-05-21 08:38:00 Iter 44200 [Train]: loss=128.84, epe=4.12, lr=0.000010, samples/sec=20.3, sec/step=1.180, eta=2 days, 3:05:16\n",
      "2019-05-21 08:40:07 Iter 44300 [Train]: loss=128.00, epe=4.09, lr=0.000010, samples/sec=20.5, sec/step=1.169, eta=2 days, 2:34:30\n",
      "2019-05-21 08:42:14 Iter 44400 [Train]: loss=129.58, epe=4.13, lr=0.000010, samples/sec=20.4, sec/step=1.174, eta=2 days, 2:44:27\n",
      "2019-05-21 08:44:21 Iter 44500 [Train]: loss=128.88, epe=4.11, lr=0.000010, samples/sec=20.4, sec/step=1.175, eta=2 days, 2:45:34\n",
      "2019-05-21 08:46:28 Iter 44600 [Train]: loss=129.00, epe=4.11, lr=0.000010, samples/sec=20.5, sec/step=1.173, eta=2 days, 2:37:00\n",
      "2019-05-21 08:48:34 Iter 44700 [Train]: loss=128.84, epe=4.11, lr=0.000010, samples/sec=20.6, sec/step=1.163, eta=2 days, 2:10:01\n",
      "2019-05-21 08:50:40 Iter 44800 [Train]: loss=128.24, epe=4.09, lr=0.000010, samples/sec=20.5, sec/step=1.168, eta=2 days, 2:21:32\n",
      "2019-05-21 08:52:46 Iter 44900 [Train]: loss=128.73, epe=4.11, lr=0.000010, samples/sec=20.6, sec/step=1.164, eta=2 days, 2:09:17\n",
      "2019-05-21 08:54:50 Iter 45000 [Train]: loss=128.74, epe=4.11, lr=0.000010, samples/sec=20.9, sec/step=1.147, eta=2 days, 1:22:13\n",
      "2019-05-21 08:54:50 Iter 45000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-45000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-45000\n",
      "2019-05-21 08:57:11 Iter 45100 [Train]: loss=129.53, epe=4.13, lr=0.000010, samples/sec=20.5, sec/step=1.172, eta=2 days, 2:25:35\n",
      "2019-05-21 08:59:21 Iter 45200 [Train]: loss=127.97, epe=4.08, lr=0.000010, samples/sec=20.2, sec/step=1.188, eta=2 days, 3:06:13\n",
      "2019-05-21 09:01:27 Iter 45300 [Train]: loss=128.86, epe=4.11, lr=0.000010, samples/sec=20.5, sec/step=1.168, eta=2 days, 2:11:28\n",
      "2019-05-21 09:03:34 Iter 45400 [Train]: loss=128.59, epe=4.11, lr=0.000010, samples/sec=20.6, sec/step=1.163, eta=2 days, 1:56:16\n",
      "2019-05-21 09:05:40 Iter 45500 [Train]: loss=129.51, epe=4.14, lr=0.000010, samples/sec=20.6, sec/step=1.167, eta=2 days, 2:04:32\n",
      "2019-05-21 09:07:47 Iter 45600 [Train]: loss=128.80, epe=4.11, lr=0.000010, samples/sec=20.5, sec/step=1.168, eta=2 days, 2:05:36\n",
      "2019-05-21 09:09:54 Iter 45700 [Train]: loss=128.49, epe=4.10, lr=0.000010, samples/sec=20.5, sec/step=1.170, eta=2 days, 2:08:50\n",
      "2019-05-21 09:12:01 Iter 45800 [Train]: loss=128.21, epe=4.09, lr=0.000010, samples/sec=20.3, sec/step=1.180, eta=2 days, 2:32:26\n",
      "2019-05-21 09:14:08 Iter 45900 [Train]: loss=128.21, epe=4.09, lr=0.000010, samples/sec=20.4, sec/step=1.177, eta=2 days, 2:23:44\n",
      "2019-05-21 09:16:16 Iter 46000 [Train]: loss=128.96, epe=4.12, lr=0.000010, samples/sec=20.4, sec/step=1.175, eta=2 days, 2:14:48\n",
      "2019-05-21 09:16:16 Iter 46000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-46000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-46000\n",
      "2019-05-21 09:18:39 Iter 46100 [Train]: loss=128.64, epe=4.11, lr=0.000010, samples/sec=20.2, sec/step=1.187, eta=2 days, 2:45:25\n",
      "2019-05-21 09:20:45 Iter 46200 [Train]: loss=128.02, epe=4.09, lr=0.000010, samples/sec=20.7, sec/step=1.158, eta=2 days, 1:28:34\n",
      "2019-05-21 09:22:53 Iter 46300 [Train]: loss=129.33, epe=4.12, lr=0.000010, samples/sec=20.3, sec/step=1.185, eta=2 days, 2:34:49\n",
      "2019-05-21 09:25:00 Iter 46400 [Train]: loss=128.27, epe=4.09, lr=0.000010, samples/sec=20.5, sec/step=1.172, eta=2 days, 2:00:35\n",
      "2019-05-21 09:27:07 Iter 46500 [Train]: loss=128.31, epe=4.10, lr=0.000010, samples/sec=20.6, sec/step=1.166, eta=2 days, 1:41:56\n",
      "2019-05-21 09:29:13 Iter 46600 [Train]: loss=129.31, epe=4.13, lr=0.000010, samples/sec=20.6, sec/step=1.166, eta=2 days, 1:41:28\n",
      "2019-05-21 09:31:17 Iter 46700 [Train]: loss=128.11, epe=4.09, lr=0.000010, samples/sec=20.9, sec/step=1.148, eta=2 days, 0:53:13\n",
      "2019-05-21 09:33:21 Iter 46800 [Train]: loss=128.88, epe=4.11, lr=0.000010, samples/sec=21.0, sec/step=1.142, eta=2 days, 0:36:51\n",
      "2019-05-21 09:35:26 Iter 46900 [Train]: loss=128.62, epe=4.11, lr=0.000010, samples/sec=20.8, sec/step=1.154, eta=2 days, 1:04:08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 09:37:31 Iter 47000 [Train]: loss=128.73, epe=4.11, lr=0.000010, samples/sec=20.7, sec/step=1.158, eta=2 days, 1:12:40\n",
      "2019-05-21 09:37:31 Iter 47000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-47000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-47000\n",
      "2019-05-21 09:39:53 Iter 47100 [Train]: loss=128.66, epe=4.11, lr=0.000010, samples/sec=20.5, sec/step=1.173, eta=2 days, 1:47:58\n",
      "2019-05-21 09:41:59 Iter 47200 [Train]: loss=128.63, epe=4.11, lr=0.000010, samples/sec=20.6, sec/step=1.163, eta=2 days, 1:20:48\n",
      "2019-05-21 09:44:03 Iter 47300 [Train]: loss=128.52, epe=4.10, lr=0.000010, samples/sec=20.9, sec/step=1.146, eta=2 days, 0:37:47\n",
      "2019-05-21 09:46:10 Iter 47400 [Train]: loss=128.34, epe=4.10, lr=0.000010, samples/sec=20.5, sec/step=1.170, eta=2 days, 1:35:09\n",
      "2019-05-21 09:48:16 Iter 47500 [Train]: loss=128.66, epe=4.11, lr=0.000010, samples/sec=20.6, sec/step=1.166, eta=2 days, 1:24:07\n",
      "2019-05-21 09:50:22 Iter 47600 [Train]: loss=128.44, epe=4.10, lr=0.000010, samples/sec=20.7, sec/step=1.160, eta=2 days, 1:05:30\n",
      "2019-05-21 09:52:28 Iter 47700 [Train]: loss=129.14, epe=4.13, lr=0.000010, samples/sec=20.7, sec/step=1.161, eta=2 days, 1:08:12\n",
      "2019-05-21 09:54:33 Iter 47800 [Train]: loss=128.38, epe=4.10, lr=0.000010, samples/sec=20.7, sec/step=1.157, eta=2 days, 0:55:17\n",
      "2019-05-21 09:56:39 Iter 47900 [Train]: loss=129.70, epe=4.14, lr=0.000010, samples/sec=20.6, sec/step=1.163, eta=2 days, 1:07:54\n",
      "2019-05-21 09:58:48 Iter 48000 [Train]: loss=128.35, epe=4.10, lr=0.000010, samples/sec=20.2, sec/step=1.188, eta=2 days, 2:08:54\n",
      "2019-05-21 09:58:48 Iter 48000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-48000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-48000\n",
      "2019-05-21 10:01:04 Iter 48100 [Train]: loss=128.34, epe=4.10, lr=0.000010, samples/sec=20.7, sec/step=1.158, eta=2 days, 0:52:45\n",
      "2019-05-21 10:03:14 Iter 48200 [Train]: loss=128.84, epe=4.11, lr=0.000010, samples/sec=20.0, sec/step=1.199, eta=2 days, 2:32:19\n",
      "2019-05-21 10:05:22 Iter 48300 [Train]: loss=128.94, epe=4.12, lr=0.000010, samples/sec=20.4, sec/step=1.178, eta=2 days, 1:37:23\n",
      "2019-05-21 10:07:28 Iter 48400 [Train]: loss=128.08, epe=4.09, lr=0.000010, samples/sec=20.6, sec/step=1.163, eta=2 days, 0:59:23\n",
      "2019-05-21 10:09:35 Iter 48500 [Train]: loss=128.49, epe=4.11, lr=0.000010, samples/sec=20.5, sec/step=1.169, eta=2 days, 1:12:13\n",
      "2019-05-21 10:11:41 Iter 48600 [Train]: loss=128.09, epe=4.09, lr=0.000010, samples/sec=20.6, sec/step=1.168, eta=2 days, 1:06:10\n",
      "2019-05-21 10:13:49 Iter 48700 [Train]: loss=128.60, epe=4.11, lr=0.000010, samples/sec=20.4, sec/step=1.177, eta=2 days, 1:28:44\n",
      "2019-05-21 10:15:55 Iter 48800 [Train]: loss=128.29, epe=4.09, lr=0.000010, samples/sec=20.7, sec/step=1.162, eta=2 days, 0:47:51\n",
      "2019-05-21 10:18:01 Iter 48900 [Train]: loss=128.56, epe=4.11, lr=0.000010, samples/sec=20.6, sec/step=1.163, eta=2 days, 0:48:05\n",
      "2019-05-21 10:20:08 Iter 49000 [Train]: loss=128.99, epe=4.12, lr=0.000010, samples/sec=20.5, sec/step=1.171, eta=2 days, 1:07:24\n",
      "2019-05-21 10:20:08 Iter 49000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-49000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-49000\n",
      "2019-05-21 10:22:27 Iter 49100 [Train]: loss=127.47, epe=4.07, lr=0.000010, samples/sec=20.6, sec/step=1.167, eta=2 days, 0:54:42\n",
      "2019-05-21 10:24:33 Iter 49200 [Train]: loss=129.23, epe=4.13, lr=0.000010, samples/sec=20.7, sec/step=1.157, eta=2 days, 0:28:57\n",
      "2019-05-21 10:26:40 Iter 49300 [Train]: loss=127.77, epe=4.08, lr=0.000010, samples/sec=20.4, sec/step=1.175, eta=2 days, 1:09:59\n",
      "2019-05-21 10:28:46 Iter 49400 [Train]: loss=129.32, epe=4.13, lr=0.000010, samples/sec=20.7, sec/step=1.159, eta=2 days, 0:28:00\n",
      "2019-05-21 10:30:53 Iter 49500 [Train]: loss=127.94, epe=4.08, lr=0.000010, samples/sec=20.4, sec/step=1.177, eta=2 days, 1:12:45\n",
      "2019-05-21 10:33:01 Iter 49600 [Train]: loss=128.11, epe=4.09, lr=0.000010, samples/sec=20.5, sec/step=1.169, eta=2 days, 0:51:11\n",
      "2019-05-21 10:35:17 Iter 49700 [Train]: loss=129.28, epe=4.13, lr=0.000010, samples/sec=18.9, sec/step=1.267, eta=2 days, 4:55:02\n",
      "2019-05-21 10:37:49 Iter 49800 [Train]: loss=128.18, epe=4.09, lr=0.000010, samples/sec=16.9, sec/step=1.419, eta=2 days, 11:11:51\n",
      "2019-05-21 10:40:24 Iter 49900 [Train]: loss=128.75, epe=4.11, lr=0.000010, samples/sec=16.5, sec/step=1.451, eta=2 days, 12:29:22\n",
      "2019-05-21 10:42:54 Iter 50000 [Train]: loss=128.39, epe=4.10, lr=0.000010, samples/sec=17.1, sec/step=1.406, eta=2 days, 10:35:39\n",
      "2019-05-21 10:42:54 Iter 50000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-50000\n",
      "2019-05-21 10:45:37 Iter 50100 [Train]: loss=128.24, epe=4.10, lr=0.000010, samples/sec=17.1, sec/step=1.406, eta=2 days, 10:32:30\n",
      "2019-05-21 10:48:03 Iter 50200 [Train]: loss=127.57, epe=4.08, lr=0.000010, samples/sec=17.5, sec/step=1.368, eta=2 days, 8:54:36\n",
      "2019-05-21 10:50:36 Iter 50300 [Train]: loss=128.79, epe=4.11, lr=0.000010, samples/sec=16.7, sec/step=1.438, eta=2 days, 11:49:03\n",
      "2019-05-21 10:53:06 Iter 50400 [Train]: loss=128.37, epe=4.10, lr=0.000010, samples/sec=17.1, sec/step=1.403, eta=2 days, 10:17:32\n",
      "2019-05-21 10:55:39 Iter 50500 [Train]: loss=128.08, epe=4.10, lr=0.000010, samples/sec=16.8, sec/step=1.430, eta=2 days, 11:23:50\n",
      "2019-05-21 10:58:08 Iter 50600 [Train]: loss=128.68, epe=4.11, lr=0.000010, samples/sec=17.2, sec/step=1.397, eta=2 days, 9:59:27\n",
      "2019-05-21 11:00:39 Iter 50700 [Train]: loss=128.26, epe=4.09, lr=0.000010, samples/sec=17.0, sec/step=1.415, eta=2 days, 10:42:09\n",
      "2019-05-21 11:03:08 Iter 50800 [Train]: loss=128.58, epe=4.11, lr=0.000010, samples/sec=17.3, sec/step=1.390, eta=2 days, 9:37:05\n",
      "2019-05-21 11:05:37 Iter 50900 [Train]: loss=128.51, epe=4.10, lr=0.000010, samples/sec=17.3, sec/step=1.387, eta=2 days, 9:27:10\n",
      "2019-05-21 11:08:09 Iter 51000 [Train]: loss=128.28, epe=4.10, lr=0.000010, samples/sec=16.9, sec/step=1.424, eta=2 days, 10:56:50\n",
      "2019-05-21 11:08:09 Iter 51000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-51000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-51000\n",
      "2019-05-21 11:10:51 Iter 51100 [Train]: loss=128.21, epe=4.09, lr=0.000010, samples/sec=17.2, sec/step=1.393, eta=2 days, 9:36:51\n",
      "2019-05-21 11:13:22 Iter 51200 [Train]: loss=128.39, epe=4.11, lr=0.000010, samples/sec=17.0, sec/step=1.414, eta=2 days, 10:27:20\n",
      "2019-05-21 11:15:54 Iter 51300 [Train]: loss=129.75, epe=4.15, lr=0.000010, samples/sec=16.8, sec/step=1.425, eta=2 days, 10:51:17\n",
      "2019-05-21 11:18:25 Iter 51400 [Train]: loss=128.39, epe=4.10, lr=0.000010, samples/sec=17.0, sec/step=1.409, eta=2 days, 10:08:54\n",
      "2019-05-21 11:20:55 Iter 51500 [Train]: loss=128.54, epe=4.10, lr=0.000010, samples/sec=17.1, sec/step=1.407, eta=2 days, 10:02:25\n",
      "2019-05-21 11:23:25 Iter 51600 [Train]: loss=128.57, epe=4.11, lr=0.000010, samples/sec=17.2, sec/step=1.396, eta=2 days, 9:33:20\n",
      "2019-05-21 11:25:53 Iter 51700 [Train]: loss=127.97, epe=4.08, lr=0.000010, samples/sec=17.2, sec/step=1.392, eta=2 days, 9:20:48\n",
      "2019-05-21 11:28:25 Iter 51800 [Train]: loss=128.26, epe=4.10, lr=0.000010, samples/sec=17.0, sec/step=1.416, eta=2 days, 10:16:19\n",
      "2019-05-21 11:30:53 Iter 51900 [Train]: loss=128.52, epe=4.10, lr=0.000010, samples/sec=17.3, sec/step=1.386, eta=2 days, 9:01:01\n",
      "2019-05-21 11:33:24 Iter 52000 [Train]: loss=128.73, epe=4.10, lr=0.000010, samples/sec=17.0, sec/step=1.412, eta=2 days, 10:03:57\n",
      "2019-05-21 11:33:24 Iter 52000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-52000 is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-52000\n",
      "2019-05-21 11:36:06 Iter 52100 [Train]: loss=128.17, epe=4.10, lr=0.000010, samples/sec=17.0, sec/step=1.411, eta=2 days, 9:59:13\n",
      "2019-05-21 11:38:36 Iter 52200 [Train]: loss=127.67, epe=4.08, lr=0.000010, samples/sec=17.1, sec/step=1.404, eta=2 days, 9:38:08\n",
      "2019-05-21 11:41:09 Iter 52300 [Train]: loss=128.95, epe=4.12, lr=0.000010, samples/sec=16.7, sec/step=1.436, eta=2 days, 10:55:31\n",
      "2019-05-21 11:43:39 Iter 52400 [Train]: loss=128.19, epe=4.09, lr=0.000010, samples/sec=17.2, sec/step=1.399, eta=2 days, 9:20:25\n",
      "2019-05-21 11:46:09 Iter 52500 [Train]: loss=128.10, epe=4.09, lr=0.000010, samples/sec=17.0, sec/step=1.411, eta=2 days, 9:49:55\n",
      "2019-05-21 11:48:41 Iter 52600 [Train]: loss=127.97, epe=4.08, lr=0.000010, samples/sec=17.0, sec/step=1.413, eta=2 days, 9:51:12\n",
      "2019-05-21 11:51:11 Iter 52700 [Train]: loss=128.27, epe=4.10, lr=0.000010, samples/sec=17.0, sec/step=1.410, eta=2 days, 9:41:32\n",
      "2019-05-21 11:53:43 Iter 52800 [Train]: loss=127.77, epe=4.08, lr=0.000010, samples/sec=16.9, sec/step=1.422, eta=2 days, 10:08:35\n",
      "2019-05-21 11:56:16 Iter 52900 [Train]: loss=128.86, epe=4.12, lr=0.000010, samples/sec=16.8, sec/step=1.430, eta=2 days, 10:27:03\n",
      "2019-05-21 11:58:50 Iter 53000 [Train]: loss=128.68, epe=4.11, lr=0.000010, samples/sec=16.6, sec/step=1.442, eta=2 days, 10:51:46\n",
      "2019-05-21 11:58:50 Iter 53000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-53000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-53000\n",
      "2019-05-21 12:01:32 Iter 53100 [Train]: loss=128.57, epe=4.11, lr=0.000010, samples/sec=17.2, sec/step=1.398, eta=2 days, 9:03:35\n",
      "2019-05-21 12:04:04 Iter 53200 [Train]: loss=127.85, epe=4.08, lr=0.000010, samples/sec=16.9, sec/step=1.422, eta=2 days, 9:59:00\n",
      "2019-05-21 12:06:36 Iter 53300 [Train]: loss=127.72, epe=4.08, lr=0.000010, samples/sec=16.8, sec/step=1.427, eta=2 days, 10:08:34\n",
      "2019-05-21 12:09:07 Iter 53400 [Train]: loss=128.24, epe=4.09, lr=0.000010, samples/sec=16.9, sec/step=1.416, eta=2 days, 9:40:11\n",
      "2019-05-21 12:11:39 Iter 53500 [Train]: loss=128.32, epe=4.11, lr=0.000010, samples/sec=16.8, sec/step=1.427, eta=2 days, 10:05:04\n",
      "2019-05-21 12:14:13 Iter 53600 [Train]: loss=127.77, epe=4.08, lr=0.000010, samples/sec=16.7, sec/step=1.437, eta=2 days, 10:25:19\n",
      "2019-05-21 12:16:43 Iter 53700 [Train]: loss=128.55, epe=4.11, lr=0.000010, samples/sec=17.0, sec/step=1.409, eta=2 days, 9:14:43\n",
      "2019-05-21 12:19:16 Iter 53800 [Train]: loss=127.68, epe=4.08, lr=0.000010, samples/sec=16.7, sec/step=1.436, eta=2 days, 10:19:06\n",
      "2019-05-21 12:21:50 Iter 53900 [Train]: loss=128.06, epe=4.10, lr=0.000010, samples/sec=16.6, sec/step=1.441, eta=2 days, 10:30:01\n",
      "2019-05-21 12:24:21 Iter 54000 [Train]: loss=127.67, epe=4.08, lr=0.000010, samples/sec=16.9, sec/step=1.419, eta=2 days, 9:34:05\n",
      "2019-05-21 12:24:21 Iter 54000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-54000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-54000\n",
      "2019-05-21 12:27:04 Iter 54100 [Train]: loss=128.21, epe=4.10, lr=0.000010, samples/sec=16.9, sec/step=1.418, eta=2 days, 9:28:14\n",
      "2019-05-21 12:29:35 Iter 54200 [Train]: loss=128.64, epe=4.11, lr=0.000010, samples/sec=17.0, sec/step=1.410, eta=2 days, 9:05:37\n",
      "2019-05-21 12:32:06 Iter 54300 [Train]: loss=127.31, epe=4.07, lr=0.000010, samples/sec=17.0, sec/step=1.414, eta=2 days, 9:13:05\n",
      "2019-05-21 12:34:37 Iter 54400 [Train]: loss=129.29, epe=4.13, lr=0.000010, samples/sec=16.9, sec/step=1.418, eta=2 days, 9:20:16\n",
      "2019-05-21 12:37:08 Iter 54500 [Train]: loss=128.24, epe=4.09, lr=0.000010, samples/sec=17.0, sec/step=1.412, eta=2 days, 9:03:31\n",
      "2019-05-21 12:39:40 Iter 54600 [Train]: loss=128.58, epe=4.11, lr=0.000010, samples/sec=16.8, sec/step=1.426, eta=2 days, 9:35:53\n",
      "2019-05-21 12:42:12 Iter 54700 [Train]: loss=128.67, epe=4.11, lr=0.000010, samples/sec=16.8, sec/step=1.430, eta=2 days, 9:41:47\n",
      "2019-05-21 12:44:43 Iter 54800 [Train]: loss=128.54, epe=4.10, lr=0.000010, samples/sec=17.0, sec/step=1.411, eta=2 days, 8:53:43\n",
      "2019-05-21 12:47:15 Iter 54900 [Train]: loss=127.98, epe=4.09, lr=0.000010, samples/sec=16.8, sec/step=1.425, eta=2 days, 9:27:10\n",
      "2019-05-21 12:49:45 Iter 55000 [Train]: loss=128.54, epe=4.11, lr=0.000010, samples/sec=17.1, sec/step=1.406, eta=2 days, 8:36:53\n",
      "2019-05-21 12:49:45 Iter 55000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-55000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-55000\n",
      "2019-05-21 12:52:27 Iter 55100 [Train]: loss=128.32, epe=4.10, lr=0.000010, samples/sec=17.1, sec/step=1.403, eta=2 days, 8:28:07\n",
      "2019-05-21 12:54:59 Iter 55200 [Train]: loss=127.23, epe=4.07, lr=0.000010, samples/sec=16.9, sec/step=1.419, eta=2 days, 9:05:20\n",
      "2019-05-21 12:57:29 Iter 55300 [Train]: loss=127.56, epe=4.08, lr=0.000010, samples/sec=17.0, sec/step=1.411, eta=2 days, 8:43:06\n",
      "2019-05-21 13:00:01 Iter 55400 [Train]: loss=129.04, epe=4.13, lr=0.000010, samples/sec=17.0, sec/step=1.416, eta=2 days, 8:52:17\n",
      "2019-05-21 13:02:30 Iter 55500 [Train]: loss=127.85, epe=4.09, lr=0.000010, samples/sec=17.1, sec/step=1.404, eta=2 days, 8:20:56\n",
      "2019-05-21 13:05:00 Iter 55600 [Train]: loss=128.05, epe=4.09, lr=0.000010, samples/sec=17.2, sec/step=1.396, eta=2 days, 7:59:38\n",
      "2019-05-21 13:07:29 Iter 55700 [Train]: loss=127.87, epe=4.09, lr=0.000010, samples/sec=17.2, sec/step=1.399, eta=2 days, 8:05:34\n",
      "2019-05-21 13:09:57 Iter 55800 [Train]: loss=128.13, epe=4.09, lr=0.000010, samples/sec=17.3, sec/step=1.389, eta=2 days, 7:38:00\n",
      "2019-05-21 13:12:29 Iter 55900 [Train]: loss=128.19, epe=4.10, lr=0.000010, samples/sec=16.8, sec/step=1.425, eta=2 days, 9:02:03\n",
      "2019-05-21 13:15:01 Iter 56000 [Train]: loss=128.87, epe=4.12, lr=0.000010, samples/sec=16.9, sec/step=1.421, eta=2 days, 8:50:10\n",
      "2019-05-21 13:15:01 Iter 56000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-56000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-56000\n",
      "2019-05-21 13:17:47 Iter 56100 [Train]: loss=127.31, epe=4.07, lr=0.000010, samples/sec=16.8, sec/step=1.425, eta=2 days, 8:58:12\n",
      "2019-05-21 13:20:17 Iter 56200 [Train]: loss=129.07, epe=4.12, lr=0.000010, samples/sec=17.1, sec/step=1.403, eta=2 days, 8:03:16\n",
      "2019-05-21 13:22:49 Iter 56300 [Train]: loss=128.34, epe=4.10, lr=0.000010, samples/sec=16.9, sec/step=1.424, eta=2 days, 8:51:05\n",
      "2019-05-21 13:25:23 Iter 56400 [Train]: loss=127.39, epe=4.07, lr=0.000010, samples/sec=16.7, sec/step=1.437, eta=2 days, 9:19:18\n",
      "2019-05-21 13:27:52 Iter 56500 [Train]: loss=127.93, epe=4.09, lr=0.000010, samples/sec=17.2, sec/step=1.395, eta=2 days, 7:35:57\n",
      "2019-05-21 13:30:22 Iter 56600 [Train]: loss=127.79, epe=4.08, lr=0.000010, samples/sec=17.1, sec/step=1.403, eta=2 days, 7:52:45\n",
      "2019-05-21 13:32:52 Iter 56700 [Train]: loss=127.85, epe=4.09, lr=0.000010, samples/sec=17.1, sec/step=1.404, eta=2 days, 7:52:13\n",
      "2019-05-21 13:35:23 Iter 56800 [Train]: loss=127.89, epe=4.09, lr=0.000010, samples/sec=17.0, sec/step=1.408, eta=2 days, 8:00:15\n",
      "2019-05-21 13:37:53 Iter 56900 [Train]: loss=127.75, epe=4.09, lr=0.000010, samples/sec=17.1, sec/step=1.406, eta=2 days, 7:54:20\n",
      "2019-05-21 13:40:24 Iter 57000 [Train]: loss=127.87, epe=4.09, lr=0.000010, samples/sec=17.0, sec/step=1.411, eta=2 days, 8:03:47\n",
      "2019-05-21 13:40:24 Iter 57000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-57000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-57000\n",
      "2019-05-21 13:43:05 Iter 57100 [Train]: loss=128.71, epe=4.12, lr=0.000010, samples/sec=17.1, sec/step=1.400, eta=2 days, 7:33:54\n",
      "2019-05-21 13:45:38 Iter 57200 [Train]: loss=128.07, epe=4.09, lr=0.000010, samples/sec=16.8, sec/step=1.430, eta=2 days, 8:44:10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 13:48:08 Iter 57300 [Train]: loss=127.54, epe=4.08, lr=0.000010, samples/sec=17.1, sec/step=1.404, eta=2 days, 7:39:10\n",
      "2019-05-21 13:50:42 Iter 57400 [Train]: loss=127.54, epe=4.07, lr=0.000010, samples/sec=16.6, sec/step=1.446, eta=2 days, 9:16:44\n",
      "2019-05-21 13:53:13 Iter 57500 [Train]: loss=127.24, epe=4.07, lr=0.000010, samples/sec=17.0, sec/step=1.409, eta=2 days, 7:46:21\n",
      "2019-05-21 13:55:43 Iter 57600 [Train]: loss=127.82, epe=4.09, lr=0.000010, samples/sec=17.2, sec/step=1.399, eta=2 days, 7:20:53\n",
      "2019-05-21 13:58:15 Iter 57700 [Train]: loss=128.53, epe=4.11, lr=0.000010, samples/sec=16.8, sec/step=1.427, eta=2 days, 8:24:08\n",
      "2019-05-21 14:00:46 Iter 57800 [Train]: loss=127.76, epe=4.08, lr=0.000010, samples/sec=17.0, sec/step=1.413, eta=2 days, 7:49:50\n",
      "2019-05-21 14:03:19 Iter 57900 [Train]: loss=128.40, epe=4.11, lr=0.000010, samples/sec=16.7, sec/step=1.435, eta=2 days, 8:38:23\n",
      "2019-05-21 14:05:49 Iter 58000 [Train]: loss=127.35, epe=4.08, lr=0.000010, samples/sec=17.0, sec/step=1.410, eta=2 days, 7:36:02\n",
      "2019-05-21 14:05:49 Iter 58000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-58000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-58000\n",
      "2019-05-21 14:08:31 Iter 58100 [Train]: loss=128.61, epe=4.11, lr=0.000010, samples/sec=17.0, sec/step=1.408, eta=2 days, 7:29:09\n",
      "2019-05-21 14:11:02 Iter 58200 [Train]: loss=127.47, epe=4.08, lr=0.000010, samples/sec=17.0, sec/step=1.410, eta=2 days, 7:33:28\n",
      "2019-05-21 14:13:32 Iter 58300 [Train]: loss=128.27, epe=4.10, lr=0.000010, samples/sec=17.1, sec/step=1.403, eta=2 days, 7:13:21\n",
      "2019-05-21 14:16:01 Iter 58400 [Train]: loss=127.85, epe=4.09, lr=0.000010, samples/sec=17.2, sec/step=1.394, eta=2 days, 6:50:24\n",
      "2019-05-21 14:18:33 Iter 58500 [Train]: loss=126.75, epe=4.05, lr=0.000010, samples/sec=17.0, sec/step=1.414, eta=2 days, 7:33:54\n",
      "2019-05-21 14:21:03 Iter 58600 [Train]: loss=127.11, epe=4.07, lr=0.000010, samples/sec=17.1, sec/step=1.406, eta=2 days, 7:13:16\n",
      "2019-05-21 14:23:35 Iter 58700 [Train]: loss=128.46, epe=4.11, lr=0.000010, samples/sec=16.9, sec/step=1.422, eta=2 days, 7:48:22\n",
      "2019-05-21 14:26:08 Iter 58800 [Train]: loss=128.08, epe=4.10, lr=0.000010, samples/sec=16.8, sec/step=1.431, eta=2 days, 8:06:36\n",
      "2019-05-21 14:28:38 Iter 58900 [Train]: loss=127.66, epe=4.08, lr=0.000010, samples/sec=17.1, sec/step=1.403, eta=2 days, 7:00:21\n",
      "2019-05-21 14:31:13 Iter 59000 [Train]: loss=127.36, epe=4.07, lr=0.000010, samples/sec=16.5, sec/step=1.451, eta=2 days, 8:50:22\n",
      "2019-05-21 14:31:13 Iter 59000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-59000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-59000\n",
      "2019-05-21 14:33:53 Iter 59100 [Train]: loss=128.00, epe=4.09, lr=0.000010, samples/sec=17.1, sec/step=1.400, eta=2 days, 6:47:55\n",
      "2019-05-21 14:36:25 Iter 59200 [Train]: loss=128.90, epe=4.13, lr=0.000010, samples/sec=16.9, sec/step=1.419, eta=2 days, 7:30:33\n",
      "2019-05-21 14:38:57 Iter 59300 [Train]: loss=127.09, epe=4.07, lr=0.000010, samples/sec=17.0, sec/step=1.415, eta=2 days, 7:18:10\n",
      "2019-05-21 14:41:33 Iter 59400 [Train]: loss=127.68, epe=4.08, lr=0.000010, samples/sec=16.5, sec/step=1.452, eta=2 days, 8:43:14\n",
      "2019-05-21 14:44:09 Iter 59500 [Train]: loss=127.75, epe=4.08, lr=0.000010, samples/sec=16.5, sec/step=1.452, eta=2 days, 8:40:17\n",
      "2019-05-21 14:46:47 Iter 59600 [Train]: loss=128.35, epe=4.11, lr=0.000010, samples/sec=16.5, sec/step=1.452, eta=2 days, 8:37:07\n",
      "2019-05-21 14:49:20 Iter 59700 [Train]: loss=128.09, epe=4.10, lr=0.000010, samples/sec=16.8, sec/step=1.432, eta=2 days, 7:49:15\n",
      "2019-05-21 14:51:58 Iter 59800 [Train]: loss=127.78, epe=4.08, lr=0.000010, samples/sec=16.3, sec/step=1.468, eta=2 days, 9:11:17\n",
      "2019-05-21 14:54:35 Iter 59900 [Train]: loss=128.09, epe=4.10, lr=0.000010, samples/sec=16.5, sec/step=1.454, eta=2 days, 8:36:11\n",
      "2019-05-21 14:57:11 Iter 60000 [Train]: loss=127.42, epe=4.07, lr=0.000010, samples/sec=16.6, sec/step=1.444, eta=2 days, 8:08:47\n",
      "2019-05-21 14:57:11 Iter 60000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-60000\n",
      "2019-05-21 15:00:06 Iter 60100 [Train]: loss=127.88, epe=4.09, lr=0.000010, samples/sec=16.3, sec/step=1.472, eta=2 days, 9:12:33\n",
      "2019-05-21 15:02:42 Iter 60200 [Train]: loss=128.59, epe=4.11, lr=0.000010, samples/sec=16.7, sec/step=1.439, eta=2 days, 7:52:19\n",
      "2019-05-21 15:05:22 Iter 60300 [Train]: loss=127.58, epe=4.08, lr=0.000010, samples/sec=16.3, sec/step=1.473, eta=2 days, 9:08:37\n",
      "2019-05-21 15:07:55 Iter 60400 [Train]: loss=128.01, epe=4.10, lr=0.000010, samples/sec=16.7, sec/step=1.436, eta=2 days, 7:41:07\n",
      "2019-05-21 15:10:33 Iter 60500 [Train]: loss=127.73, epe=4.09, lr=0.000010, samples/sec=16.4, sec/step=1.461, eta=2 days, 8:37:24\n",
      "2019-05-21 15:13:09 Iter 60600 [Train]: loss=127.75, epe=4.08, lr=0.000010, samples/sec=16.6, sec/step=1.445, eta=2 days, 7:56:52\n",
      "2019-05-21 15:15:42 Iter 60700 [Train]: loss=127.62, epe=4.08, lr=0.000010, samples/sec=17.0, sec/step=1.411, eta=2 days, 6:36:52\n",
      "2019-05-21 15:18:16 Iter 60800 [Train]: loss=128.13, epe=4.10, lr=0.000010, samples/sec=16.7, sec/step=1.434, eta=2 days, 7:26:37\n",
      "2019-05-21 15:20:54 Iter 60900 [Train]: loss=127.95, epe=4.09, lr=0.000010, samples/sec=16.5, sec/step=1.459, eta=2 days, 8:22:11\n",
      "2019-05-21 15:23:27 Iter 61000 [Train]: loss=127.66, epe=4.08, lr=0.000010, samples/sec=17.0, sec/step=1.414, eta=2 days, 6:35:56\n",
      "2019-05-21 15:23:27 Iter 61000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-61000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-61000\n",
      "2019-05-21 15:26:18 Iter 61100 [Train]: loss=128.44, epe=4.11, lr=0.000010, samples/sec=16.6, sec/step=1.447, eta=2 days, 7:50:30\n",
      "2019-05-21 15:28:54 Iter 61200 [Train]: loss=127.50, epe=4.08, lr=0.000010, samples/sec=16.7, sec/step=1.441, eta=2 days, 7:34:20\n",
      "2019-05-21 15:31:36 Iter 61300 [Train]: loss=128.37, epe=4.11, lr=0.000010, samples/sec=16.1, sec/step=1.493, eta=2 days, 9:31:52\n",
      "2019-05-21 15:34:11 Iter 61400 [Train]: loss=127.97, epe=4.09, lr=0.000010, samples/sec=16.6, sec/step=1.446, eta=2 days, 7:41:17\n",
      "2019-05-21 15:36:50 Iter 61500 [Train]: loss=128.55, epe=4.11, lr=0.000010, samples/sec=16.3, sec/step=1.469, eta=2 days, 8:30:00\n",
      "2019-05-21 15:39:27 Iter 61600 [Train]: loss=127.12, epe=4.06, lr=0.000010, samples/sec=16.6, sec/step=1.448, eta=2 days, 7:40:58\n",
      "2019-05-21 15:42:03 Iter 61700 [Train]: loss=127.64, epe=4.08, lr=0.000010, samples/sec=16.7, sec/step=1.439, eta=2 days, 7:17:47\n",
      "2019-05-21 15:44:39 Iter 61800 [Train]: loss=128.04, epe=4.09, lr=0.000010, samples/sec=16.5, sec/step=1.457, eta=2 days, 7:56:31\n",
      "2019-05-21 15:47:19 Iter 61900 [Train]: loss=127.82, epe=4.09, lr=0.000010, samples/sec=16.4, sec/step=1.467, eta=2 days, 8:15:47\n",
      "2019-05-21 15:49:56 Iter 62000 [Train]: loss=127.32, epe=4.08, lr=0.000010, samples/sec=16.5, sec/step=1.454, eta=2 days, 7:44:22\n",
      "2019-05-21 15:49:56 Iter 62000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-62000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-62000\n",
      "2019-05-21 15:52:47 Iter 62100 [Train]: loss=128.13, epe=4.09, lr=0.000010, samples/sec=16.5, sec/step=1.453, eta=2 days, 7:40:09\n",
      "2019-05-21 15:55:24 Iter 62200 [Train]: loss=127.99, epe=4.10, lr=0.000010, samples/sec=16.4, sec/step=1.467, eta=2 days, 8:08:24\n",
      "2019-05-21 15:58:01 Iter 62300 [Train]: loss=128.90, epe=4.12, lr=0.000010, samples/sec=16.5, sec/step=1.454, eta=2 days, 7:37:24\n",
      "2019-05-21 16:00:35 Iter 62400 [Train]: loss=127.88, epe=4.09, lr=0.000010, samples/sec=16.8, sec/step=1.426, eta=2 days, 6:31:25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-21 16:03:13 Iter 62500 [Train]: loss=127.69, epe=4.09, lr=0.000010, samples/sec=16.4, sec/step=1.462, eta=2 days, 7:51:07\n",
      "2019-05-21 16:05:51 Iter 62600 [Train]: loss=127.68, epe=4.09, lr=0.000010, samples/sec=16.4, sec/step=1.466, eta=2 days, 7:57:59\n",
      "2019-05-21 16:08:26 Iter 62700 [Train]: loss=128.09, epe=4.10, lr=0.000010, samples/sec=16.7, sec/step=1.440, eta=2 days, 6:55:16\n",
      "2019-05-21 16:11:03 Iter 62800 [Train]: loss=127.45, epe=4.08, lr=0.000010, samples/sec=16.6, sec/step=1.446, eta=2 days, 7:07:08\n",
      "2019-05-21 16:13:38 Iter 62900 [Train]: loss=126.95, epe=4.06, lr=0.000010, samples/sec=16.7, sec/step=1.438, eta=2 days, 6:45:36\n",
      "2019-05-21 16:16:15 Iter 63000 [Train]: loss=128.88, epe=4.12, lr=0.000010, samples/sec=16.6, sec/step=1.450, eta=2 days, 7:10:46\n",
      "2019-05-21 16:16:15 Iter 63000 [Val]: loss=nan, epe=nan\n",
      "Saving model...\n",
      "INFO:tensorflow:./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-63000 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "... model saved in ./pwcnet-lg-6-2-cyclic-KITTI2015_finetuned/pwcnet.ckpt-63000\n",
      "2019-05-21 16:19:07 Iter 63100 [Train]: loss=127.14, epe=4.07, lr=0.000010, samples/sec=16.5, sec/step=1.453, eta=2 days, 7:14:46\n",
      "2019-05-21 16:21:44 Iter 63200 [Train]: loss=128.62, epe=4.12, lr=0.000010, samples/sec=16.5, sec/step=1.451, eta=2 days, 7:08:58\n",
      "2019-05-21 16:24:17 Iter 63300 [Train]: loss=127.21, epe=4.07, lr=0.000010, samples/sec=16.9, sec/step=1.423, eta=2 days, 6:03:01\n",
      "2019-05-21 16:26:48 Iter 63400 [Train]: loss=127.87, epe=4.09, lr=0.000010, samples/sec=17.1, sec/step=1.406, eta=2 days, 5:21:15\n",
      "2019-05-21 16:29:19 Iter 63500 [Train]: loss=127.84, epe=4.09, lr=0.000010, samples/sec=16.9, sec/step=1.420, eta=2 days, 5:51:02\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "nn.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the training curves for the run above:\n",
    "\n",
    "![](img/pwcnet-sm-6-2-cyclic-chairsthingsmix_finetuned/loss.png)\n",
    "![](img/pwcnet-sm-6-2-cyclic-chairsthingsmix_finetuned/epe.png)\n",
    "![](img/pwcnet-sm-6-2-cyclic-chairsthingsmix_finetuned/lr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the predictions issued by the model for a few validation samples:\n",
    "\n",
    "![](img/pwcnet-sm-6-2-cyclic-chairsthingsmix_finetuned/val1.png)\n",
    "![](img/pwcnet-sm-6-2-cyclic-chairsthingsmix_finetuned/val2.png)\n",
    "![](img/pwcnet-sm-6-2-cyclic-chairsthingsmix_finetuned/val3.png)\n",
    "![](img/pwcnet-sm-6-2-cyclic-chairsthingsmix_finetuned/val4.png)\n",
    "![](img/pwcnet-sm-6-2-cyclic-chairsthingsmix_finetuned/val5.png)\n",
    "![](img/pwcnet-sm-6-2-cyclic-chairsthingsmix_finetuned/val6.png)\n",
    "![](img/pwcnet-sm-6-2-cyclic-chairsthingsmix_finetuned/val7.png)\n",
    "![](img/pwcnet-sm-6-2-cyclic-chairsthingsmix_finetuned/val8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
