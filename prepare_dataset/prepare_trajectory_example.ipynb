{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p dataset_source\n",
    "!>dataset_source/0.png\n",
    "!>dataset_source/1.png\n",
    "!>dataset_source/2.png\n",
    "!>dataset_source/3.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleParser():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def run(self):\n",
    "        import pandas as pd\n",
    "        import os\n",
    "        df = pd.DataFrame(data=[[os.getcwd() + '/dataset_source/0.png', 2, 3, 4], \n",
    "                                [os.getcwd() + '/dataset_source/1.png', 12, 13, 14], \n",
    "                                [os.getcwd() + '/dataset_source/2.png', 22, 23, 24], \n",
    "                                [os.getcwd() + '/dataset_source/3.png', 32, 33, 34]],\n",
    "                          columns=['path_to_rgb', 'x', 'y', 'z'])\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "import itertools\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import pyquaternion\n",
    "\n",
    "from linalg.linalg_utils import (form_se3,\n",
    "                                 convert_global_se3_matrices_to_relative,\n",
    "                                 convert_relative_se3_matrices_to_euler)\n",
    "\n",
    "\n",
    "class BaseParser:\n",
    "    def __init__(self,\n",
    "                 sequence_directory,\n",
    "                 global_csv_filename):\n",
    "        self.sequence_directory = sequence_directory\n",
    "        self.global_csv_filename = global_csv_filename\n",
    "        self.global_csv_path = os.path.join(self.sequence_directory, self.global_csv_filename)\n",
    "    \n",
    "    def _load_data(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def _calculate_global_pose_matrices(self):\n",
    "        pass\n",
    "\n",
    "    def _make_absolute_filepath(self):\n",
    "        self.global_dataframe.path_to_rgb = self.global_dataframe.path_to_rgb.apply(\n",
    "            lambda filename: os.path.join(self.directory, filename))\n",
    "        self.global_dataframe.path_to_depth = self.global_dataframe.path_to_depth.apply(\n",
    "            lambda filename: os.path.join(self.directory, filename))\n",
    "    \n",
    "    def _create_global_dataframe(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def parse(self):\n",
    "        self._load_data()\n",
    "        self._create_global_dataframe()\n",
    "        self._make_absolute_filepath()\n",
    "        self.global_dataframe.to_csv(self.global_csv_path, index=False)\n",
    "        print('Parse ok...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TUMParser(BaseParser):\n",
    "    def __init__(self,\n",
    "                 sequence_directory,\n",
    "                 directory,\n",
    "                 global_csv_filename='global.csv'):\n",
    "        super(TUMParser, self).__init__(sequence_directory, \n",
    "                                        global_csv_filename\n",
    "                                        )\n",
    "        self.directory = directory\n",
    "        self.gt_txt_path = os.path.join(self.directory, 'groundtruth.txt')\n",
    "        self.depth_txt_path = os.path.join(self.directory, 'depth.txt')\n",
    "        self.rgb_txt_path = os.path.join(self.directory, 'rgb.txt')\n",
    "\n",
    "    @staticmethod\n",
    "    def associate_timestamps(timestamps, other_timestamps, max_difference=0.02):\n",
    "        timestamps = list(timestamps)\n",
    "        other_timestamps = list(other_timestamps)\n",
    "        potential_matches = [(timestamp, other_timestamp)\n",
    "                             for timestamp in timestamps\n",
    "                             for other_timestamp in other_timestamps\n",
    "                             if abs(timestamp - other_timestamp) < max_difference]\n",
    "        potential_matches.sort(key=lambda x: abs(x[0] - x[1]))\n",
    "        \n",
    "        matches = []\n",
    "        for timestamp, other_timestamp in potential_matches:\n",
    "            if timestamp in timestamps and other_timestamp in other_timestamps:\n",
    "                timestamps.remove(timestamp)\n",
    "                other_timestamps.remove(other_timestamp)\n",
    "                matches.append((timestamp, other_timestamp))\n",
    "\n",
    "        matches.sort()\n",
    "        return list(zip(*matches))\n",
    "    \n",
    "    @staticmethod\n",
    "    def associate_dataframes(dataframes, timestamp_cols):\n",
    "        dataframe = dataframes[0]\n",
    "        timestamp_col = timestamp_cols[0]\n",
    "        for other_dataframe, other_timestamp_col in zip(dataframes[1:], timestamp_cols[1:]):\n",
    "            timestamps, other_timestamps = \\\n",
    "                TUMParser.associate_timestamps(dataframe[timestamp_col].values, other_dataframe[other_timestamp_col].values)\n",
    "            dataframe = dataframe[dataframe[timestamp_col].isin(timestamps)]\n",
    "            dataframe.index = np.arange(len(dataframe))\n",
    "            other_dataframe = other_dataframe[other_dataframe[other_timestamp_col].isin(other_timestamps)]\n",
    "            other_dataframe.index = timestamps\n",
    "            \n",
    "            assert len(dataframe) == len(other_dataframe)\n",
    "            dataframe = dataframe.join(other_dataframe, on=timestamp_col)\n",
    "        return dataframe\n",
    "\n",
    "    def _load_txt(self, txt_path, columns):\n",
    "        dataframe = pd.read_csv(txt_path, skiprows=3, sep=' ', index_col=False, names=columns)\n",
    "        dataframe.columns = columns\n",
    "        timestamp_col = columns[0]\n",
    "        dataframe[timestamp_col] = dataframe[timestamp_col].apply(float)\n",
    "        return dataframe\n",
    "\n",
    "    def _load_data(self):\n",
    "        gt_dataframe = self._load_txt(self.gt_txt_path, columns=['timestamp_gt', 'x', 'y', 'z', 'qx', 'qy', 'qz', 'qw'])\n",
    "        depth_dataframe = self._load_txt(self.depth_txt_path, columns=['timestamp_depth', 'path_to_depth'])\n",
    "        rgb_dataframe = self._load_txt(self.rgb_txt_path, columns=['timestamp_rgb', 'path_to_rgb'])\n",
    "        self.dataframes = [depth_dataframe, rgb_dataframe, gt_dataframe]\n",
    "        self.timestamp_cols = ['timestamp_depth', 'timestamp_rgb', 'timestamp_gt']\n",
    "\n",
    "    def _create_global_dataframe(self):\n",
    "        self.global_dataframe = self.associate_dataframes(self.dataframes, self.timestamp_cols)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'TUMParser(dir={}, txt_path={}, global_csv_filename={})'.format(\n",
    "            self.sequence_directory, self.gt_txt_path, self.global_csv_filename\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_directory = '/Vol0/user/f.konokhov/newodomety/odometry/notebooks/tudum'\n",
    "directory = '/Vol1/dbstore/datasets/tum_rgbd_flow/data/rgbd_dataset_freiburg2_coke'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tum = TUMParser(sequence_directory, directory=directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse ok...\n"
     ]
    }
   ],
   "source": [
    "tum.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetailBotParser(TUMParser):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 sequence_directory,\n",
    "                 directory,\n",
    "                 global_csv_filename='global.csv'):\n",
    "        super(RetailBotParser, self).__init__(sequence_directory, \n",
    "                                        global_csv_filename\n",
    "                                        )\n",
    "        self.directory = directory\n",
    "        self.gt_txt_path = os.path.join(self.directory, 'pose.txt')\n",
    "        self.depth_txt_path = os.path.join(self.directory, 'depth.txt')\n",
    "        self.rgb_txt_path = os.path.join(self.directory, 'rgb.txt')\n",
    "    \n",
    "    def _load_txt(self, txt_path, columns):\n",
    "        dataframe = pd.read_csv(txt_path, skiprows=0, sep=' ', index_col=False, names=columns)\n",
    "        dataframe.columns = columns\n",
    "        timestamp_col = columns[0]\n",
    "        dataframe[timestamp_col] = dataframe[timestamp_col].apply(float)\n",
    "        return dataframe\n",
    "    \n",
    "    def _load_pic(self, txt_path, columns):\n",
    "        dataframe = pd.read_csv(txt_path, skiprows=0, sep=' ', index_col=False, names=columns)\n",
    "        dataframe.columns = columns\n",
    "        timestamp_col = columns[0]\n",
    "        pic = columns[1]\n",
    "        dataframe[timestamp_col] = dataframe[timestamp_col].apply(float)\n",
    "        dataframe[pic] = dataframe[pic].apply(lambda x : directory + x[1:])\n",
    "        return dataframe\n",
    "    \n",
    "    def _load_data(self):\n",
    "        gt_dataframe = self._load_txt(self.gt_txt_path, columns=['timestamp_gt', 'x', 'y', 'z', 'qx', 'qy', 'qz', 'qw'])\n",
    "        depth_dataframe = self._load_pic(self.depth_txt_path, columns=['timestamp_depth', 'path_to_depth'])\n",
    "        rgb_dataframe = self._load_pic(self.rgb_txt_path, columns=['timestamp_rgb', 'path_to_rgb'])\n",
    "        self.dataframes = [depth_dataframe, rgb_dataframe, gt_dataframe]\n",
    "        self.timestamp_cols = ['timestamp_depth', 'timestamp_rgb', 'timestamp_gt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_directory = '/Vol0/user/f.konokhov/newodomety/odometry/notebooks/meetingroom_04_rgbd_ir_imu_pose_KITTI_finetuned_tum2'\n",
    "directory = '/dbstore/datasets/retail_bot/meetingroom_04_rgbd_ir_imu_pose'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail = RetailBotParser(sequence_directory, directory=directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse ok...\n"
     ]
    }
   ],
   "source": [
    "retail.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DISCOMANParser(BaseParser):\n",
    "    def __init__(self,\n",
    "                 sequence_directory,\n",
    "                 json_path,\n",
    "                 global_csv_filename='global.csv'):\n",
    "        super(DISCOMANParser, self).__init__(sequence_directory,\n",
    "                                             global_csv_filename,)\n",
    "        self.directory = os.path.dirname(json_path)\n",
    "        self.image_directory = os.path.dirname(json_path)\n",
    "        self.depth_directory = os.path.dirname(json_path)\n",
    "        self.json_path = json_path\n",
    "\n",
    "    def _load_data(self):\n",
    "        with open(self.json_path) as read_file:\n",
    "            data = json.load(read_file)\n",
    "        self.trajectory = data['trajectory']['frames']\n",
    "\n",
    "    @staticmethod    \n",
    "    def get_path_to_rgb(item):\n",
    "        return '{}_raycast.jpg'.format(item['id'])\n",
    "\n",
    "    @staticmethod\n",
    "    def get_path_to_depth(item):\n",
    "        return '{}_depth.png'.format(item['id'])\n",
    "\n",
    "    @staticmethod\n",
    "    def get_timestamp(item):\n",
    "        return item['id']\n",
    "\n",
    "    @staticmethod\n",
    "    def get_global_quaternion(item):\n",
    "        return item['state']['global']['orientation']\n",
    "\n",
    "    @staticmethod\n",
    "    def get_global_translation(item):\n",
    "        return item['state']['global']['position']\n",
    "\n",
    "    @staticmethod\n",
    "    def get_global_pose_matrix(item):\n",
    "        return pyquaternion.Quaternion(item['state']['global']['orientation']).rotation_matrix\n",
    "\n",
    "    def _create_global_dataframe(self):\n",
    "        trajectory_parsed = []\n",
    "\n",
    "        for point in self.trajectory[::5]:\n",
    "            parsed_point = {}\n",
    "            parsed_point['timestamp'] = self.get_timestamp(point)\n",
    "            parsed_point['path_to_rgb'] = self.get_path_to_rgb(point)\n",
    "            parsed_point['path_to_depth'] = self.get_path_to_depth(point)\n",
    "            parsed_point.update(dict(zip(['qw', 'qx', 'qy', 'qz'], self.get_global_quaternion(point))))\n",
    "            parsed_point.update(dict(zip(['x', 'y', 'z'], self.get_global_translation(point))))\n",
    "            trajectory_parsed.append(parsed_point)\n",
    "\n",
    "        self.global_dataframe = pd.DataFrame.from_dict(trajectory_parsed)\n",
    "\n",
    "    def _calculate_global_pose_matrices(self):\n",
    "        self.global_pose_matrices = np.array([self.get_global_pose_matrix(item) for item in self.trajectory])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'JSONParser(dir={}, json_path={}, global_csv_filename={})'.format(\n",
    "            self.sequence_directory, self.json_path, self.global_csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_directory = '/Vol0/user/f.konokhov/newodomety/odometry/notebooks/disco'\n",
    "json_path = '/dbstore/datasets/renderbox/iros2019/dset/output/deprecated/000001/0_traj.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "discoman = DISCOMANParser(sequence_directory, json_path=json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse ok...\n"
     ]
    }
   ],
   "source": [
    "discoman.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OldDISCOMANParser(DISCOMANParser):\n",
    "    def __init__(self,\n",
    "                 sequence_directory,\n",
    "                 json_path,\n",
    "                 global_csv_filename='global.csv'):\n",
    "        super(OldDISCOMANParser, self).__init__(sequence_directory,\n",
    "                                                json_path,\n",
    "                                                global_csv_filename,\n",
    "                                                relative_csv_filename,\n",
    "                                                stride)\n",
    "\n",
    "    def _load_data(self):\n",
    "        with open(self.json_path) as read_file:\n",
    "            data = json.load(read_file)\n",
    "            self.trajectory = data['data']\n",
    "\n",
    "    @staticmethod\n",
    "    def get_path_to_rgb(item):\n",
    "        return '{}_raycast.jpg'.format(str(item['time']).zfill(6))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_path_to_depth(item):\n",
    "        return '{}_depth.png'.format(str(item['time']).zfill(6))\n",
    "\n",
    "    @staticmethod\n",
    "    def get_timestamp(item):\n",
    "        return item['time']\n",
    "\n",
    "    @staticmethod\n",
    "    def get_global_quaternion(item):\n",
    "        return item['info']['agent_state']['orientation']\n",
    "\n",
    "    @staticmethod\n",
    "    def get_global_translation(item):\n",
    "        return item['info']['agent_state']['position']\n",
    "\n",
    "    @staticmethod\n",
    "    def get_global_pose_matrix(item):\n",
    "        return pyquaternion.Quaternion(item['info']['agent_state']['orientation']).rotation_matrix\n",
    "    \n",
    "    def _create_global_dataframe(self):\n",
    "        trajectory_parsed = []\n",
    "\n",
    "        for point in self.trajectory:\n",
    "            parsed_point = {}\n",
    "            parsed_point['timestamp'] = self.get_timestamp(point)\n",
    "            parsed_point['path_to_rgb'] = self.get_path_to_rgb(point)\n",
    "            parsed_point['path_to_depth'] = self.get_path_to_depth(point)\n",
    "            parsed_point.update(dict(zip(['qw', 'qx', 'qy', 'qz'], self.get_global_quaternion(point))))\n",
    "            parsed_point.update(dict(zip(['x', 'y', 'z'], self.get_global_translation(point))))\n",
    "            trajectory_parsed.append(parsed_point)\n",
    "\n",
    "        self.global_dataframe = pd.DataFrame.from_dict(trajectory_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KITTIParser(BaseParser):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleEstimator():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def run(self, root, index, row):\n",
    "        result = row._asdict()\n",
    "        # save file <root + optical_flow_stride1 + index.npy>\n",
    "        result['path_to_optical_flow_stride1'] = '{}.npy'.format(index)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Vol0/user/f.konokhov/newodomety/odometry/prepare_dataset/')\n",
    "sys.path.append('/Vol0/user/f.konokhov/newodomety')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path_to_rgb</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Vol0/user/f.konokhov/newodomety/odometry/prep...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Vol0/user/f.konokhov/newodomety/odometry/prep...</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Vol0/user/f.konokhov/newodomety/odometry/prep...</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Vol0/user/f.konokhov/newodomety/odometry/prep...</td>\n",
       "      <td>32</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         path_to_rgb   x   y   z\n",
       "0  /Vol0/user/f.konokhov/newodomety/odometry/prep...   2   3   4\n",
       "1  /Vol0/user/f.konokhov/newodomety/odometry/prep...  12  13  14\n",
       "2  /Vol0/user/f.konokhov/newodomety/odometry/prep...  22  23  24\n",
       "3  /Vol0/user/f.konokhov/newodomety/odometry/prep...  32  33  34"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SampleParser.run(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odometry.prepare_dataset.prepare_trajectory import prepare_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path_to_rgb</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>path_to_optical_flow_stride1</th>\n",
       "      <th>path_to_rgb_next</th>\n",
       "      <th>x_next</th>\n",
       "      <th>y_next</th>\n",
       "      <th>z_next</th>\n",
       "      <th>path_to_optical_flow_stride1_next</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset_tmp/rgb/0.png</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.npy</td>\n",
       "      <td>dataset_tmp/rgb/1.png</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>1.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset_tmp/rgb/1.png</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>1.npy</td>\n",
       "      <td>dataset_tmp/rgb/2.png</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>2.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset_tmp/rgb/2.png</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>2.npy</td>\n",
       "      <td>dataset_tmp/rgb/3.png</td>\n",
       "      <td>32</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>3.npy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             path_to_rgb   x   y   z path_to_optical_flow_stride1  \\\n",
       "0  dataset_tmp/rgb/0.png   2   3   4                        0.npy   \n",
       "1  dataset_tmp/rgb/1.png  12  13  14                        1.npy   \n",
       "2  dataset_tmp/rgb/2.png  22  23  24                        2.npy   \n",
       "\n",
       "        path_to_rgb_next  x_next  y_next  z_next  \\\n",
       "0  dataset_tmp/rgb/1.png      12      13      14   \n",
       "1  dataset_tmp/rgb/2.png      22      23      24   \n",
       "2  dataset_tmp/rgb/3.png      32      33      34   \n",
       "\n",
       "  path_to_optical_flow_stride1_next  \n",
       "0                             1.npy  \n",
       "1                             2.npy  \n",
       "2                             3.npy  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_trajectory('dataset_tmp', parser=SampleParser(), \n",
    "                   single_frame_estimators=[SampleEstimator()],\n",
    "                   stride=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
